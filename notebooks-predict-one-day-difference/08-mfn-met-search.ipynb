{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/predict-one-day-diff/pol-met-search'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control experiment using last observed value for missing data imputation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 7, 52)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mfn(X_train, y_train, X_valid, y_valid, X_test, y_test, configs):\n",
    "#     p = np.random.permutation(X_train.shape[0])\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "    def swap_axes(nparr):\n",
    "        return nparr.swapaxes(0,1)\n",
    "    X_train = swap_axes(X_train)\n",
    "    X_valid = swap_axes(X_valid)\n",
    "    X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters \n",
    "    input_size = X_train.shape[2]\n",
    "    h = 128\n",
    "    t = X_train.shape[0]\n",
    "    output_dim = 1\n",
    "    dropout = 0.5\n",
    "\n",
    "#     d = X_train.shape[2]\n",
    "#     h = 128\n",
    "#     t = X_train.shape[0]\n",
    "#     output_dim = 1\n",
    "#     dropout = 0.5\n",
    "\n",
    "    [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig] = configs\n",
    "\n",
    "    \n",
    "    #model = EFLSTM(d,h,output_dim,dropout)\n",
    "    model = MFN(config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "    #optimizer = optim.SGD(model.parameters(),lr=config[\"lr\"],momentum=config[\"momentum\"])\n",
    "\n",
    "    # optimizer = optim.SGD([\n",
    "    #                 {'params':model.lstm_l.parameters(), 'lr':config[\"lr\"]},\n",
    "    #                 {'params':model.classifier.parameters(), 'lr':config[\"lr\"]}\n",
    "    #             ], momentum=0.9)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=5, factor=0.5, verbose=True)\n",
    "    \n",
    "#     criterion = nn.L1Loss()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer,mode='min',patience=100,factor=0.5,verbose=True)\n",
    "\n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[1]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[:,start:end])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/temp_models/mfn_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "#     print 'model number is:', rand\n",
    "    model = torch.load('models/temp_models/mfn_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFN(nn.Module):\n",
    "    def __init__(self,config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig):\n",
    "        super(MFN, self).__init__()\n",
    "        [self.d_l,self.d_a] = config[\"input_dims\"]\n",
    "        [self.dh_l,self.dh_a] = config[\"h_dims\"]\n",
    "        total_h_dim = self.dh_l+self.dh_a\n",
    "        \n",
    "        self.mem_dim = config[\"memsize\"]\n",
    "        window_dim = config[\"windowsize\"]\n",
    "        output_dim = 1\n",
    "        attInShape = total_h_dim*window_dim\n",
    "        gammaInShape = attInShape+self.mem_dim\n",
    "        final_out = total_h_dim+self.mem_dim\n",
    "        h_att1 = NN1Config[\"shapes\"]\n",
    "        h_att2 = NN2Config[\"shapes\"]\n",
    "        h_gamma1 = gamma1Config[\"shapes\"]\n",
    "        h_gamma2 = gamma2Config[\"shapes\"]\n",
    "        h_out = outConfig[\"shapes\"]\n",
    "        att1_dropout = NN1Config[\"drop\"]\n",
    "        att2_dropout = NN2Config[\"drop\"]\n",
    "        gamma1_dropout = gamma1Config[\"drop\"]\n",
    "        gamma2_dropout = gamma2Config[\"drop\"]\n",
    "        out_dropout = outConfig[\"drop\"]\n",
    "\n",
    "        self.lstm_l = nn.GRUCell(self.d_l, self.dh_l)\n",
    "        self.lstm_a = nn.GRUCell(self.d_a, self.dh_a)\n",
    "\n",
    "        self.att1_fc1 = nn.Linear(attInShape, h_att1)\n",
    "        self.att1_fc2 = nn.Linear(h_att1, attInShape)\n",
    "        self.att1_dropout = nn.Dropout(att1_dropout)\n",
    "\n",
    "        self.att2_fc1 = nn.Linear(attInShape, h_att2)\n",
    "        self.att2_fc2 = nn.Linear(h_att2, self.mem_dim)\n",
    "        self.att2_dropout = nn.Dropout(att2_dropout)\n",
    "\n",
    "        self.gamma1_fc1 = nn.Linear(gammaInShape, h_gamma1)\n",
    "        self.gamma1_fc2 = nn.Linear(h_gamma1, self.mem_dim)\n",
    "        self.gamma1_dropout = nn.Dropout(gamma1_dropout)\n",
    "\n",
    "        self.gamma2_fc1 = nn.Linear(gammaInShape, h_gamma2)\n",
    "        self.gamma2_fc2 = nn.Linear(h_gamma2, self.mem_dim)\n",
    "        self.gamma2_dropout = nn.Dropout(gamma2_dropout)\n",
    "\n",
    "        self.out_fc1 = nn.Linear(final_out, h_out)\n",
    "        self.out_fc2 = nn.Linear(h_out, output_dim)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_l = x[:,:,:self.d_l]\n",
    "        x_a = x[:,:,self.d_l:self.d_l+self.d_a]\n",
    "        # x is t x n x d\n",
    "        n = x.shape[1]\n",
    "        t = x.shape[0]\n",
    "        self.h_l = torch.zeros(n, self.dh_l)\n",
    "        self.h_a = torch.zeros(n, self.dh_a)\n",
    "\n",
    "        self.c_l = torch.zeros(n, self.dh_l)\n",
    "        self.c_a = torch.zeros(n, self.dh_a)\n",
    "        \n",
    "        self.mem = torch.zeros(n, self.mem_dim)\n",
    "        all_h_ls = []\n",
    "        all_h_as = []\n",
    "\n",
    "        all_c_ls = []\n",
    "        all_c_as = []\n",
    "\n",
    "        all_mems = []\n",
    "        for i in range(t):\n",
    "            # prev time step\n",
    "            prev_h_l = self.h_l\n",
    "            prev_h_a = self.h_a\n",
    "\n",
    "            # curr time step\n",
    "            new_h_l = self.lstm_l(x_l[i], self.h_l)\n",
    "            new_h_a = self.lstm_a(x_a[i], self.h_a)\n",
    "   \n",
    "            # concatenate\n",
    "            prev_cs = torch.cat([prev_h_l,prev_h_a], dim=1)\n",
    "            new_cs = torch.cat([new_h_l,new_h_a], dim=1)\n",
    "            \n",
    "            cStar = torch.cat([prev_cs,new_cs], dim=1)\n",
    "            attention = F.softmax(self.att1_fc2(self.att1_dropout(F.relu(self.att1_fc1(cStar)))),dim=1)\n",
    "            attended = attention*cStar\n",
    "            \n",
    "            cHat = F.tanh(self.att2_fc2(self.att2_dropout(F.relu(self.att2_fc1(attended)))))\n",
    "            \n",
    "            both = torch.cat([attended,self.mem], dim=1)\n",
    "            gamma1 = F.sigmoid(self.gamma1_fc2(self.gamma1_dropout(F.relu(self.gamma1_fc1(both)))))\n",
    "            gamma2 = F.sigmoid(self.gamma2_fc2(self.gamma2_dropout(F.relu(self.gamma2_fc1(both)))))\n",
    "            \n",
    "            self.mem = gamma1*self.mem + gamma2*cHat\n",
    "            all_mems.append(self.mem)\n",
    "            # update\n",
    "            self.h_l = new_h_l\n",
    "            self.h_a = new_h_a\n",
    "\n",
    "            all_h_ls.append(self.h_l)\n",
    "            all_h_as.append(self.h_a)\n",
    "\n",
    "        # last hidden layer last_hs is n x h\n",
    "        last_h_l = all_h_ls[-1]\n",
    "        last_h_a = all_h_as[-1]\n",
    "\n",
    "        last_mem = all_mems[-1]\n",
    "        last_hs = torch.cat([last_h_l,last_h_a,last_mem], dim=1)\n",
    "        output = self.out_fc2(self.out_dropout(F.relu(self.out_fc1(last_hs))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 40.60454813639323 33.560943603515625 saving model\n",
      "1 40.45938491821289 33.44845199584961 saving model\n",
      "2 40.39717356363932 33.322845458984375 saving model\n",
      "3 40.2489865620931 33.1762809753418 saving model\n",
      "4 40.09260940551758 33.000648498535156 saving model\n",
      "5 39.98527399698893 32.798492431640625 saving model\n",
      "6 39.834634145100914 32.57582473754883 saving model\n",
      "7 39.73832321166992 32.35267639160156 saving model\n",
      "8 39.538394927978516 32.13829803466797 saving model\n",
      "9 39.270407358805336 31.897539138793945 saving model\n",
      "10 38.85156504313151 31.603099822998047 saving model\n",
      "11 38.20111338297526 31.230321884155273 saving model\n",
      "12 37.498269399007164 30.785438537597656 saving model\n",
      "13 36.54711659749349 30.396337509155273 saving model\n",
      "14 35.46378644307455 30.359203338623047 saving model\n",
      "15 34.40589841206869 30.790096282958984\n",
      "16 33.484090169270836 30.6092472076416\n",
      "17 32.17392794291178 29.768112182617188 saving model\n",
      "18 31.05070622762044 29.419456481933594 saving model\n",
      "19 29.59978739420573 29.70461082458496\n",
      "20 28.470988591512043 30.458120346069336\n",
      "21 28.200339635213215 31.783567428588867\n",
      "22 26.373106002807617 33.06891632080078\n",
      "23 25.603479385375977 34.11183166503906\n",
      "Epoch    25: reducing learning rate of group 0 to 2.5000e-04.\n",
      "24 24.979630788167317 35.22492980957031\n",
      "25 23.35496457417806 35.51237106323242\n",
      "26 23.157631556193035 35.60469436645508\n",
      "27 21.764314651489258 35.699913024902344\n",
      "28 21.99108123779297 35.903499603271484\n",
      "29 21.082369486490887 36.04659652709961\n",
      "Epoch    31: reducing learning rate of group 0 to 1.2500e-04.\n",
      "30 21.131590525309246 36.123191833496094\n",
      "31 20.19806416829427 36.076759338378906\n",
      "32 20.960779190063477 35.878414154052734\n",
      "33 20.238162358601887 35.632198333740234\n",
      "34 19.19900385538737 35.54792022705078\n",
      "35 18.6112060546875 35.44887924194336\n",
      "Epoch    37: reducing learning rate of group 0 to 6.2500e-05.\n",
      "36 18.883814493815105 35.28998947143555\n",
      "37 18.793636957804363 35.25437927246094\n",
      "38 18.95562744140625 35.28342056274414\n",
      "39 18.8691832224528 35.31825637817383\n",
      "40 18.3810240427653 35.32263946533203\n",
      "41 18.413851420084637 35.28135681152344\n",
      "Epoch    43: reducing learning rate of group 0 to 3.1250e-05.\n",
      "42 18.654054005940754 35.204139709472656\n",
      "43 17.928094228108723 35.13978958129883\n",
      "44 17.842756271362305 35.06911849975586\n",
      "45 17.606733004252117 34.9840202331543\n",
      "46 17.9002685546875 34.93014144897461\n",
      "47 18.512575149536133 34.916812896728516\n",
      "Epoch    49: reducing learning rate of group 0 to 1.5625e-05.\n",
      "48 17.737316131591797 34.91615676879883\n",
      "49 17.618024190266926 34.92327880859375\n",
      "50 17.854656219482422 34.92649841308594\n",
      "51 17.398502667744953 34.92752456665039\n",
      "52 18.041866302490234 34.91773986816406\n",
      "53 17.077046712239582 34.90156555175781\n",
      "Epoch    55: reducing learning rate of group 0 to 7.8125e-06.\n",
      "54 17.46743106842041 34.891170501708984\n",
      "55 17.125107129414875 34.889156341552734\n",
      "56 17.618361790974934 34.894630432128906\n",
      "57 17.3378324508667 34.90009307861328\n",
      "58 17.634220759073894 34.90151596069336\n",
      "59 17.09853458404541 34.895164489746094\n",
      "Epoch    61: reducing learning rate of group 0 to 3.9063e-06.\n",
      "60 17.346492449442547 34.88951873779297\n",
      "61 17.36678473154704 34.88713836669922\n",
      "62 17.302602132161457 34.88677978515625\n",
      "63 17.141557057698567 34.888526916503906\n",
      "64 16.769163449605305 34.890010833740234\n",
      "65 17.236217816670734 34.88856887817383\n",
      "Epoch    67: reducing learning rate of group 0 to 1.9531e-06.\n",
      "66 16.68824513753255 34.8845100402832\n",
      "67 17.345942815144856 34.881507873535156\n",
      "68 17.143922487894695 34.8773307800293\n",
      "69 17.38358275095622 34.87218475341797\n",
      "70 17.176180521647137 34.867679595947266\n",
      "71 16.28306293487549 34.86539077758789\n",
      "Epoch    73: reducing learning rate of group 0 to 9.7656e-07.\n",
      "72 17.459092775980633 34.864383697509766\n",
      "73 16.422141075134277 34.86451721191406\n",
      "74 17.694940249125164 34.86469650268555\n",
      "75 17.246398607889812 34.86458206176758\n",
      "76 17.575110117594402 34.86418151855469\n",
      "77 17.80728530883789 34.863487243652344\n",
      "Epoch    79: reducing learning rate of group 0 to 4.8828e-07.\n",
      "78 17.408924738566082 34.862491607666016\n",
      "79 17.14103412628174 34.86200714111328\n",
      "mae:  4.199789772275066\n",
      "mse:  26.882487046177097\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [4, 47]\n",
    "hl = 256\n",
    "ha = 256\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = hl\n",
    "config[\"num_epochs\"] = 80\n",
    "config[\"lr\"] = 0.0005\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = hl\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train[:,:,1:], label_train, last_dev[:,:,1:], label_dev, last_test[:,:,1:], label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFN(nn.Module):\n",
    "    def __init__(self,config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig):\n",
    "        super(MFN, self).__init__()\n",
    "        [self.d_l,self.d_a] = config[\"input_dims\"]\n",
    "        [self.dh_l,self.dh_a] = config[\"h_dims\"]\n",
    "        total_h_dim = self.dh_l+self.dh_a\n",
    "        \n",
    "        self.mem_dim = config[\"memsize\"]\n",
    "        window_dim = config[\"windowsize\"]\n",
    "        output_dim = 1\n",
    "        attInShape = total_h_dim*window_dim\n",
    "        gammaInShape = attInShape+self.mem_dim\n",
    "        final_out = total_h_dim+self.mem_dim\n",
    "        h_att1 = NN1Config[\"shapes\"]\n",
    "        h_att2 = NN2Config[\"shapes\"]\n",
    "        h_gamma1 = gamma1Config[\"shapes\"]\n",
    "        h_gamma2 = gamma2Config[\"shapes\"]\n",
    "        h_out = outConfig[\"shapes\"]\n",
    "        att1_dropout = NN1Config[\"drop\"]\n",
    "        att2_dropout = NN2Config[\"drop\"]\n",
    "        gamma1_dropout = gamma1Config[\"drop\"]\n",
    "        gamma2_dropout = gamma2Config[\"drop\"]\n",
    "        out_dropout = outConfig[\"drop\"]\n",
    "\n",
    "        self.lstm_l = nn.LSTMCell(self.d_l, self.dh_l)\n",
    "        self.lstm_a = nn.LSTMCell(self.d_a, self.dh_a)\n",
    "\n",
    "        self.att1_fc1 = nn.Linear(attInShape, h_att1)\n",
    "        self.att1_fc2 = nn.Linear(h_att1, attInShape)\n",
    "        self.att1_dropout = nn.Dropout(att1_dropout)\n",
    "\n",
    "        self.att2_fc1 = nn.Linear(attInShape, h_att2)\n",
    "        self.att2_fc2 = nn.Linear(h_att2, self.mem_dim)\n",
    "        self.att2_dropout = nn.Dropout(att2_dropout)\n",
    "\n",
    "        self.gamma1_fc1 = nn.Linear(gammaInShape, h_gamma1)\n",
    "        self.gamma1_fc2 = nn.Linear(h_gamma1, self.mem_dim)\n",
    "        self.gamma1_dropout = nn.Dropout(gamma1_dropout)\n",
    "\n",
    "        self.gamma2_fc1 = nn.Linear(gammaInShape, h_gamma2)\n",
    "        self.gamma2_fc2 = nn.Linear(h_gamma2, self.mem_dim)\n",
    "        self.gamma2_dropout = nn.Dropout(gamma2_dropout)\n",
    "\n",
    "        self.out_fc1 = nn.Linear(final_out, h_out)\n",
    "        self.out_fc2 = nn.Linear(h_out, output_dim)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_l = x[:,:,:self.d_l]\n",
    "        x_a = x[:,:,self.d_l:self.d_l+self.d_a]\n",
    "        # x is t x n x d\n",
    "        n = x.shape[1]\n",
    "        t = x.shape[0]\n",
    "        self.h_l = torch.zeros(n, self.dh_l)\n",
    "        self.h_a = torch.zeros(n, self.dh_a)\n",
    "\n",
    "        self.c_l = torch.zeros(n, self.dh_l)\n",
    "        self.c_a = torch.zeros(n, self.dh_a)\n",
    "        \n",
    "        self.mem = torch.zeros(n, self.mem_dim)\n",
    "        all_h_ls = []\n",
    "        all_h_as = []\n",
    "\n",
    "        all_c_ls = []\n",
    "        all_c_as = []\n",
    "\n",
    "        all_mems = []\n",
    "        for i in range(t):\n",
    "            # prev time step\n",
    "            prev_c_l = self.c_l\n",
    "            prev_c_a = self.c_a\n",
    "\n",
    "            # curr time step\n",
    "            new_h_l, new_c_l = self.lstm_l(x_l[i], (self.h_l, self.c_l))\n",
    "            new_h_a, new_c_a = self.lstm_a(x_a[i], (self.h_a, self.c_a))\n",
    "   \n",
    "            # concatenate\n",
    "            prev_cs = torch.cat([prev_c_l,prev_c_a], dim=1)\n",
    "            new_cs = torch.cat([new_c_l,new_c_a], dim=1)\n",
    "            \n",
    "            cStar = torch.cat([prev_cs,new_cs], dim=1)\n",
    "            attention = F.softmax(self.att1_fc2(self.att1_dropout(F.relu(self.att1_fc1(cStar)))),dim=1)\n",
    "            attended = attention*cStar\n",
    "            \n",
    "            cHat = F.tanh(self.att2_fc2(self.att2_dropout(F.relu(self.att2_fc1(attended)))))\n",
    "            \n",
    "            both = torch.cat([attended,self.mem], dim=1)\n",
    "            gamma1 = F.sigmoid(self.gamma1_fc2(self.gamma1_dropout(F.relu(self.gamma1_fc1(both)))))\n",
    "            gamma2 = F.sigmoid(self.gamma2_fc2(self.gamma2_dropout(F.relu(self.gamma2_fc1(both)))))\n",
    "            \n",
    "            self.mem = gamma1*self.mem + gamma2*cHat\n",
    "            all_mems.append(self.mem)\n",
    "            # update\n",
    "            self.h_l, self.c_l = new_h_l, new_c_l\n",
    "            self.h_a, self.c_a = new_h_a, new_c_a\n",
    "\n",
    "            all_h_ls.append(self.h_l)\n",
    "            all_h_as.append(self.h_a)\n",
    " \n",
    "            all_c_ls.append(self.c_l)\n",
    "            all_c_as.append(self.c_a)\n",
    "\n",
    "        # last hidden layer last_hs is n x h\n",
    "        last_h_l = all_h_ls[-1]\n",
    "        last_h_a = all_h_as[-1]\n",
    "\n",
    "        last_mem = all_mems[-1]\n",
    "        last_hs = torch.cat([last_h_l,last_h_a,last_mem], dim=1)\n",
    "        output = self.out_fc2(self.out_dropout(F.relu(self.out_fc1(last_hs))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 40.61702982584635 33.58317184448242 saving model\n",
      "1 40.529825846354164 33.52313232421875 saving model\n",
      "2 40.46124267578125 33.4550895690918 saving model\n",
      "3 40.396976470947266 33.37479019165039 saving model\n",
      "4 40.44697570800781 33.283809661865234 saving model\n",
      "5 40.35490417480469 33.17273712158203 saving model\n",
      "6 40.152960459391274 33.02576446533203 saving model\n",
      "7 40.03765741984049 32.84747314453125 saving model\n",
      "8 40.03054682413737 32.6578254699707 saving model\n",
      "9 39.78166198730469 32.43970489501953 saving model\n",
      "10 39.5495719909668 32.176422119140625 saving model\n",
      "11 39.222390492757164 31.82850456237793 saving model\n",
      "12 38.681479136149086 31.36661720275879 saving model\n",
      "13 38.073123931884766 30.80330467224121 saving model\n",
      "14 37.13399124145508 30.26140785217285 saving model\n",
      "15 35.839656829833984 29.96523094177246 saving model\n",
      "16 35.06632296244303 30.117525100708008\n",
      "17 34.04050064086914 29.39242172241211 saving model\n",
      "18 32.36841011047363 28.494014739990234 saving model\n",
      "19 31.4815616607666 28.24098777770996 saving model\n",
      "20 30.253300348917644 28.655166625976562\n",
      "21 28.115926106770832 29.444091796875\n",
      "22 27.359766642252605 30.393537521362305\n",
      "23 25.327234268188477 31.54915428161621\n",
      "24 24.973485946655273 32.05328369140625\n",
      "Epoch    26: reducing learning rate of group 0 to 2.5000e-04.\n",
      "25 23.68649164835612 32.476341247558594\n",
      "26 21.65366045633952 32.9128532409668\n",
      "27 21.99486796061198 33.02580261230469\n",
      "28 21.319088617960613 33.088279724121094\n",
      "29 20.199940999348957 33.577693939208984\n",
      "30 19.443925221761067 34.136146545410156\n",
      "Epoch    32: reducing learning rate of group 0 to 1.2500e-04.\n",
      "31 18.851264317830402 34.0244026184082\n",
      "32 18.55735206604004 34.17292404174805\n",
      "33 18.049237887064617 34.45302963256836\n",
      "34 19.047749201456707 34.56437683105469\n",
      "35 17.760504722595215 34.42831802368164\n",
      "36 17.011948585510254 34.444610595703125\n",
      "Epoch    38: reducing learning rate of group 0 to 6.2500e-05.\n",
      "37 17.58270486195882 34.57746505737305\n",
      "38 17.054690996805828 34.70949935913086\n",
      "39 16.53874937693278 34.81629180908203\n",
      "40 16.503737449645996 34.856658935546875\n",
      "41 16.911241213480633 34.92760467529297\n",
      "42 15.993006388346354 34.99337387084961\n",
      "Epoch    44: reducing learning rate of group 0 to 3.1250e-05.\n",
      "43 15.158431053161621 35.112266540527344\n",
      "44 16.314255078633625 35.201332092285156\n",
      "45 15.01516596476237 35.27155303955078\n",
      "46 15.45235538482666 35.344879150390625\n",
      "47 15.341867129007975 35.43636703491211\n",
      "48 15.13257090250651 35.52715301513672\n",
      "Epoch    50: reducing learning rate of group 0 to 1.5625e-05.\n",
      "49 15.36588986714681 35.57356262207031\n",
      "50 15.011205037434896 35.578224182128906\n",
      "51 15.512821833292643 35.58815383911133\n",
      "52 14.644233067830404 35.5984001159668\n",
      "53 15.262028376261393 35.61073303222656\n",
      "54 14.925596237182617 35.62654113769531\n",
      "Epoch    56: reducing learning rate of group 0 to 7.8125e-06.\n",
      "55 14.772518157958984 35.65494918823242\n",
      "56 15.773173332214355 35.667415618896484\n",
      "57 15.253423690795898 35.68256378173828\n",
      "58 14.914860407511393 35.70294189453125\n",
      "59 14.934848149617514 35.72398376464844\n",
      "60 15.295637130737305 35.747920989990234\n",
      "Epoch    62: reducing learning rate of group 0 to 3.9063e-06.\n",
      "61 15.250287373860678 35.77225112915039\n",
      "62 16.083418528238933 35.78192138671875\n",
      "63 15.28993034362793 35.79172897338867\n",
      "64 14.860077857971191 35.79767608642578\n",
      "65 14.648744901021322 35.79892349243164\n",
      "66 14.561261812845865 35.79484176635742\n",
      "Epoch    68: reducing learning rate of group 0 to 1.9531e-06.\n",
      "67 14.773842811584473 35.794776916503906\n",
      "68 14.4751828511556 35.79669189453125\n",
      "69 16.09131145477295 35.79819869995117\n",
      "70 15.464254379272461 35.79801940917969\n",
      "71 15.238080660502115 35.799354553222656\n",
      "72 14.686972935994467 35.802459716796875\n",
      "Epoch    74: reducing learning rate of group 0 to 9.7656e-07.\n",
      "73 15.675834973653158 35.80519104003906\n",
      "74 15.087560017903646 35.80655288696289\n",
      "75 15.137493451436361 35.80741500854492\n",
      "76 14.549830118815104 35.80912780761719\n",
      "77 14.896890004475912 35.8105354309082\n",
      "78 14.883946736653646 35.81232452392578\n",
      "Epoch    80: reducing learning rate of group 0 to 4.8828e-07.\n",
      "79 14.729421933492025 35.814056396484375\n",
      "mae:  4.285850610516288\n",
      "mse:  27.68595491054094\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [4, 47]\n",
    "hl = 256\n",
    "ha = 256\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = hl\n",
    "config[\"num_epochs\"] = 80\n",
    "config[\"lr\"] = 0.0005\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = hl\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train[:,:,1:], label_train, last_dev[:,:,1:], label_dev, last_test[:,:,1:], label_test, configs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
