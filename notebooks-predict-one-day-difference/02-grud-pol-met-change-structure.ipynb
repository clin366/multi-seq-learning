{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/predict-one-day-diff/pol-met-search'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays\n",
    "\n",
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]\n",
    "\n",
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize delta\n",
    "# we can normlize this because it's know features \n",
    "delta_seq = (delta_seq - np.mean(delta_seq)) / np.std(delta_seq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train, masking_train, delta_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq, masking_seq, delta_seq\n",
    "                                                                 ], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev, masking_dev, delta_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq, masking_seq, delta_seq\n",
    "                                                           ], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test, masking_test, delta_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq, masking_seq, delta_seq\n",
    "                                                              ], label_seq, test_index)\n",
    "\n",
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])\n",
    "\n",
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, filter_square_matrix, bias=True):\n",
    "        '''\n",
    "        filter_square_matrix : filter square matrix, whose each elements is 0 or 1.\n",
    "        '''\n",
    "        super(FilterLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        self.filter_square_matrix = None\n",
    "        if use_gpu:\n",
    "            self.filter_square_matrix = Variable(filter_square_matrix.cuda(), requires_grad=False)\n",
    "        else:\n",
    "            self.filter_square_matrix = Variable(filter_square_matrix, requires_grad=False)\n",
    "        \n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "#         print(self.weight.data)\n",
    "#         print(self.bias.data)\n",
    "\n",
    "    def forward(self, input):\n",
    "#         print(self.filter_square_matrix.mul(self.weight))\n",
    "        return F.linear(input, self.filter_square_matrix.mul(self.weight), self.bias)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) \\\n",
    "            + ', bias=' + str(self.bias is not None) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset for grud \n",
    "def dataset_aggregation(feature_array, last_obsv, mask, delta):\n",
    "    # expand dimension of array\n",
    "    def expd(arr):\n",
    "        return np.expand_dims(arr, axis=1)\n",
    "    return np.concatenate((expd(feature_array), expd(last_obsv), expd(mask), expd(delta)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_aggregation for train, dev, test \n",
    "# train_aggr = dataset_aggregation(fea_train, last_train, masking_train, delta_train)\n",
    "# pol and met \n",
    "train_aggr = dataset_aggregation(last_train[:,:,0:5], \n",
    "                                 last_train[:,:,0:5], masking_train[:,:,0:5], delta_train[:,:,0:5])\n",
    "# dev_aggr = dataset_aggregation(fea_dev, last_dev, masking_dev, delta_dev)\n",
    "# test_aggr = dataset_aggregation(fea_test, last_test, masking_test, delta_test)\n",
    "dev_aggr = dataset_aggregation(last_dev[:,:,0:5], \n",
    "                               last_dev[:,:,0:5], masking_dev[:,:,0:5], delta_dev[:,:,0:5])\n",
    "test_aggr = dataset_aggregation(last_test[:,:,0:5], \n",
    "                                last_test[:,:,0:5], masking_test[:,:,0:5], delta_test[:,:,0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_grud(X_train, y_train, X_valid, y_valid, X_test, y_test, config, x_mean_aft_nor, dropout = 0):\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "#     def swap_axes(nparr):\n",
    "#         return nparr.swapaxes(0,1)\n",
    "#     X_train = swap_axes(X_train)\n",
    "#     X_valid = swap_axes(X_valid)\n",
    "#     X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters\n",
    "    input_size = X_train.shape[3]\n",
    "    h = config[\"h\"]\n",
    "    t = X_train.shape[2]\n",
    "    output_dim = 1\n",
    "    dropout = config[\"drop\"]\n",
    "    \n",
    "    model = GRUD(input_size, h, x_mean_aft_nor, output_last = True, dropout=dropout)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[0]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[start:end, :])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches \n",
    "    \n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    # timing\n",
    "#     start_time = time.time()\n",
    "#     predictions = predict(model, X_test)\n",
    "#     print(predictions.shape)\n",
    "#     print(predictions)\n",
    "#     end_time = time.time()\n",
    "#     print(end_time-start_time)\n",
    "#     assert False\n",
    "     \n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/lstm_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "    model = torch.load('models/lstm_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)\n",
    "#     corr = np.corrcoef(predictions,y_test)[0][1]\n",
    "#     print(\"corr: \", corr)\n",
    "#     true_label = (y_test >= 0)\n",
    "#     sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 4, 7, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aggr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean_aft_nor = np.expand_dims(x_mean_aft_nor, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean_aft_nor[:,:,0:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUD(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, X_mean, output_last = True, dropout=0):\n",
    "        \"\"\"\n",
    "        Recurrent Neural Networks for Multivariate Times Series with Missing Values\n",
    "        GRU-D: GRU exploit two representations of informative missingness patterns, i.e., masking and time interval.\n",
    "        cell_size is the size of cell_state.\n",
    "        \n",
    "        GRU-D:\n",
    "            input_size: variable dimension of each time\n",
    "            hidden_size: dimension of hidden_state\n",
    "            mask_size: dimension of masking vector\n",
    "            X_mean: the mean of the historical input data\n",
    "        \"\"\"\n",
    "        \n",
    "        super(GRUD, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.delta_size = input_size\n",
    "        self.mask_size = input_size\n",
    "        \n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            self.identity = torch.eye(input_size).cuda()\n",
    "            self.zeros = Variable(torch.zeros(input_size).cuda())\n",
    "            self.X_mean = Variable(torch.Tensor(X_mean).cuda())\n",
    "        else:\n",
    "            self.identity = torch.eye(input_size)\n",
    "            self.zeros = Variable(torch.zeros(input_size))\n",
    "            self.X_mean = Variable(torch.Tensor(X_mean))\n",
    "        \n",
    "        self.zl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size)\n",
    "        self.rl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size)\n",
    "        self.hl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size)\n",
    "        \n",
    "        self.gamma_x_l = FilterLinear(self.delta_size, self.delta_size, self.identity)\n",
    "        \n",
    "        self.gamma_h_l = nn.Linear(self.delta_size, input_size)\n",
    "        \n",
    "        self.map_hidden_gamma = nn.Linear(input_size, 1)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def step(self, x, x_last_obsv, x_mean, h, mask, delta):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        dim_size = x.shape[1]\n",
    "        \n",
    "        delta_x = torch.exp(-torch.max(self.zeros, self.gamma_x_l(delta)))\n",
    "    \n",
    "        x = mask * x + (1 - mask) * (delta_x * x_last_obsv + (1 - delta_x) * x_mean)\n",
    "        \n",
    "        combined = torch.cat((x, h, mask), 1)\n",
    "        z = torch.sigmoid(self.zl(combined))\n",
    "        r = torch.sigmoid(self.rl(combined))\n",
    "        combined_r = torch.cat((x, r * h, mask), 1)\n",
    "        h_tilde = torch.tanh(self.hl(combined_r))\n",
    "        h = (1 - z) * h + z * h_tilde\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        type_size = input.size(1)\n",
    "        step_size = input.size(2)\n",
    "        spatial_size = input.size(3)\n",
    "        \n",
    "        Hidden_State = self.initHidden(batch_size)\n",
    "        \n",
    "        def squeeze_d1(matrix):\n",
    "            return torch.squeeze(matrix, dim=1)\n",
    "        X = squeeze_d1(input[:,0,:,:])\n",
    "        X_last_obsv = squeeze_d1(input[:,1,:,:])\n",
    "        Mask = squeeze_d1(input[:,2,:,:])\n",
    "        Delta = squeeze_d1(input[:,3,:,:])\n",
    "        \n",
    "        outputs = None\n",
    "        for i in range(step_size):\n",
    "            Hidden_State = self.step(squeeze_d1(X[:,i:i+1,:])\\\n",
    "                                     , squeeze_d1(X_last_obsv[:,i:i+1,:])\\\n",
    "                                     , squeeze_d1(self.X_mean[:,i:i+1,:])\\\n",
    "                                     , Hidden_State\\\n",
    "                                     , squeeze_d1(Mask[:,i:i+1,:])\\\n",
    "                                     , squeeze_d1(Delta[:,i:i+1,:]))\n",
    "            if outputs is None:\n",
    "                outputs = Hidden_State.unsqueeze(1)\n",
    "            else:\n",
    "                outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "                \n",
    "        if self.output_last:\n",
    "            last_hs = outputs[:,-1,:]\n",
    "            output = F.relu(self.fc1(last_hs))\n",
    "            output = self.dropout(output)\n",
    "            output = self.fc2(output)\n",
    "            return output\n",
    "        else:\n",
    "            raise Exception(\"Not output last\")\n",
    "\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            return Hidden_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            return Hidden_State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 39.1354892821539 27.73879623413086 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type GRUD. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type FilterLinear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 31.859557832990372 20.308773040771484 saving model\n",
      "2 26.215213139851887 19.68356704711914 saving model\n",
      "3 26.5877141498384 18.73686981201172 saving model\n",
      "4 24.221855663117907 17.836490631103516 saving model\n",
      "5 23.8710511525472 17.857942581176758\n",
      "6 22.695493107750302 17.79047393798828 saving model\n",
      "7 22.450421106247674 17.765544891357422 saving model\n",
      "8 22.2658657346453 17.71568489074707 saving model\n",
      "9 22.250603176298597 17.537153244018555 saving model\n",
      "10 21.92252613249279 17.35944175720215 saving model\n",
      "11 21.75543685186477 17.124847412109375 saving model\n",
      "12 21.758784521193732 17.182846069335938\n",
      "13 20.843877065749396 17.347431182861328\n",
      "14 21.440978776840936 16.955883026123047 saving model\n",
      "15 20.711157844180153 17.308639526367188\n",
      "16 20.059633255004883 17.224584579467773\n",
      "17 19.68612779889788 17.209074020385742\n",
      "18 19.91121024177188 16.88689422607422 saving model\n",
      "19 19.52487591334752 17.322040557861328\n",
      "20 18.648843401954288 17.19211769104004\n",
      "21 19.16448488689604 17.01597785949707\n",
      "22 18.51030722118559 17.55048179626465\n",
      "23 18.668449538094656 17.38701629638672\n",
      "24 18.560858862740652 17.168453216552734\n",
      "25 17.844378335135325 17.578676223754883\n",
      "26 17.674565496898833 17.339622497558594\n",
      "27 17.491035143534344 17.604446411132812\n",
      "28 17.236543428330194 17.56910514831543\n",
      "Epoch    30: reducing learning rate of group 0 to 5.0000e-04.\n",
      "29 17.199575923738024 17.944660186767578\n",
      "30 16.48596277691069 18.284311294555664\n",
      "31 16.616107168651762 17.66095733642578\n",
      "32 16.57380755742391 17.650394439697266\n",
      "33 16.553922358013335 17.851699829101562\n",
      "34 16.306604521615164 18.16512107849121\n",
      "35 15.747608525412423 18.04299545288086\n",
      "36 15.480209486825126 18.536006927490234\n",
      "37 15.944685209365119 18.37116241455078\n",
      "38 15.55331005368914 18.48798179626465\n",
      "39 15.55501551855178 18.276395797729492\n",
      "Epoch    41: reducing learning rate of group 0 to 2.5000e-04.\n",
      "40 15.15691866193499 18.520410537719727\n",
      "41 15.21908153806414 18.4433536529541\n",
      "42 14.782790774390811 18.71393585205078\n",
      "43 14.911714780898322 18.524248123168945\n",
      "44 15.229808444068546 18.806243896484375\n",
      "45 14.868220261165074 18.561323165893555\n",
      "46 14.990002950032553 18.67978286743164\n",
      "47 15.097883406139555 18.720762252807617\n",
      "48 14.83734891528175 18.871723175048828\n",
      "49 14.856166794186546 18.759414672851562\n",
      "50 14.595248335883731 18.70652198791504\n",
      "Epoch    52: reducing learning rate of group 0 to 1.2500e-04.\n",
      "51 14.523150012606667 18.7772159576416\n",
      "52 14.668034122103737 18.96906852722168\n",
      "53 14.486762569064187 18.887300491333008\n",
      "54 14.189248834337507 19.156967163085938\n",
      "55 14.367970670972552 19.15032196044922\n",
      "56 14.701299122401647 18.917037963867188\n",
      "57 14.157642001197452 19.03871726989746\n",
      "58 14.040584859393892 19.107295989990234\n",
      "59 14.339425404866537 18.98214340209961\n",
      "60 14.21664147149949 19.150074005126953\n",
      "61 14.01491592043922 19.311199188232422\n",
      "Epoch    63: reducing learning rate of group 0 to 6.2500e-05.\n",
      "62 14.658422515505837 19.33848762512207\n",
      "63 13.779587654840379 19.34512710571289\n",
      "64 13.89745603288923 19.189651489257812\n",
      "65 13.940576303572882 19.23617172241211\n",
      "66 13.86680807386126 19.283893585205078\n",
      "67 14.205118179321289 19.252239227294922\n",
      "68 14.134384314219156 19.354768753051758\n",
      "69 14.020682289486839 19.414981842041016\n",
      "70 13.751543567294167 19.41025161743164\n",
      "71 13.826981181190128 19.45288848876953\n",
      "72 13.704653467450823 19.392147064208984\n",
      "Epoch    74: reducing learning rate of group 0 to 3.1250e-05.\n",
      "73 13.53978917712257 19.477203369140625\n",
      "74 13.88780827749343 19.482681274414062\n",
      "75 14.269248462858654 19.498916625976562\n",
      "76 13.648924804869152 19.53108024597168\n",
      "77 13.944894404638381 19.502145767211914\n",
      "78 13.523620809827532 19.507158279418945\n",
      "79 14.207030364445277 19.54086685180664\n",
      "80 13.640884967077346 19.605241775512695\n",
      "81 13.943462780543737 19.556861877441406\n",
      "82 13.902629897707985 19.51900863647461\n",
      "83 13.80886509304955 19.488126754760742\n",
      "Epoch    85: reducing learning rate of group 0 to 1.5625e-05.\n",
      "84 13.35551745550973 19.49272918701172\n",
      "85 13.72758776800973 19.531057357788086\n",
      "86 13.75236481711978 19.540481567382812\n",
      "87 14.105175154549736 19.549320220947266\n",
      "88 13.79312971660069 19.54745101928711\n",
      "89 14.143774509429932 19.50095558166504\n",
      "90 14.137315795535134 19.49451446533203\n",
      "91 13.608760129837762 19.50484848022461\n",
      "92 13.817790712629046 19.494604110717773\n",
      "93 13.573470342726935 19.529558181762695\n",
      "94 13.432706832885742 19.562686920166016\n",
      "Epoch    96: reducing learning rate of group 0 to 7.8125e-06.\n",
      "95 13.716910135178338 19.534122467041016\n",
      "96 13.830126330966042 19.546518325805664\n",
      "97 13.757348174140567 19.5523738861084\n",
      "98 13.62349696386428 19.56864356994629\n",
      "99 14.086035864693779 19.549911499023438\n",
      "mae:  3.1334580662575635\n",
      "mse:  14.989891529689226\n"
     ]
    }
   ],
   "source": [
    "# pol and met # structure one\n",
    "config = {'h':128, 'lr':0.001, 'num_epochs':100, 'batchsize':32, 'drop':0.5}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_grud(train_aggr, label_train, dev_aggr, label_dev, test_aggr, label_test, config, x_mean_aft_nor[:,:,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 39.67365242186047 29.49156379699707 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type GRUD. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type FilterLinear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 33.212131727309455 19.959796905517578 saving model\n",
      "2 27.969510078430176 17.87363052368164 saving model\n",
      "3 26.164809408641997 18.966472625732422\n",
      "4 24.425521123976935 17.514774322509766 saving model\n",
      "5 24.32747404915946 17.448461532592773 saving model\n",
      "6 22.880385943821498 17.30282974243164 saving model\n",
      "7 22.904385930015927 17.016151428222656 saving model\n",
      "8 22.659231458391464 16.945783615112305 saving model\n",
      "9 22.605987412588938 16.628305435180664 saving model\n",
      "10 22.281026386079333 16.504920959472656 saving model\n",
      "11 22.29319890340169 16.37874984741211 saving model\n",
      "12 22.12033766791934 16.241493225097656 saving model\n",
      "13 21.247778165908088 16.185623168945312 saving model\n",
      "14 21.973103841145832 16.27969741821289\n",
      "15 21.031504767281668 16.048723220825195 saving model\n",
      "16 20.93706871214367 16.229759216308594\n",
      "17 20.408146358671644 16.00263023376465 saving model\n",
      "18 20.87461235409691 15.986905097961426 saving model\n",
      "19 20.44986997331892 16.121902465820312\n",
      "20 19.544604573931014 16.23101234436035\n",
      "21 20.224775132678804 16.12818145751953\n",
      "22 19.952630587986537 16.147254943847656\n",
      "23 19.748627435593377 16.34134864807129\n",
      "24 19.442306336902437 16.10890769958496\n",
      "25 19.289443288530624 16.558897018432617\n",
      "26 18.829339345296223 16.422882080078125\n",
      "27 18.93652713866461 16.43381118774414\n",
      "28 18.57939842769078 16.34589958190918\n",
      "Epoch    30: reducing learning rate of group 0 to 5.0000e-04.\n",
      "29 18.599735714140394 16.703609466552734\n",
      "30 18.09342179979597 16.864728927612305\n",
      "31 18.48820463816325 16.688833236694336\n",
      "32 18.21622893923805 16.60853385925293\n",
      "33 18.235145523434593 16.590652465820312\n",
      "34 18.08264952614194 16.650968551635742\n",
      "35 17.68016906011672 16.75601577758789\n",
      "36 17.395281655447825 16.938430786132812\n",
      "37 17.940963313693093 16.786867141723633\n",
      "38 17.60057851246425 17.02054786682129\n",
      "39 17.308764253343856 16.707202911376953\n",
      "Epoch    41: reducing learning rate of group 0 to 2.5000e-04.\n",
      "40 17.387906437828427 16.88490104675293\n",
      "41 17.347234816778276 16.905410766601562\n",
      "42 16.99785495939709 17.04698371887207\n",
      "43 17.223113832019624 16.964073181152344\n",
      "44 17.24106134687151 17.0013484954834\n",
      "45 17.049826394943963 16.978382110595703\n",
      "46 17.110187371571858 17.127872467041016\n",
      "47 17.357560203188942 17.014619827270508\n",
      "48 17.26366476785569 17.054725646972656\n",
      "49 17.382435866764613 17.148269653320312\n",
      "50 16.782085827418737 17.135393142700195\n",
      "Epoch    52: reducing learning rate of group 0 to 1.2500e-04.\n",
      "51 16.642342204139347 17.170228958129883\n",
      "52 16.87782135463896 17.227191925048828\n",
      "53 17.117565790812176 17.203876495361328\n",
      "54 16.89991721652803 17.277389526367188\n",
      "55 16.92574605487642 17.367841720581055\n",
      "56 17.21286492120652 17.25809669494629\n",
      "57 16.696599778674898 17.226932525634766\n",
      "58 16.70820912860689 17.247713088989258\n",
      "59 16.923254648844402 17.213191986083984\n",
      "60 17.001199358985538 17.18785858154297\n",
      "61 16.52728591646467 17.277889251708984\n",
      "Epoch    63: reducing learning rate of group 0 to 6.2500e-05.\n",
      "62 17.065956433614094 17.41162109375\n",
      "63 16.355951172964915 17.408851623535156\n",
      "64 16.65012925011771 17.35671615600586\n",
      "65 16.82580652691069 17.334829330444336\n",
      "66 16.56178819565546 17.36220359802246\n",
      "67 16.881091958000546 17.339792251586914\n",
      "68 16.760726928710938 17.345703125\n",
      "69 16.934892154875257 17.380937576293945\n",
      "70 16.41921715509324 17.387035369873047\n",
      "71 16.71277077992757 17.408952713012695\n",
      "72 16.744626453944615 17.362186431884766\n",
      "Epoch    74: reducing learning rate of group 0 to 3.1250e-05.\n",
      "73 16.267048336210706 17.382038116455078\n",
      "74 16.909180391402472 17.381572723388672\n",
      "75 16.719692139398482 17.385391235351562\n",
      "76 16.518727597736177 17.395545959472656\n",
      "77 16.882625034877233 17.40315055847168\n",
      "78 16.356622582390195 17.40981674194336\n",
      "79 16.923277400788805 17.411319732666016\n",
      "80 16.301586764199392 17.429683685302734\n",
      "81 16.739971546899703 17.404869079589844\n",
      "82 16.846073604765394 17.381610870361328\n",
      "83 16.677380175817582 17.375259399414062\n",
      "Epoch    85: reducing learning rate of group 0 to 1.5625e-05.\n",
      "84 16.4322022937593 17.370588302612305\n",
      "85 16.37353315807524 17.381803512573242\n",
      "86 16.909516924903507 17.381607055664062\n",
      "87 16.768380937122163 17.376895904541016\n",
      "88 16.605585892995197 17.3748722076416\n",
      "89 16.790450141543435 17.367372512817383\n",
      "90 16.859574386051722 17.361934661865234\n",
      "91 16.519801684788295 17.368379592895508\n",
      "92 16.696709360395158 17.361562728881836\n",
      "93 16.388318039122083 17.37272834777832\n",
      "94 16.39737642379034 17.375944137573242\n",
      "Epoch    96: reducing learning rate of group 0 to 7.8125e-06.\n",
      "95 16.37553339912778 17.376541137695312\n",
      "96 16.566479365030926 17.38372039794922\n",
      "97 16.799448376610165 17.382301330566406\n",
      "98 16.372080689384823 17.387065887451172\n",
      "99 16.7908448718843 17.38004493713379\n",
      "mae:  3.0969965743624472\n",
      "mse:  14.628440694432053\n"
     ]
    }
   ],
   "source": [
    "# pol and met \n",
    "config = {'h':128, 'lr':0.001, 'num_epochs':100, 'batchsize':32, 'drop':0.5}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_grud(train_aggr, label_train, dev_aggr, label_dev, test_aggr, label_test, config, x_mean_aft_nor[:,:,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
