{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/predict-one-day-diff/pol-met-search'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])\n",
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control experiment using last observed value for missing data imputation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_dim, dropout):\n",
    "        \"\"\"\n",
    "        input_size - the number of expected features in the input x\n",
    "        hidden_size - the number of hidden units in state h\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.h = hidden_size\n",
    "        self.lstm = nn.GRUCell(input_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (time_step, n_features)\n",
    "        \"\"\"\n",
    "        t = x.shape[0]\n",
    "        n = x.shape[1]\n",
    "        self.hx = torch.zeros(n, self.h)\n",
    "        all_hs = []\n",
    "        all_cs = []\n",
    "        # iterate through cells \n",
    "        for i in range(t):\n",
    "            self.hx = self.lstm(x[i], self.hx)\n",
    "            all_hs.append(self.hx)\n",
    "        # last hidden layer last_hs is n * h\n",
    "        last_hs = all_hs[-1]\n",
    "        output = F.relu(self.fc1(last_hs))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "def train_lstm(X_train, y_train, X_valid, y_valid, X_test, y_test, config):\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "    def swap_axes(nparr):\n",
    "        return nparr.swapaxes(0,1)\n",
    "    X_train = swap_axes(X_train)\n",
    "    X_valid = swap_axes(X_valid)\n",
    "    X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters\n",
    "    input_size = X_train.shape[2]\n",
    "    h = config[\"h\"]\n",
    "    t = X_train.shape[0]\n",
    "    output_dim = 1\n",
    "    dropout = config[\"drop\"]\n",
    "    \n",
    "    model = LSTM(input_size, h, output_dim, dropout)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[1]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[:, start:end])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches \n",
    "    \n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    # timing\n",
    "#     start_time = time.time()\n",
    "#     predictions = predict(model, X_test)\n",
    "#     print(predictions.shape)\n",
    "#     print(predictions)\n",
    "#     end_time = time.time()\n",
    "#     print(end_time-start_time)\n",
    "#     assert False\n",
    "     \n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/lstm_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "    model = torch.load('models/lstm_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)\n",
    "#     corr = np.corrcoef(predictions,y_test)[0][1]\n",
    "#     print(\"corr: \", corr)\n",
    "#     true_label = (y_test >= 0)\n",
    "#     sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 7, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_train[:,:,1:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 41.09159106299991 33.07618713378906 saving model\n",
      "1 40.80134546189081 32.35593032836914 saving model\n",
      "2 40.069341750372026 31.087417602539062 saving model\n",
      "3 38.569817860921226 29.09696388244629 saving model\n",
      "4 36.835134097508025 27.24900245666504 saving model\n",
      "5 35.7380188533238 26.383819580078125 saving model\n",
      "6 35.17970557439895 25.530658721923828 saving model\n",
      "7 34.20080352964855 25.587596893310547\n",
      "8 33.50944137573242 25.346126556396484 saving model\n",
      "9 33.144762311662944 24.983318328857422 saving model\n",
      "10 32.8745824268886 24.91716766357422 saving model\n",
      "11 32.41738478342692 24.789276123046875 saving model\n",
      "12 32.537544159662154 24.948949813842773\n",
      "13 31.979722204662504 24.50566864013672 saving model\n",
      "14 32.244703928629555 24.574337005615234\n",
      "15 31.264505522591726 24.66516876220703\n",
      "16 31.065198580423992 24.321176528930664 saving model\n",
      "17 30.451274099804106 24.574617385864258\n",
      "18 30.8044954481579 24.771865844726562\n",
      "19 30.84473428272066 24.65963363647461\n",
      "20 30.864802814665296 25.010986328125\n",
      "21 30.059023993355886 24.31477165222168 saving model\n",
      "22 30.333499863034202 24.63889503479004\n",
      "23 30.37379787081764 24.80392837524414\n",
      "24 29.44587026323591 24.977035522460938\n",
      "25 28.89982795715332 25.075469970703125\n",
      "26 29.855049087887718 24.672725677490234\n",
      "27 28.610850197928293 25.03542709350586\n",
      "28 28.60471993400937 25.24334144592285\n",
      "29 27.9856055577596 26.436342239379883\n",
      "30 28.3467496690296 25.717985153198242\n",
      "31 28.509521802266438 25.66954803466797\n",
      "Epoch    33: reducing learning rate of group 0 to 5.0000e-04.\n",
      "32 28.452368145897275 25.93671417236328\n",
      "33 28.411934443882533 26.469036102294922\n",
      "34 27.256484667460125 26.08672523498535\n",
      "35 28.031518027895974 26.305097579956055\n",
      "36 27.99825345902216 26.062376022338867\n",
      "37 27.740018890017556 25.94923210144043\n",
      "38 27.493709927513486 26.116554260253906\n",
      "39 27.106569017682755 26.18050765991211\n",
      "40 27.212218920389812 26.89238739013672\n",
      "41 26.293927783057804 26.40629768371582\n",
      "42 27.34967227209182 26.530405044555664\n",
      "Epoch    44: reducing learning rate of group 0 to 2.5000e-04.\n",
      "43 26.198953038170224 26.65892219543457\n",
      "44 26.887281735738117 26.911226272583008\n",
      "45 25.95998818533761 27.18442726135254\n",
      "46 26.61669921875 27.209550857543945\n",
      "47 26.62057535988944 27.331350326538086\n",
      "48 26.41299443017869 27.260908126831055\n",
      "49 24.866470700218564 27.61222267150879\n",
      "50 26.46272563934326 27.517826080322266\n",
      "51 26.392214911324636 27.45922088623047\n",
      "52 25.288413728986466 27.29594612121582\n",
      "53 26.167148453848704 27.61263084411621\n",
      "Epoch    55: reducing learning rate of group 0 to 1.2500e-04.\n",
      "54 26.583922386169434 27.488937377929688\n",
      "55 25.86603151048933 27.49799346923828\n",
      "56 25.246460006350564 27.659330368041992\n",
      "57 25.269879613603866 27.617277145385742\n",
      "58 24.991500445774623 27.791194915771484\n",
      "59 25.354117257254465 27.863107681274414\n",
      "60 25.512992813473655 27.946941375732422\n",
      "61 24.635084379286994 27.876319885253906\n",
      "62 25.18542112622942 27.976118087768555\n",
      "63 24.808759870983305 27.918155670166016\n",
      "64 25.473797071547736 27.929841995239258\n",
      "Epoch    66: reducing learning rate of group 0 to 6.2500e-05.\n",
      "65 25.655162175496418 27.75876235961914\n",
      "66 25.080215999058314 27.772201538085938\n",
      "67 25.16516317640032 27.970287322998047\n",
      "68 24.762320881798153 27.995777130126953\n",
      "69 26.07408396402995 28.04598045349121\n",
      "70 25.309653554643905 28.077865600585938\n",
      "71 25.27538576580229 28.062498092651367\n",
      "72 25.785457156953356 27.8809757232666\n",
      "73 25.580011140732537 27.906068801879883\n",
      "74 24.52154763539632 27.936025619506836\n",
      "75 25.47730473109654 27.901430130004883\n",
      "Epoch    77: reducing learning rate of group 0 to 3.1250e-05.\n",
      "76 24.84029974256243 27.95638084411621\n",
      "77 24.48742843809582 28.02132225036621\n",
      "78 25.06789865947905 28.07291603088379\n",
      "79 25.14390450432187 28.125892639160156\n",
      "80 25.595125289190385 28.12786293029785\n",
      "81 25.00274776277088 28.117263793945312\n",
      "82 25.54742690495082 28.13358497619629\n",
      "83 25.75141225542341 28.129735946655273\n",
      "84 24.758116722106934 28.05933952331543\n",
      "85 25.099368549528577 27.997573852539062\n",
      "86 25.514770644051687 27.98821449279785\n",
      "Epoch    88: reducing learning rate of group 0 to 1.5625e-05.\n",
      "87 24.213029157547723 28.03649139404297\n",
      "88 24.084774380638486 28.07078742980957\n",
      "89 23.86340690794445 28.058534622192383\n",
      "90 24.293538911002024 28.061445236206055\n",
      "91 25.081965401059104 28.023893356323242\n",
      "92 24.65214643024263 28.045408248901367\n",
      "93 24.723048073904856 28.032161712646484\n",
      "94 23.802080608549574 28.050161361694336\n",
      "95 24.978762762887136 28.08171272277832\n",
      "96 25.20160897572835 28.071962356567383\n",
      "97 25.613195283072336 28.0600643157959\n",
      "Epoch    99: reducing learning rate of group 0 to 7.8125e-06.\n",
      "98 25.512395495460147 28.042844772338867\n",
      "99 25.06135127657936 28.0592098236084\n",
      "100 25.04641514732724 28.062246322631836\n",
      "101 25.192811965942383 28.067745208740234\n",
      "102 24.736912182399205 28.063152313232422\n",
      "103 25.232974506559827 28.07090187072754\n",
      "104 24.403336252485 28.080516815185547\n",
      "105 24.728308450608026 28.09563636779785\n",
      "106 25.204955827622186 28.09605598449707\n",
      "107 24.544811021713983 28.097593307495117\n",
      "108 24.830161412556965 28.11801528930664\n",
      "Epoch   110: reducing learning rate of group 0 to 3.9063e-06.\n",
      "109 25.7262354805356 28.130088806152344\n",
      "110 24.410407429649716 28.13337516784668\n",
      "111 24.599177360534668 28.13857078552246\n",
      "112 24.63215360187349 28.140565872192383\n",
      "113 25.310287566412065 28.13786506652832\n",
      "114 25.334852445693244 28.1358585357666\n",
      "115 24.928347860063827 28.140201568603516\n",
      "116 24.675358181908017 28.142032623291016\n",
      "117 24.584076109386626 28.151426315307617\n",
      "118 24.51395725068592 28.15863800048828\n",
      "119 25.351144154866535 28.165599822998047\n",
      "Epoch   121: reducing learning rate of group 0 to 1.9531e-06.\n",
      "120 24.349068096705846 28.167461395263672\n",
      "121 24.93078831263951 28.16971778869629\n",
      "122 24.52391815185547 28.165740966796875\n",
      "123 24.75547999427432 28.163284301757812\n",
      "124 25.2475342523484 28.15991973876953\n",
      "125 24.654151099068777 28.160566329956055\n",
      "126 23.77937089829218 28.161083221435547\n",
      "127 25.117648487999325 28.158906936645508\n",
      "128 24.622647785005114 28.15928840637207\n",
      "129 25.086805434454057 28.158153533935547\n",
      "130 24.095604169936408 28.1561279296875\n",
      "Epoch   132: reducing learning rate of group 0 to 9.7656e-07.\n",
      "131 24.23761817387172 28.15631675720215\n",
      "132 25.118729682195756 28.15532684326172\n",
      "133 26.164346558707102 28.15366554260254\n",
      "134 24.605786005655926 28.154266357421875\n",
      "135 25.862912541344052 28.154939651489258\n",
      "136 23.77967884427025 28.1564998626709\n",
      "137 25.578375271388463 28.155916213989258\n",
      "138 25.92185642605736 28.154178619384766\n",
      "139 24.68954867408389 28.154539108276367\n",
      "140 23.622032982962473 28.154544830322266\n",
      "141 23.990938845134917 28.1579647064209\n",
      "Epoch   143: reducing learning rate of group 0 to 4.8828e-07.\n",
      "142 24.398003850664413 28.161293029785156\n",
      "143 24.526247251601447 28.16157341003418\n",
      "144 25.0476040158953 28.162174224853516\n",
      "145 25.202736037118093 28.16231918334961\n",
      "146 25.376238414219447 28.162673950195312\n",
      "147 24.562063716706774 28.16260528564453\n",
      "148 25.31914638337635 28.162912368774414\n",
      "149 25.256309963407972 28.163549423217773\n",
      "mae:  3.7650087608009084\n",
      "mse:  23.20785972088223\n"
     ]
    }
   ],
   "source": [
    "# met \n",
    "config = {'h':128, 'lr':0.001, 'num_epochs':150, 'batchsize':32, 'drop':0.7}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train[:,:,1:5], label_train, last_dev[:,:,1:5], label_dev, last_test[:,:,1:5], label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 41.226154781523206 33.508792877197266 saving model\n",
      "1 40.854309263683504 33.313995361328125 saving model\n",
      "2 40.62112553914388 32.999267578125 saving model\n",
      "3 39.925202551342196 32.49979019165039 saving model\n",
      "4 39.056315013340544 32.13646697998047 saving model\n",
      "5 35.947140103294736 31.79549217224121 saving model\n",
      "6 31.169902710687545 32.542999267578125\n",
      "7 26.50968869527181 34.175052642822266\n",
      "8 23.220348176502046 34.171390533447266\n",
      "9 18.813694045657204 33.30949783325195\n",
      "10 15.745402699425107 35.11029815673828\n",
      "11 13.23677971249535 33.449249267578125\n",
      "12 11.91861867904663 33.66128921508789\n",
      "13 9.694603216080438 34.45226287841797\n",
      "14 10.093519142695836 34.142948150634766\n",
      "15 8.599857103256952 33.89802551269531\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0000e-04.\n",
      "16 7.089729729152861 34.558223724365234\n",
      "17 6.2807410671597435 35.77287292480469\n",
      "18 4.65983399890718 35.13926696777344\n",
      "19 4.835804786000933 34.67116165161133\n",
      "20 4.223460180418832 34.79456329345703\n",
      "21 4.621299107869466 34.97544479370117\n",
      "22 3.780789466131301 35.388084411621094\n",
      "23 4.127051614579701 34.645172119140625\n",
      "24 3.9343234925043014 35.06702423095703\n",
      "25 3.8044661169960383 34.704254150390625\n",
      "26 3.7783810184115456 34.97679138183594\n",
      "Epoch    28: reducing learning rate of group 0 to 2.5000e-04.\n",
      "27 3.103118141492208 35.31022262573242\n",
      "28 3.576260350999378 34.780128479003906\n",
      "29 2.548990113394601 34.98250198364258\n",
      "30 2.6670298860186623 35.43050765991211\n",
      "31 3.362748776163374 35.94206237792969\n",
      "32 2.2893700003623962 35.63413619995117\n",
      "33 2.9634063641230264 35.912593841552734\n",
      "34 2.4683044524419877 36.1831169128418\n",
      "35 2.3810943024499074 36.00720977783203\n",
      "36 2.2071859126999263 36.13100814819336\n",
      "37 2.2258896146501814 35.8430061340332\n",
      "Epoch    39: reducing learning rate of group 0 to 1.2500e-04.\n",
      "38 2.198205817313421 36.56439971923828\n",
      "39 2.333811036178044 36.78792953491211\n",
      "40 2.5059857425235568 36.641788482666016\n",
      "41 2.0826003750165305 36.683895111083984\n",
      "42 2.4543950869923545 36.70166778564453\n",
      "43 2.39750227474031 36.45170211791992\n",
      "44 2.2075324853261313 36.32063674926758\n",
      "45 2.0522702676909312 36.51510238647461\n",
      "46 2.029095476581937 36.56736373901367\n",
      "47 1.7935819923877716 36.672054290771484\n",
      "48 2.2024277448654175 36.79350662231445\n",
      "Epoch    50: reducing learning rate of group 0 to 6.2500e-05.\n",
      "49 2.374017658687773 36.82456588745117\n",
      "50 1.848648834796179 36.84452819824219\n",
      "51 1.901370732557206 36.82823181152344\n",
      "52 2.3461208570571173 36.62135314941406\n",
      "53 2.1684642178671703 36.52052307128906\n",
      "54 2.407253475416274 36.63152313232422\n",
      "55 2.3501784262203036 36.980037689208984\n",
      "56 2.0413128393036977 36.99346923828125\n",
      "57 2.0423167660122825 36.87258529663086\n",
      "58 2.1215994216146923 36.680511474609375\n",
      "59 2.143839779354277 36.67393493652344\n",
      "Epoch    61: reducing learning rate of group 0 to 3.1250e-05.\n",
      "60 2.226642058009193 36.50347900390625\n",
      "61 1.8960401983488173 36.50379943847656\n",
      "62 2.143861861456008 36.47783660888672\n",
      "63 2.2374699257668995 36.47654342651367\n",
      "64 1.9234042394728887 36.606082916259766\n",
      "65 2.4356896508307684 36.69560241699219\n",
      "66 2.101174317655109 36.81221389770508\n",
      "67 2.100067995843433 36.73427963256836\n",
      "68 2.507584946496146 36.679649353027344\n",
      "69 1.8708893685113817 36.72923278808594\n",
      "70 2.1421595840227035 36.80856704711914\n",
      "Epoch    72: reducing learning rate of group 0 to 1.5625e-05.\n",
      "71 1.7565683381898063 36.71189498901367\n",
      "72 2.092421137151264 36.686180114746094\n",
      "73 2.319751256988162 36.6845588684082\n",
      "74 2.0577301297869 36.733943939208984\n",
      "75 1.9518930940400987 36.77397155761719\n",
      "76 1.9777396122614543 36.78217315673828\n",
      "77 1.754820378053756 36.79366683959961\n",
      "78 2.076322521482195 36.79062271118164\n",
      "79 1.6838597825595312 36.78778839111328\n",
      "80 2.176708610284896 36.76919174194336\n",
      "81 2.201296854586828 36.76310729980469\n",
      "Epoch    83: reducing learning rate of group 0 to 7.8125e-06.\n",
      "82 2.028604371207101 36.754302978515625\n",
      "83 1.9225120629583086 36.779083251953125\n",
      "84 2.487494931334541 36.812252044677734\n",
      "85 2.067590994494302 36.81202697753906\n",
      "86 2.152145030952635 36.807029724121094\n",
      "87 1.9370061130750746 36.80759811401367\n",
      "88 1.871877914383298 36.782318115234375\n",
      "89 2.0077711939811707 36.75566101074219\n",
      "90 2.2666059931119285 36.76108169555664\n",
      "91 1.926008851755233 36.740875244140625\n",
      "92 1.9089416861534119 36.749755859375\n",
      "Epoch    94: reducing learning rate of group 0 to 3.9063e-06.\n",
      "93 2.0316184205668315 36.763397216796875\n",
      "94 2.2050982259568714 36.75651550292969\n",
      "95 1.8746177738621121 36.744319915771484\n",
      "96 2.3634247041883922 36.727909088134766\n",
      "97 1.834667024158296 36.71915817260742\n",
      "98 2.1395732164382935 36.71256637573242\n",
      "99 1.9291801963533675 36.7200813293457\n",
      "100 2.255077526682899 36.72024917602539\n",
      "101 1.958677121571132 36.7214469909668\n",
      "102 2.8953366307985213 36.726192474365234\n",
      "103 1.9079633071309043 36.731136322021484\n",
      "Epoch   105: reducing learning rate of group 0 to 1.9531e-06.\n",
      "104 1.7735612846556164 36.72476577758789\n",
      "105 1.8771571658906483 36.72962188720703\n",
      "106 1.8464281104859852 36.72574996948242\n",
      "107 1.942596998952684 36.72877502441406\n",
      "108 2.0606569165275213 36.73550796508789\n",
      "109 1.893101141566322 36.74252700805664\n",
      "110 2.376383046309153 36.74658203125\n",
      "111 1.9288640448025294 36.75028610229492\n",
      "112 1.9252378259386336 36.75181198120117\n",
      "113 2.003977460520608 36.7547721862793\n",
      "114 1.7388356782141186 36.76495361328125\n",
      "Epoch   116: reducing learning rate of group 0 to 9.7656e-07.\n",
      "115 1.878205500897907 36.772117614746094\n",
      "116 1.5339574728693282 36.77449035644531\n",
      "117 1.7469320382390703 36.772544860839844\n",
      "118 1.9433546236583166 36.77003860473633\n",
      "119 1.9089617672420682 36.76451110839844\n",
      "120 2.420550831726619 36.759124755859375\n",
      "121 2.282015357698713 36.7586784362793\n",
      "122 2.0517287850379944 36.760406494140625\n",
      "123 2.4984332692055475 36.75984191894531\n",
      "124 2.1105747449965704 36.76123046875\n",
      "125 1.8953189111891247 36.7577018737793\n",
      "Epoch   127: reducing learning rate of group 0 to 4.8828e-07.\n",
      "126 1.9118053033238365 36.76005935668945\n",
      "127 1.9883013765017192 36.761295318603516\n",
      "128 1.6602951657204401 36.76255798339844\n",
      "129 2.0759782705988203 36.76232147216797\n",
      "130 1.7127251000631423 36.76254653930664\n",
      "131 2.2098528827939714 36.76139450073242\n",
      "132 1.9072537847927638 36.76063919067383\n",
      "133 2.1106898614338467 36.76011276245117\n",
      "134 1.6024424660773504 36.75981140136719\n",
      "135 2.182092226686932 36.75881576538086\n",
      "136 2.323061071691059 36.758094787597656\n",
      "Epoch   138: reducing learning rate of group 0 to 2.4414e-07.\n",
      "137 1.8435423913456144 36.757083892822266\n",
      "138 1.7823297494933719 36.75800323486328\n",
      "139 2.2919768492380777 36.75734329223633\n",
      "140 1.9375770148776827 36.75687026977539\n",
      "141 2.4658460844130743 36.757503509521484\n",
      "142 1.7803484542029244 36.756935119628906\n",
      "143 1.9040254269327437 36.75593948364258\n",
      "144 1.94217464469728 36.75564193725586\n",
      "145 2.01428576026644 36.75550079345703\n",
      "146 2.029290735721588 36.75534439086914\n",
      "147 2.299291939962478 36.75552749633789\n",
      "Epoch   149: reducing learning rate of group 0 to 1.2207e-07.\n",
      "148 1.9144249558448792 36.75532531738281\n",
      "149 1.953032871087392 36.7552375793457\n",
      "mae:  4.387448138583544\n",
      "mse:  29.031691760479923\n"
     ]
    }
   ],
   "source": [
    "# met and search \n",
    "config = {'h':128, 'lr':0.001, 'num_epochs':150, 'batchsize':32, 'drop':0.7}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train[:,:,1:], label_train, last_dev[:,:,1:], label_dev, last_test[:,:,1:], label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 39.545395397004626 29.60646629333496 saving model\n",
      "1 32.19648465656099 20.465513229370117 saving model\n",
      "2 27.154795374189103 20.411623001098633 saving model\n",
      "3 26.70655595688593 19.806772232055664 saving model\n",
      "4 25.66730526515416 19.702316284179688 saving model\n",
      "5 25.613938513256254 19.571622848510742 saving model\n",
      "6 25.043526467822847 19.318038940429688 saving model\n",
      "7 25.299606414068315 19.432289123535156\n",
      "8 25.253958066304524 19.438507080078125\n",
      "9 25.96671099889846 19.20871353149414 saving model\n",
      "10 24.311976841517858 19.317289352416992\n",
      "11 24.9241973786127 19.489521026611328\n",
      "12 25.539459773472377 19.364763259887695\n",
      "13 25.92112681979225 19.33141326904297\n",
      "14 24.435290291195823 19.275493621826172\n",
      "15 24.8737089520409 19.43120765686035\n",
      "16 25.087226913088845 19.16067886352539 saving model\n",
      "17 24.985576629638672 19.35442352294922\n",
      "18 24.06506429399763 19.205854415893555\n",
      "19 24.048133623032342 19.488351821899414\n",
      "20 24.351695651099796 19.322265625\n",
      "21 25.19601276942662 19.185813903808594\n",
      "22 24.639760335286457 19.459333419799805\n",
      "23 25.36745988754999 19.018667221069336 saving model\n",
      "24 24.648581822713215 19.199827194213867\n",
      "25 24.41571957724435 18.957805633544922 saving model\n",
      "26 25.12797210330055 19.330780029296875\n",
      "27 25.0857329141526 19.07439613342285\n",
      "28 24.28632032303583 19.333181381225586\n",
      "29 25.288088753109886 19.02851104736328\n",
      "30 23.91116237640381 19.23044776916504\n",
      "31 24.810724440075102 19.16593360900879\n",
      "32 24.223310016450426 19.422910690307617\n",
      "33 24.941609745933896 19.093721389770508\n",
      "34 25.06862395150321 19.11649513244629\n",
      "35 23.577171688988095 19.14406967163086\n",
      "Epoch    37: reducing learning rate of group 0 to 5.0000e-04.\n",
      "36 24.939524332682293 19.0245418548584\n",
      "37 24.19474034082322 19.22185707092285\n",
      "38 24.256779171171644 19.241558074951172\n",
      "39 24.02198959532238 18.964841842651367\n",
      "40 24.349579584030877 19.27744483947754\n",
      "41 24.135455358596076 19.399560928344727\n",
      "42 24.64142100016276 19.044252395629883\n",
      "43 24.577932902744838 19.356489181518555\n",
      "44 24.00001639411563 19.322755813598633\n",
      "45 23.760774703252885 19.05661964416504\n",
      "46 24.293304806663876 19.26778793334961\n",
      "Epoch    48: reducing learning rate of group 0 to 2.5000e-04.\n",
      "47 24.976633934747603 19.215801239013672\n",
      "48 25.106152080354235 19.33997917175293\n",
      "49 23.826581319173176 19.266162872314453\n",
      "50 24.465733528137207 19.288150787353516\n",
      "51 23.376951399303618 19.028047561645508\n",
      "52 24.11290391286214 19.12987518310547\n",
      "53 24.34782364254906 19.2559757232666\n",
      "54 24.153539385114396 19.25717544555664\n",
      "55 23.84860860733759 19.102706909179688\n",
      "56 24.794680822463263 19.071870803833008\n",
      "57 24.341403643290203 19.1793270111084\n",
      "Epoch    59: reducing learning rate of group 0 to 1.2500e-04.\n",
      "58 24.049702962239582 19.136945724487305\n",
      "59 24.289635385785783 19.221101760864258\n",
      "60 23.595015571230935 19.216903686523438\n",
      "61 24.255194164457777 19.325746536254883\n",
      "62 23.767832438151043 19.280031204223633\n",
      "63 23.39509051186698 19.25496482849121\n",
      "64 24.696946689060756 19.209468841552734\n",
      "65 23.4192540759132 19.217449188232422\n",
      "66 24.218482153756277 19.247642517089844\n",
      "67 23.889405068897066 19.18899154663086\n",
      "68 24.06560720716204 19.193771362304688\n",
      "Epoch    70: reducing learning rate of group 0 to 6.2500e-05.\n",
      "69 24.156836827596027 19.12050437927246\n",
      "70 24.2221615200951 19.14649200439453\n",
      "71 23.820664769127255 19.117626190185547\n",
      "72 23.957260222662065 19.158981323242188\n",
      "73 23.56244100843157 19.199493408203125\n",
      "74 23.6275729678926 19.171367645263672\n",
      "75 23.82374382019043 19.23772430419922\n",
      "76 24.04143783024379 19.21873664855957\n",
      "77 24.276535125005815 19.235546112060547\n",
      "78 23.058764730181014 19.25713348388672\n",
      "79 24.692723047165643 19.264558792114258\n",
      "Epoch    81: reducing learning rate of group 0 to 3.1250e-05.\n",
      "80 23.821030480521067 19.24997329711914\n",
      "81 24.151435397920153 19.2115478515625\n",
      "82 24.201280911763508 19.206132888793945\n",
      "83 24.194726898556663 19.18628692626953\n",
      "84 23.542692048209055 19.177610397338867\n",
      "85 23.474761463346937 19.187515258789062\n",
      "86 23.920671917143324 19.20289421081543\n",
      "87 23.938132921854656 19.23053741455078\n",
      "88 23.466511181422643 19.238468170166016\n",
      "89 24.040761765979585 19.243059158325195\n",
      "90 24.320855413164413 19.24623680114746\n",
      "Epoch    92: reducing learning rate of group 0 to 1.5625e-05.\n",
      "91 23.78033687954857 19.222734451293945\n",
      "92 23.783026876903715 19.218862533569336\n",
      "93 24.403071630568732 19.22170639038086\n",
      "94 23.76770205724807 19.22429656982422\n",
      "95 24.75333563486735 19.237260818481445\n",
      "96 23.95813660394578 19.247526168823242\n",
      "97 24.120591390700568 19.24522590637207\n",
      "98 24.31060250600179 19.250131607055664\n",
      "99 23.391947382972354 19.26201629638672\n",
      "100 24.13451485406785 19.270456314086914\n",
      "101 25.026765414646693 19.266284942626953\n",
      "Epoch   103: reducing learning rate of group 0 to 7.8125e-06.\n",
      "102 24.463603882562545 19.264156341552734\n",
      "103 23.227122215997603 19.25757598876953\n",
      "104 23.55796582358224 19.260540008544922\n",
      "105 23.531823112851097 19.25621795654297\n",
      "106 23.779131071908132 19.25727653503418\n",
      "107 23.822303544907342 19.26105499267578\n",
      "108 24.309018907092867 19.260658264160156\n",
      "109 23.939444087800524 19.253372192382812\n",
      "110 22.97875181833903 19.25130271911621\n",
      "111 24.448089417957124 19.24566078186035\n",
      "112 23.88106990995861 19.242979049682617\n",
      "Epoch   114: reducing learning rate of group 0 to 3.9063e-06.\n",
      "113 22.907228833153134 19.245506286621094\n",
      "114 23.40081941513788 19.244503021240234\n",
      "115 23.92817733401344 19.241878509521484\n",
      "116 24.183621542794363 19.24073028564453\n",
      "117 24.24586564018613 19.243003845214844\n",
      "118 24.2325226465861 19.242704391479492\n",
      "119 23.875702267601376 19.24353790283203\n",
      "120 23.34824661981492 19.24492073059082\n",
      "121 23.72342958904448 19.246904373168945\n",
      "122 23.398711249941872 19.245712280273438\n",
      "123 24.406709625607444 19.24811363220215\n",
      "Epoch   125: reducing learning rate of group 0 to 1.9531e-06.\n",
      "124 23.800169399806432 19.246047973632812\n",
      "125 24.053819520132883 19.246381759643555\n",
      "126 24.09150981903076 19.245832443237305\n",
      "127 23.594856035141717 19.247055053710938\n",
      "128 24.15870780036563 19.245521545410156\n",
      "129 24.11277721041725 19.24553680419922\n",
      "130 24.11151041303362 19.245555877685547\n",
      "131 24.344411395844958 19.247005462646484\n",
      "132 24.091474351428804 19.249591827392578\n",
      "133 24.197030839465913 19.249940872192383\n",
      "134 23.511765025910876 19.247753143310547\n",
      "Epoch   136: reducing learning rate of group 0 to 9.7656e-07.\n",
      "135 24.58621969677153 19.246721267700195\n",
      "136 23.34717945825486 19.247358322143555\n",
      "137 23.79046971457345 19.24741554260254\n",
      "138 23.559546334402903 19.248092651367188\n",
      "139 23.788490113757906 19.247695922851562\n",
      "140 24.662185532706125 19.248470306396484\n",
      "141 23.90326449984596 19.24913787841797\n",
      "142 24.567664555140905 19.24831199645996\n",
      "143 23.266295069739932 19.248188018798828\n",
      "144 23.885832423255557 19.248525619506836\n",
      "145 24.187661034720286 19.24838638305664\n",
      "Epoch   147: reducing learning rate of group 0 to 4.8828e-07.\n",
      "146 24.02566855294364 19.247695922851562\n",
      "147 23.56519839877174 19.247671127319336\n",
      "148 23.97363362993513 19.24745750427246\n",
      "149 24.473049935840425 19.247623443603516\n",
      "mae:  3.6550485098041783\n",
      "mse:  19.286654840197023\n"
     ]
    }
   ],
   "source": [
    "# pol only\n",
    "config = {'h':128, 'lr':0.001, 'num_epochs':150, 'batchsize':32, 'drop':0.7}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train[:,:,0:1], label_train, last_dev[:,:,0:1], label_dev, last_test[:,:,0:1], label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 39.67372976030622 29.619243621826172 saving model\n",
      "1 34.4580473672776 21.728351593017578 saving model\n",
      "2 27.089752469744003 21.53615379333496 saving model\n",
      "3 24.696594283694314 19.295978546142578 saving model\n",
      "4 23.30074846176874 19.840543746948242\n",
      "5 23.87181722550165 18.851125717163086 saving model\n",
      "6 23.003882453555153 18.976682662963867\n",
      "7 22.80754225594657 19.034927368164062\n",
      "8 22.72718702043806 19.237028121948242\n",
      "9 22.94171991802397 18.40932273864746 saving model\n",
      "10 22.388197263081867 18.758708953857422\n",
      "11 22.492624964032853 18.938552856445312\n",
      "12 22.287170909699938 18.31076431274414 saving model\n",
      "13 20.707391057695663 18.631088256835938\n",
      "14 21.157987594604492 18.076000213623047 saving model\n",
      "15 21.196017174493697 18.58933448791504\n",
      "16 20.174860522860573 19.09343910217285\n",
      "17 20.67708728426979 17.642457962036133 saving model\n",
      "18 20.06414558773949 18.677982330322266\n",
      "19 20.194218998863583 18.120304107666016\n",
      "20 20.13174243200393 17.89269256591797\n",
      "21 20.09222039722261 17.883731842041016\n",
      "22 20.331072035289946 17.58086585998535 saving model\n",
      "23 19.44376986367362 17.83183479309082\n",
      "24 18.930975051153276 17.0938663482666 saving model\n",
      "25 18.671474025363015 17.341230392456055\n",
      "26 19.47668693179176 18.00815200805664\n",
      "27 19.18344443184989 17.557659149169922\n",
      "28 18.054361479622976 18.718059539794922\n",
      "29 18.676120258512952 17.998722076416016\n",
      "30 17.59927731468564 17.95920181274414\n",
      "31 18.361024765741256 17.92975616455078\n",
      "32 17.69552162715367 18.25438690185547\n",
      "33 17.768296060108003 18.043413162231445\n",
      "34 18.167511258806503 18.110252380371094\n",
      "Epoch    36: reducing learning rate of group 0 to 5.0000e-04.\n",
      "35 16.971495651063464 18.855022430419922\n",
      "36 17.059467270260765 18.78787612915039\n",
      "37 16.89369249343872 18.55125617980957\n",
      "38 16.58246962229411 19.285301208496094\n",
      "39 16.54784733908517 18.647680282592773\n",
      "40 16.629063197544642 19.51386260986328\n",
      "41 15.827470416114444 18.98488426208496\n",
      "42 16.64400945390974 19.63193130493164\n",
      "43 16.10647909981864 18.96166229248047\n",
      "44 16.530240172431583 19.31708335876465\n",
      "45 16.35786244982765 19.391889572143555\n",
      "Epoch    47: reducing learning rate of group 0 to 2.5000e-04.\n",
      "46 15.868790717352004 19.564537048339844\n",
      "47 15.488083362579346 19.902658462524414\n",
      "48 15.712762219565255 19.587417602539062\n",
      "49 15.458906287238712 19.4304256439209\n",
      "50 15.797385238465809 19.8408145904541\n",
      "51 15.848274094717842 20.18657875061035\n",
      "52 15.508028030395508 19.96454620361328\n",
      "53 15.785137448992048 19.92060089111328\n",
      "54 14.89204758689517 19.854034423828125\n",
      "55 15.766833396185012 19.644926071166992\n",
      "56 15.592562334878105 19.935710906982422\n",
      "Epoch    58: reducing learning rate of group 0 to 1.2500e-04.\n",
      "57 15.79145760763259 20.055767059326172\n",
      "58 15.25212560381208 19.890071868896484\n",
      "59 15.18306777590797 19.96823501586914\n",
      "60 15.182171458289737 20.031352996826172\n",
      "61 15.127238296327137 20.02411651611328\n",
      "62 15.800030163356237 19.9172420501709\n",
      "63 14.481560775211879 19.939714431762695\n",
      "64 15.54462244397118 20.055143356323242\n",
      "65 14.860813504173642 20.069555282592773\n",
      "66 14.890481403895787 20.14432144165039\n",
      "67 15.200599193572998 20.13951301574707\n",
      "Epoch    69: reducing learning rate of group 0 to 6.2500e-05.\n",
      "68 15.055492741721016 20.084566116333008\n",
      "69 15.515429701123919 20.173065185546875\n",
      "70 14.992978709084648 20.200101852416992\n",
      "71 14.848517145429339 20.18474769592285\n",
      "72 14.64382080804734 20.224899291992188\n",
      "73 15.27801817939395 20.239105224609375\n",
      "74 15.186446462358747 20.308340072631836\n",
      "75 14.354945546104794 20.354814529418945\n",
      "76 14.796003455207462 20.34073257446289\n",
      "77 15.112176282065255 20.300739288330078\n",
      "78 14.690519514537993 20.314128875732422\n",
      "Epoch    80: reducing learning rate of group 0 to 3.1250e-05.\n",
      "79 14.916700817289806 20.380516052246094\n",
      "80 14.863222394670759 20.3470401763916\n",
      "81 14.84484236580985 20.33328628540039\n",
      "82 15.352769397553944 20.398820877075195\n",
      "83 15.033636047726585 20.39345932006836\n",
      "84 14.655508631751651 20.3169002532959\n",
      "85 15.290155274527413 20.328458786010742\n",
      "86 14.446842170896984 20.323272705078125\n",
      "87 14.882439772288004 20.382139205932617\n",
      "88 15.579531487964449 20.38654899597168\n",
      "89 15.37984866187686 20.419649124145508\n",
      "Epoch    91: reducing learning rate of group 0 to 1.5625e-05.\n",
      "90 14.98862479981922 20.47525978088379\n",
      "91 15.131401697794596 20.452009201049805\n",
      "92 14.752190090361095 20.465970993041992\n",
      "93 15.157812663487025 20.48238182067871\n",
      "94 14.758048943110875 20.477962493896484\n",
      "95 14.404003756386894 20.49953269958496\n",
      "96 14.776000204540434 20.49480438232422\n",
      "97 14.851421991984049 20.494338989257812\n",
      "98 14.721395288194929 20.52820587158203\n",
      "99 14.079078674316406 20.544036865234375\n",
      "100 15.242676326206752 20.54426383972168\n",
      "Epoch   102: reducing learning rate of group 0 to 7.8125e-06.\n",
      "101 14.966136251177106 20.511898040771484\n",
      "102 14.505302928742909 20.501367568969727\n",
      "103 14.89967409769694 20.502891540527344\n",
      "104 14.917481604076567 20.502614974975586\n",
      "105 14.80836296081543 20.505096435546875\n",
      "106 14.670405842009044 20.514652252197266\n",
      "107 14.38441106251308 20.52167510986328\n",
      "108 15.303034827822732 20.51894760131836\n",
      "109 14.799812589372907 20.52351951599121\n",
      "110 14.779118378957113 20.51938819885254\n",
      "111 14.961269287835984 20.517465591430664\n",
      "Epoch   113: reducing learning rate of group 0 to 3.9063e-06.\n",
      "112 14.859988189878917 20.516752243041992\n",
      "113 14.797158740815663 20.512939453125\n",
      "114 15.352773870740618 20.515174865722656\n",
      "115 14.40727204368228 20.520336151123047\n",
      "116 14.482612905048189 20.52064323425293\n",
      "117 14.644055003211612 20.519603729248047\n",
      "118 15.108780088878813 20.513656616210938\n",
      "119 14.685451598394485 20.510141372680664\n",
      "120 14.443547861916679 20.509471893310547\n",
      "121 14.619318190075102 20.504085540771484\n",
      "122 14.703758375985283 20.50577735900879\n",
      "Epoch   124: reducing learning rate of group 0 to 1.9531e-06.\n",
      "123 14.166548728942871 20.505184173583984\n",
      "124 14.93562264669509 20.503437042236328\n",
      "125 14.943660395486015 20.50214385986328\n",
      "126 14.682647546132406 20.49921989440918\n",
      "127 14.584610144297281 20.50264549255371\n",
      "128 15.152779125031971 20.501602172851562\n",
      "129 14.24120959781465 20.499113082885742\n",
      "130 14.150275934310187 20.498212814331055\n",
      "131 15.119770867483956 20.49980354309082\n",
      "132 14.295240129743304 20.49638557434082\n",
      "133 14.728248596191406 20.496829986572266\n",
      "Epoch   135: reducing learning rate of group 0 to 9.7656e-07.\n",
      "134 14.784285976773216 20.497480392456055\n",
      "135 14.85493264879499 20.497915267944336\n",
      "136 14.84948187782651 20.496055603027344\n",
      "137 14.451261111668178 20.49602508544922\n",
      "138 14.620541981288365 20.49773406982422\n",
      "139 14.99282555353074 20.495338439941406\n",
      "140 14.679284209296817 20.494558334350586\n",
      "141 14.38004173551287 20.494342803955078\n",
      "142 14.466993422735305 20.493141174316406\n",
      "143 14.751767748878116 20.493684768676758\n",
      "144 14.167445182800293 20.492666244506836\n",
      "Epoch   146: reducing learning rate of group 0 to 4.8828e-07.\n",
      "145 15.179999283381871 20.490755081176758\n",
      "146 15.133172898065476 20.49072265625\n",
      "147 14.974532354445685 20.490924835205078\n",
      "148 14.83186301730928 20.490388870239258\n",
      "149 15.388678482600621 20.490629196166992\n",
      "mae:  3.208220705697852\n",
      "mse:  16.158608959042503\n"
     ]
    }
   ],
   "source": [
    "# pol and met \n",
    "config = {'h':128, 'lr':0.001, 'num_epochs':150, 'batchsize':32, 'drop':0.7}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train[:,:,0:5], label_train, last_dev[:,:,0:5], label_dev, last_test[:,:,0:5], label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
