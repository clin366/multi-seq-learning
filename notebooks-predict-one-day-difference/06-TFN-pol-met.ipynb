{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/predict-one-day-diff/pol-met-search'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_uniform, xavier_normal, orthogonal\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 7, 52)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mfn(X_train, y_train, X_valid, y_valid, X_test, y_test, configs):\n",
    "#     p = np.random.permutation(X_train.shape[0])\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "#     def swap_axes(nparr):\n",
    "#         return nparr.swapaxes(0,1)\n",
    "#     X_train = swap_axes(X_train)\n",
    "#     X_valid = swap_axes(X_valid)\n",
    "#     X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters \n",
    "    input_size = X_train.shape[2]\n",
    "    h = 128\n",
    "    t = X_train.shape[1]\n",
    "    output_dim = 1\n",
    "    dropout = 0.5\n",
    "    \n",
    "    model = TFN(configs[\"input_dims\"], configs[\"h_dims\"], configs[\"text_out\"],\n",
    "               configs[\"dropouts\"], configs[\"post_fusion_dim\"])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "#     criterion = nn.L1Loss()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer,mode='min',patience=100,factor=0.5,verbose=True)\n",
    "\n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[0]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[start:end, :])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/temp_models/mfn_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "#     print 'model number is:', rand\n",
    "    model = torch.load('models/temp_models/mfn_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSubNet(nn.Module):\n",
    "    '''\n",
    "    The LSTM-based subnetwork that is used in TFN for text\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_size, hidden_size, out_size, num_layers=7, dropout=0.2, bidirectional=False):\n",
    "        '''\n",
    "        Args:\n",
    "            in_size: input dimension\n",
    "            hidden_size: hidden layer dimension\n",
    "            num_layers: specify the number of layers of LSTMs.\n",
    "            dropout: dropout probability\n",
    "            bidirectional: specify usage of bidirectional LSTM\n",
    "        Output:\n",
    "            (return value in forward) a tensor of shape (batch_size, out_size)\n",
    "        '''\n",
    "        super(TextSubNet, self).__init__()\n",
    "        self.rnn = nn.LSTM(in_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_1 = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x: tensor of shape (batch_size, sequence_len, in_size)\n",
    "        '''\n",
    "        _, final_states = self.rnn(x)\n",
    "#         print(\"shape of the final_states\")\n",
    "#         print(final_states[0].size())\n",
    "        h = self.dropout(final_states[0][-1].squeeze())\n",
    "        y_1 = self.linear_1(h)\n",
    "        return y_1\n",
    "\n",
    "\n",
    "class TFN(nn.Module):\n",
    "    '''\n",
    "    Implements the Tensor Fusion Networks for multimodal sentiment analysis as is described in:\n",
    "    Zadeh, Amir, et al. \"Tensor fusion network for multimodal sentiment analysis.\" EMNLP 2017 Oral.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_dims, hidden_dims, text_out, dropouts, post_fusion_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dims - a length-3 tuple, contains (audio_dim, video_dim, text_dim)\n",
    "            hidden_dims - another length-3 tuple, similar to input_dims\n",
    "            text_out - int, specifying the resulting dimensions of the text subnetwork\n",
    "            dropouts - a length-4 tuple, contains (audio_dropout, video_dropout, text_dropout, post_fusion_dropout)\n",
    "            post_fusion_dim - int, specifying the size of the sub-networks after tensorfusion\n",
    "        Output:\n",
    "            (return value in forward) a scalar value between -3 and 3\n",
    "        '''\n",
    "        super(TFN, self).__init__()\n",
    "\n",
    "        # dimensions are specified in the order of audio, video and text\n",
    "        self.audio_in = input_dims[0]\n",
    "        self.text_in = input_dims[1]\n",
    "\n",
    "        self.audio_hidden = hidden_dims[0]\n",
    "        self.text_hidden = hidden_dims[1]\n",
    "        self.audio_out = text_out[0]\n",
    "        self.text_out= text_out[1]\n",
    "        self.post_fusion_dim = post_fusion_dim\n",
    "\n",
    "        self.audio_prob = dropouts[0]\n",
    "        self.text_prob = dropouts[1]\n",
    "        self.post_fusion_prob = dropouts[2]\n",
    "\n",
    "        # define the pre-fusion subnetworks\n",
    "        self.audio_subnet = TextSubNet(self.audio_in, self.audio_hidden, self.audio_out, dropout=self.audio_prob)\n",
    "        self.text_subnet = TextSubNet(self.text_in, self.text_hidden, self.text_out, dropout=self.text_prob)\n",
    "\n",
    "        # define the post_fusion layers\n",
    "        self.post_fusion_dropout = nn.Dropout(p=self.post_fusion_prob)\n",
    "        self.post_fusion_layer_1 = nn.Linear((self.text_out + 1) * (self.audio_hidden + 1), self.post_fusion_dim)\n",
    "        self.post_fusion_layer_2 = nn.Linear(self.post_fusion_dim, self.post_fusion_dim)\n",
    "        self.post_fusion_layer_3 = nn.Linear(self.post_fusion_dim, 1)\n",
    "\n",
    "        # in TFN we are doing a regression with constrained output range: (-3, 3), hence we'll apply sigmoid to output\n",
    "        # shrink it to (0, 1), and scale\\shift it back to range (-3, 3)\n",
    "#         self.output_range = Parameter(torch.FloatTensor([6]), requires_grad=False)\n",
    "#         self.output_shift = Parameter(torch.FloatTensor([-3]), requires_grad=False)\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        '''\n",
    "        Args:\n",
    "            audio_x: tensor of shape (batch_size, sequence_len, audio_in)\n",
    "            text_x: tensor of shape (batch_size, sequence_len, text_in)\n",
    "        '''\n",
    "        audio_x = input_x[:,:,:self.audio_in]\n",
    "        text_x = input_x[:,:,self.audio_in:self.audio_in+self.text_in]\n",
    "#         print(audio_x.size())\n",
    "        audio_h = self.audio_subnet(audio_x)\n",
    "#         print(audio_h.size())\n",
    "        text_h = self.text_subnet(text_x)\n",
    "        batch_size = audio_h.data.shape[0]\n",
    "\n",
    "        # next we perform \"tensor fusion\", which is essentially appending 1s to the tensors and take Kronecker product\n",
    "        if audio_h.is_cuda:\n",
    "            DTYPE = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            DTYPE = torch.FloatTensor\n",
    "\n",
    "#         print(\"the size of audio_h\")\n",
    "        _audio_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), audio_h), dim=1)\n",
    "        _text_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), text_h), dim=1)\n",
    "\n",
    "        # _audio_h has shape (batch_size, audio_in + 1), _video_h has shape (batch_size, _video_in + 1)\n",
    "        # we want to perform outer product between the two batch, hence we unsqueenze them to get\n",
    "        # (batch_size, audio_in + 1, 1) X (batch_size, 1, video_in + 1)\n",
    "        # fusion_tensor will have shape (batch_size, audio_in + 1, video_in + 1)\n",
    "        fusion_tensor = torch.bmm(_audio_h.unsqueeze(2), _text_h.unsqueeze(1))\n",
    "        \n",
    "        # next we do kronecker product between fusion_tensor and _text_h. This is even trickier\n",
    "        # we have to reshape the fusion tensor during the computation\n",
    "        # in the end we don't keep the 3-D tensor, instead we flatten it\n",
    "        fusion_tensor = fusion_tensor.view(batch_size, -1)\n",
    "\n",
    "        post_fusion_dropped = self.post_fusion_dropout(fusion_tensor)\n",
    "        post_fusion_y_1 = F.relu(self.post_fusion_layer_1(post_fusion_dropped))\n",
    "        post_fusion_y_2 = F.relu(self.post_fusion_layer_2(post_fusion_y_1))\n",
    "        output = self.post_fusion_layer_3(post_fusion_y_2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 53.44606590270996 33.65911865234375 saving model\n",
      "1 53.440216700236 33.66810607910156\n",
      "2 53.44312381744385 33.669673919677734\n",
      "3 53.42582925160726 33.660953521728516\n",
      "4 53.45153999328613 33.62373352050781 saving model\n",
      "5 53.31860192616781 33.38136291503906 saving model\n",
      "6 52.401777267456055 32.42032241821289 saving model\n",
      "7 51.99789841969808 32.9852409362793\n",
      "8 49.45961856842041 32.91842269897461\n",
      "9 44.89142100016276 31.043685913085938 saving model\n",
      "10 47.79404513041178 31.435747146606445\n",
      "11 43.48728593190511 24.02444839477539 saving model\n",
      "12 40.28859519958496 23.613401412963867 saving model\n",
      "13 42.18061923980713 27.063631057739258\n",
      "14 35.16040325164795 18.914690017700195 saving model\n",
      "15 39.25727812449137 22.444538116455078\n",
      "16 35.50307559967041 20.327707290649414\n",
      "17 35.30454730987549 20.056400299072266\n",
      "18 38.43170293172201 24.724164962768555\n",
      "19 38.644378662109375 20.747798919677734\n",
      "20 36.59397602081299 19.10542869567871\n",
      "21 35.24626890818278 21.195058822631836\n",
      "22 32.90003172556559 19.83803367614746\n",
      "23 34.13777987162272 19.15007972717285\n",
      "24 32.72418181101481 20.616910934448242\n",
      "Epoch    26: reducing learning rate of group 0 to 5.0000e-04.\n",
      "25 33.467962900797524 18.927764892578125\n",
      "26 31.77651246388753 19.736068725585938\n",
      "27 32.29338534673055 20.09538459777832\n",
      "28 30.294254302978516 19.11258888244629\n",
      "29 30.394140084584553 18.963605880737305\n",
      "30 30.90605576833089 19.413410186767578\n",
      "31 30.702365239461262 19.297256469726562\n",
      "32 30.99835268656413 19.31704330444336\n",
      "33 31.142776171366375 18.946767807006836\n",
      "34 29.867988268534344 19.1240177154541\n",
      "35 30.045942942301433 19.47425651550293\n",
      "Epoch    37: reducing learning rate of group 0 to 2.5000e-04.\n",
      "36 30.536263465881348 19.385665893554688\n",
      "37 30.92090129852295 19.410907745361328\n",
      "38 30.075501759847004 19.499954223632812\n",
      "39 30.33217652638753 19.20231056213379\n",
      "40 29.91827170054118 19.08824920654297\n",
      "41 29.642762184143066 19.323925018310547\n",
      "42 29.97301785151164 19.585336685180664\n",
      "43 30.316413243611652 19.512632369995117\n",
      "44 30.36810080210368 19.26875114440918\n",
      "45 29.71023178100586 19.162837982177734\n",
      "46 30.0425230662028 19.296157836914062\n",
      "Epoch    48: reducing learning rate of group 0 to 1.2500e-04.\n",
      "47 29.713157018025715 19.52161407470703\n",
      "48 30.19658597310384 19.590871810913086\n",
      "49 30.62081750233968 19.591533660888672\n",
      "mae:  3.504799284471953\n",
      "mse:  17.731157864234753\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 128\n",
    "ha = 128\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"text_out\"] = (hl, ha)\n",
    "config[\"dropouts\"] = (0.2, 0.2, 0.2)\n",
    "config[\"post_fusion_dim\"] = hl\n",
    "config[\"batchsize\"] = hl\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.001\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train[:,:,:5], label_train, last_dev[:,:,:5], label_dev, last_test[:,:,:5], label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 7, 52)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
