{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, filter_square_matrix, bias=True):\n",
    "        '''\n",
    "        filter_square_matrix : filter square matrix, whose each elements is 0 or 1.\n",
    "        '''\n",
    "        super(FilterLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        self.filter_square_matrix = None\n",
    "        if use_gpu:\n",
    "            self.filter_square_matrix = Variable(filter_square_matrix.cuda(), requires_grad=False)\n",
    "        else:\n",
    "            self.filter_square_matrix = Variable(filter_square_matrix, requires_grad=False)\n",
    "        \n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "#         print(self.weight.data)\n",
    "#         print(self.bias.data)\n",
    "\n",
    "    def forward(self, input):\n",
    "#         print(self.filter_square_matrix.mul(self.weight))\n",
    "        return F.linear(input, self.filter_square_matrix.mul(self.weight), self.bias)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) \\\n",
    "            + ', bias=' + str(self.bias is not None) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/predict-one-day-diff/pol-met-search'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays\n",
    "\n",
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]\n",
    "\n",
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize delta\n",
    "# we can normlize this because it's know features \n",
    "delta_seq = (delta_seq - np.mean(delta_seq)) / np.std(delta_seq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train, masking_train, delta_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq, masking_seq, delta_seq\n",
    "                                                                 ], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev, masking_dev, delta_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq, masking_seq, delta_seq\n",
    "                                                           ], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test, masking_test, delta_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq, masking_seq, delta_seq\n",
    "                                                              ], label_seq, test_index)\n",
    "\n",
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])\n",
    "\n",
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset for grud \n",
    "def dataset_aggregation(feature_array, last_obsv, mask, delta):\n",
    "    # expand dimension of array\n",
    "    def expd(arr):\n",
    "        return np.expand_dims(arr, axis=1)\n",
    "    return np.concatenate((expd(feature_array), expd(last_obsv), expd(mask), expd(delta)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_aggregation for train, dev, test \n",
    "# train_aggr = dataset_aggregation(fea_train, last_train, masking_train, delta_train)\n",
    "train_aggr = dataset_aggregation(last_train, last_train, masking_train, delta_train)\n",
    "# dev_aggr = dataset_aggregation(fea_dev, last_dev, masking_dev, delta_dev)\n",
    "# test_aggr = dataset_aggregation(fea_test, last_test, masking_test, delta_test)\n",
    "dev_aggr = dataset_aggregation(last_dev, last_dev, masking_dev, delta_dev)\n",
    "test_aggr = dataset_aggregation(last_test, last_test, masking_test, delta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 4, 7, 52)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aggr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mfn(X_train, y_train, X_valid, y_valid, X_test, y_test, configs, X_mean):\n",
    "#     p = np.random.permutation(X_train.shape[0])\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "#     def swap_axes(nparr):\n",
    "#         return nparr.swapaxes(0,1)\n",
    "#     X_train = swap_axes(X_train)\n",
    "#     X_valid = swap_axes(X_valid)\n",
    "#     X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters \n",
    "    input_size = X_train.shape[3]\n",
    "    h = 32\n",
    "    t = X_train.shape[2]\n",
    "    output_dim = 1\n",
    "    dropout = 0.5\n",
    "\n",
    "#     d = X_train.shape[2]\n",
    "#     h = 128\n",
    "#     t = X_train.shape[0]\n",
    "#     output_dim = 1\n",
    "#     dropout = 0.5\n",
    "\n",
    "    [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig] = configs\n",
    "\n",
    "    \n",
    "    #model = EFLSTM(d,h,output_dim,dropout)\n",
    "    model = MFN(config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig, X_mean)\n",
    "\n",
    "#     optimizer = optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "    optimizer = optim.SGD(model.parameters(),lr=config[\"lr\"],momentum=0.9)\n",
    "\n",
    "    # optimizer = optim.SGD([\n",
    "    #                 {'params':model.lstm_l.parameters(), 'lr':config[\"lr\"]},\n",
    "    #                 {'params':model.classifier.parameters(), 'lr':config[\"lr\"]}\n",
    "    #             ], momentum=0.9)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=7, factor=0.5, verbose=True)\n",
    "    \n",
    "#     criterion = nn.L1Loss()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer,mode='min',patience=100,factor=0.5,verbose=True)\n",
    "\n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[0]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[start:end,:])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/temp_models/mfn_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "#     print 'model number is:', rand\n",
    "    model = torch.load('models/temp_models/mfn_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUD(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, X_mean, output_last = True, dropout=0):\n",
    "        \"\"\"\n",
    "        Recurrent Neural Networks for Multivariate Times Series with Missing Values\n",
    "        GRU-D: GRU exploit two representations of informative missingness patterns, i.e., masking and time interval.\n",
    "        cell_size is the size of cell_state.\n",
    "        \n",
    "        GRU-D:\n",
    "            input_size: variable dimension of each time\n",
    "            hidden_size: dimension of hidden_state\n",
    "            mask_size: dimension of masking vector\n",
    "            X_mean: the mean of the historical input data\n",
    "        \"\"\"\n",
    "        super(GRUD, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.delta_size = input_size\n",
    "        self.mask_size = input_size\n",
    "        \n",
    "        self.identity = torch.eye(input_size)\n",
    "        self.zeros = Variable(torch.zeros(input_size))\n",
    "        \n",
    "        self.zl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size)\n",
    "        self.rl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size)\n",
    "        self.hl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size)\n",
    "        \n",
    "        self.gamma_x_l = FilterLinear(self.delta_size, self.delta_size, self.identity)\n",
    "        \n",
    "#         self.gamma_h_l = nn.Linear(self.delta_size, self.delta_size)\n",
    "        self.gamma_h_l = nn.Linear(self.delta_size, hidden_size)\n",
    "        \n",
    "        self.map_hidden_gamma = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, x, x_last_obsv, x_mean, h, mask, delta):\n",
    "    # use input to feed the four dimensions \n",
    "    def forward(self, dataset, X_mean, h):\n",
    "        \"\"\"\n",
    "        dataset: matrix shape (n * c * d)\n",
    "                    n: batchsize; c: class; d number of dims\n",
    "        \"\"\"\n",
    "        x_mean = Variable(torch.Tensor(X_mean))\n",
    "#         print(\"dataset size\")\n",
    "#         print(dataset.size())\n",
    "        batch_size = dataset.size(0)\n",
    "        dim_size = dataset.size(2)\n",
    "    \n",
    "        x = dataset[:,0,:]\n",
    "        x_last_obsv = dataset[:,1,:]\n",
    "        mask = dataset[:,2,:]\n",
    "        delta = dataset[:,3,:]\n",
    "        \n",
    "        delta_x = torch.exp(-torch.max(self.zeros, self.gamma_x_l(delta)))\n",
    "        \n",
    "        delta_h = F.sigmoid(self.map_hidden_gamma(F.relu(self.gamma_h_l(delta))))\n",
    "        \n",
    "        x = mask * x + (1 - mask) * (delta_x * x_last_obsv + (1 - delta_x) * x_mean)\n",
    "        h = delta_h * h \n",
    "    \n",
    "#         print(\"combined size\")\n",
    "#         print(x.size())\n",
    "#         print(h.size())\n",
    "#         print(mask.size())\n",
    "        combined = torch.cat((x, h, mask), 1)\n",
    "        z = torch.sigmoid(self.zl(combined))\n",
    "        r = torch.sigmoid(self.rl(combined))\n",
    "        combined_r = torch.cat((x, r * h, mask), 1)\n",
    "        h_tilde = torch.tanh(self.hl(combined_r))\n",
    "        h = (1 - z) * h + z * h_tilde\n",
    "        \n",
    "        return h\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFN(nn.Module):\n",
    "    def __init__(self,config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig, X_mean):\n",
    "        super(MFN, self).__init__()\n",
    "        [self.d_l,self.d_a] = config[\"input_dims\"]\n",
    "        [self.dh_l,self.dh_a] = config[\"h_dims\"]\n",
    "        total_h_dim = self.dh_l+self.dh_a\n",
    "        \n",
    "        self.mem_dim = config[\"memsize\"]\n",
    "        window_dim = config[\"windowsize\"]\n",
    "        output_dim = 1\n",
    "        attInShape = total_h_dim*window_dim\n",
    "        gammaInShape = attInShape+self.mem_dim\n",
    "        final_out = total_h_dim+self.mem_dim\n",
    "        h_att1 = NN1Config[\"shapes\"]\n",
    "        h_att2 = NN2Config[\"shapes\"]\n",
    "        h_gamma1 = gamma1Config[\"shapes\"]\n",
    "        h_gamma2 = gamma2Config[\"shapes\"]\n",
    "        h_out = outConfig[\"shapes\"]\n",
    "        att1_dropout = NN1Config[\"drop\"]\n",
    "        att2_dropout = NN2Config[\"drop\"]\n",
    "        gamma1_dropout = gamma1Config[\"drop\"]\n",
    "        gamma2_dropout = gamma2Config[\"drop\"]\n",
    "        out_dropout = outConfig[\"drop\"]\n",
    "    \n",
    "        # use grud cell instead of LSTM Cell\n",
    "#         (self, input_size, hidden_size, X_mean, output_last = True, dropout=0):\n",
    "        self.gru_l = GRUD(self.d_l, self.dh_l, X_mean)\n",
    "        self.gru_a = GRUD(self.d_a, self.dh_a, X_mean)\n",
    "\n",
    "        self.att1_fc1 = nn.Linear(attInShape, h_att1)\n",
    "        self.att1_fc2 = nn.Linear(h_att1, attInShape)\n",
    "        self.att1_dropout = nn.Dropout(att1_dropout)\n",
    "\n",
    "        self.att2_fc1 = nn.Linear(attInShape, h_att2)\n",
    "        self.att2_fc2 = nn.Linear(h_att2, self.mem_dim)\n",
    "        self.att2_dropout = nn.Dropout(att2_dropout)\n",
    "\n",
    "        self.gamma1_fc1 = nn.Linear(gammaInShape, h_gamma1)\n",
    "        self.gamma1_fc2 = nn.Linear(h_gamma1, self.mem_dim)\n",
    "        self.gamma1_dropout = nn.Dropout(gamma1_dropout)\n",
    "\n",
    "        self.gamma2_fc1 = nn.Linear(gammaInShape, h_gamma2)\n",
    "        self.gamma2_fc2 = nn.Linear(h_gamma2, self.mem_dim)\n",
    "        self.gamma2_dropout = nn.Dropout(gamma2_dropout)\n",
    "\n",
    "        self.out_fc1 = nn.Linear(final_out, h_out)\n",
    "        self.out_fc2 = nn.Linear(h_out, output_dim)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "        \n",
    "        ## add the part for grud missing imputation \n",
    "        self.x_mean = X_mean \n",
    "        self.hidden_size = self.dh_l\n",
    "        \n",
    "        \n",
    "        ## \n",
    "#         self.gamma_decay = nn.Linear(self.d_l+self.d_a, self.hidden_size)\n",
    "#         self.map_gamma_decay = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "        \n",
    "    def squeeze_d2(self, matrix):\n",
    "        return torch.squeeze(matrix, dim=2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_l = x[:,:,:,:self.d_l]\n",
    "        x_a = x[:,:,:,self.d_l:self.d_l+self.d_a]\n",
    "        # x is n x c x t x d\n",
    "        # seperate x_mean_l and x_mean_a \n",
    "        x_mean_l = self.x_mean[:,:self.d_l]\n",
    "        x_mean_a = self.x_mean[:,self.d_l:self.d_l+self.d_a]\n",
    "        \n",
    "        n = x.shape[0]\n",
    "        t = x.shape[2]\n",
    "        \n",
    "        self.h_l = torch.zeros(n, self.dh_l)\n",
    "        self.h_a = torch.zeros(n, self.dh_a)\n",
    "        \n",
    "        self.mem = torch.zeros(n, self.mem_dim)\n",
    "        all_h_ls = []\n",
    "        all_h_as = []\n",
    "\n",
    "        all_mems = []\n",
    "        \n",
    "        for i in range(t):\n",
    "            # prev time step\n",
    "            prev_h_l = self.h_l\n",
    "            prev_h_a = self.h_a\n",
    "#             print(\"x_l size\")\n",
    "#             print(x_l.size())\n",
    "            # curr time step \n",
    "            new_h_l = self.gru_l(self.squeeze_d2(x_l[:,:,i:i+1,:]), x_mean_l[i:i+1,:], prev_h_l)\n",
    "            new_h_a = self.gru_a(self.squeeze_d2(x_a[:,:,i:i+1,:]), x_mean_a[i:i+1,:], prev_h_a)\n",
    "            # curr time step\n",
    "#             new_h_l, new_c_l = self.lstm_l(x_l[i], (self.h_l, self.c_l))\n",
    "#             new_h_a, new_c_a = self.lstm_a(x_a[i], (self.h_a, self.c_a))\n",
    "   \n",
    "            # concatenate\n",
    "            prev_hs = torch.cat([prev_h_l,prev_h_a], dim=1)\n",
    "            new_hs = torch.cat([new_h_l,new_h_a], dim=1)\n",
    "            \n",
    "            cStar = torch.cat([prev_hs,new_hs], dim=1)\n",
    "            attention = F.softmax(self.att1_fc2(self.att1_dropout(F.relu(self.att1_fc1(cStar)))),dim=1)\n",
    "            attended = attention*cStar\n",
    "            \n",
    "            cHat = F.tanh(self.att2_fc2(self.att2_dropout(F.relu(self.att2_fc1(attended)))))\n",
    "            \n",
    "            both = torch.cat([attended,self.mem], dim=1)\n",
    "            gamma1 = F.sigmoid(self.gamma1_fc2(self.gamma1_dropout(F.relu(self.gamma1_fc1(both)))))\n",
    "            gamma2 = F.sigmoid(self.gamma2_fc2(self.gamma2_dropout(F.relu(self.gamma2_fc1(both)))))\n",
    "            self.mem = gamma1*self.mem + gamma2*cHat\n",
    "        \n",
    "            # decay teh memory according to delta\n",
    "#             gamma_decay = F.sigmoid(self.map_gamma_decay(F.relu(self.gamma_decay(torch.squeeze(x[:,3,i:i+1,:], dim=1)))))\n",
    "#             self.mem = self.mem * gamma_decay \n",
    "        \n",
    "            # record delta condition for memory \n",
    "            \n",
    "            all_mems.append(self.mem)\n",
    "            # update\n",
    "            self.h_l = new_h_l\n",
    "            self.h_a = new_h_a\n",
    "\n",
    "            all_h_ls.append(self.h_l)\n",
    "            all_h_as.append(self.h_a)\n",
    "\n",
    "        # last hidden layer last_hs is n x h\n",
    "        last_h_l = all_h_ls[-1]\n",
    "        last_h_a = all_h_as[-1]\n",
    "\n",
    "        last_mem = all_mems[-1]\n",
    "        last_hs = torch.cat([last_h_l,last_h_a,last_mem], dim=1)\n",
    "        output = self.out_fc2(self.out_dropout(F.relu(self.out_fc1(last_hs))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 4, 7, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aggr[:,:,:,0:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 52)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_mean_aft_nor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 41.124459402901785 33.30825424194336 saving model\n",
      "1 40.5195552280971 32.57960891723633 saving model\n",
      "2 39.12946896325974 30.645034790039062 saving model\n",
      "3 35.56536538260324 25.559152603149414 saving model\n",
      "4 29.912216640654066 19.538232803344727 saving model\n",
      "5 26.86618200937907 18.482820510864258 saving model\n",
      "6 27.514121055603027 19.137409210205078\n",
      "7 25.076367786952428 18.531661987304688\n",
      "8 25.83615934281122 18.748781204223633\n",
      "9 26.572116079784575 18.411460876464844 saving model\n",
      "10 25.64906183878581 18.386533737182617 saving model\n",
      "11 24.23189876193092 17.928932189941406 saving model\n",
      "12 24.889502616155717 17.891202926635742 saving model\n",
      "13 26.096662067231676 18.072412490844727\n",
      "14 25.13059679667155 17.75757598876953 saving model\n",
      "15 25.402059055510023 18.022396087646484\n",
      "16 25.79093755994524 17.790239334106445\n",
      "17 25.201192356291273 17.72968101501465 saving model\n",
      "18 24.611724581037247 17.29775619506836 saving model\n",
      "19 24.181691396804084 17.749574661254883\n",
      "20 24.772257486979168 17.565664291381836\n",
      "21 22.954859915233794 17.358978271484375\n",
      "22 24.602683248974028 17.929447174072266\n",
      "23 23.56189051128569 17.251800537109375 saving model\n",
      "24 25.007737023489817 17.27961540222168\n",
      "25 24.741470336914062 17.36103057861328\n",
      "26 24.47867084684826 17.4547119140625\n",
      "27 23.142966679164342 17.303539276123047\n",
      "28 23.899612835475377 17.392763137817383\n",
      "29 24.284215609232586 17.2897891998291\n",
      "30 24.562842823210218 17.165332794189453 saving model\n",
      "31 22.961286998930433 17.056446075439453 saving model\n",
      "32 23.195610318865096 17.24688148498535\n",
      "33 24.30388609568278 17.368528366088867\n",
      "34 23.79854152316139 17.079673767089844\n",
      "35 24.30060795375279 17.353248596191406\n",
      "36 23.828256062098912 17.414133071899414\n",
      "37 23.554139455159504 16.97313117980957 saving model\n",
      "38 22.827372687203543 17.127195358276367\n",
      "39 23.48395115988595 17.06590461730957\n",
      "40 23.221526917957124 17.08652114868164\n",
      "41 23.488334292457218 17.18412971496582\n",
      "42 23.121100244067964 17.20036506652832\n",
      "43 24.159555662245978 17.696922302246094\n",
      "44 22.77284013657343 16.731828689575195 saving model\n",
      "45 23.188800403050013 17.030742645263672\n",
      "46 23.203325634910946 16.69264793395996 saving model\n",
      "47 22.7793946039109 16.70061683654785\n",
      "48 23.07972440265474 17.06167984008789\n",
      "49 22.81265672047933 16.609270095825195 saving model\n",
      "50 22.720981280008953 16.828533172607422\n",
      "51 22.468419710795086 16.519210815429688 saving model\n",
      "52 22.75138518923805 16.53016471862793\n",
      "53 22.011319523765927 16.60893440246582\n",
      "54 22.554506211053756 16.623065948486328\n",
      "55 21.802204904102144 16.2175235748291 saving model\n",
      "56 22.080824125380744 16.39542579650879\n",
      "57 21.673962275187176 16.392141342163086\n",
      "58 22.789177440461657 16.255268096923828\n",
      "59 23.223377908979142 16.213457107543945 saving model\n",
      "60 21.808925537835982 16.20979118347168 saving model\n",
      "61 22.399271510896227 16.06138038635254 saving model\n",
      "62 22.71500723702567 15.848421096801758 saving model\n",
      "63 22.106543223063152 16.060911178588867\n",
      "64 22.076809610639298 16.021575927734375\n",
      "65 21.143496922084264 15.902219772338867\n",
      "66 22.152718180701847 15.81072998046875 saving model\n",
      "67 22.93106156303769 15.61396312713623 saving model\n",
      "68 21.58006000518799 16.103057861328125\n",
      "69 21.883910724094935 15.583955764770508 saving model\n",
      "70 21.985848018101283 15.682981491088867\n",
      "71 22.268335796537855 15.635354042053223\n",
      "72 21.762296676635742 15.418317794799805 saving model\n",
      "73 20.81574753352574 15.494551658630371\n",
      "74 21.075079554603214 15.529537200927734\n",
      "75 21.789382344200497 15.6093111038208\n",
      "76 21.888127099900018 15.33000659942627 saving model\n",
      "77 22.055494717189244 15.77659797668457\n",
      "78 22.06809711456299 15.345670700073242\n",
      "79 21.770098595392135 15.671720504760742\n",
      "80 20.367322649274552 15.392427444458008\n",
      "81 22.307368732634046 15.352779388427734\n",
      "82 21.368307340712775 15.464759826660156\n",
      "83 21.61270032610212 15.449604034423828\n",
      "Epoch    85: reducing learning rate of group 0 to 2.5000e-04.\n",
      "84 21.467550413949148 15.4871244430542\n",
      "85 20.683950515020463 15.513459205627441\n",
      "86 20.577764329456148 15.283411026000977 saving model\n",
      "87 20.575505847022647 15.2490816116333 saving model\n",
      "88 20.263564064389183 15.407742500305176\n",
      "89 20.48919661839803 15.478489875793457\n",
      "90 19.831033570425852 15.544336318969727\n",
      "91 20.0487790788923 15.248830795288086 saving model\n",
      "92 20.171673411414737 15.376913070678711\n",
      "93 20.587602751595632 15.190868377685547 saving model\n",
      "94 20.71610650562105 15.359492301940918\n",
      "95 19.800459089733305 15.274494171142578\n",
      "96 20.08498291742234 15.313854217529297\n",
      "97 20.190649259658088 15.361647605895996\n",
      "98 20.2319701058524 15.119102478027344 saving model\n",
      "99 20.087928953624907 15.335658073425293\n",
      "100 20.452038492475236 15.216536521911621\n",
      "101 20.55726196652367 15.319296836853027\n",
      "102 21.190474283127557 15.332975387573242\n",
      "103 19.95406895592099 15.39495849609375\n",
      "104 20.346021515982493 15.434868812561035\n",
      "105 21.226049786522275 15.390657424926758\n",
      "Epoch   107: reducing learning rate of group 0 to 1.2500e-04.\n",
      "106 19.61531975155785 15.34852409362793\n",
      "107 19.854909669785272 15.307823181152344\n",
      "108 19.983428500947497 15.254728317260742\n",
      "109 19.542534283229283 15.192391395568848\n",
      "110 19.696923800877162 15.340258598327637\n",
      "111 20.121640931992303 15.262030601501465\n",
      "112 19.90248929886591 15.21208667755127\n",
      "113 19.811000869387673 15.350442886352539\n",
      "Epoch   115: reducing learning rate of group 0 to 6.2500e-05.\n",
      "114 19.822914441426594 15.465127944946289\n",
      "115 19.668221564519975 15.431411743164062\n",
      "116 19.934750693184988 15.350027084350586\n",
      "117 19.492469151814777 15.296353340148926\n",
      "118 19.95139403570266 15.31385326385498\n",
      "119 20.184689612615678 15.367199897766113\n",
      "120 19.79659144083659 15.41848373413086\n",
      "121 19.432951700119744 15.405590057373047\n",
      "Epoch   123: reducing learning rate of group 0 to 3.1250e-05.\n",
      "122 19.677561532883416 15.355002403259277\n",
      "123 19.577741986229306 15.327648162841797\n",
      "124 19.765463783627464 15.353219032287598\n",
      "125 19.39037018730527 15.371108055114746\n",
      "126 19.008214133126394 15.399609565734863\n",
      "127 19.792498997279576 15.377470016479492\n",
      "128 18.760366076514835 15.391860008239746\n",
      "129 19.628830092293875 15.424689292907715\n",
      "Epoch   131: reducing learning rate of group 0 to 1.5625e-05.\n",
      "130 19.777312051682244 15.43620491027832\n",
      "131 18.648863156636555 15.415604591369629\n",
      "132 19.212990806216286 15.429190635681152\n",
      "133 19.076646214439755 15.43901252746582\n",
      "134 20.147735868181503 15.490426063537598\n",
      "135 19.38639177594866 15.489492416381836\n",
      "136 20.118746394202823 15.470564842224121\n",
      "137 19.896215983799525 15.459712028503418\n",
      "Epoch   139: reducing learning rate of group 0 to 7.8125e-06.\n",
      "138 20.108681224641344 15.402551651000977\n",
      "139 19.55269495646159 15.393599510192871\n",
      "140 19.215901647295272 15.392416000366211\n",
      "141 19.32671814873105 15.390616416931152\n",
      "142 19.99153305235363 15.392000198364258\n",
      "143 19.999973887488956 15.393092155456543\n",
      "144 19.359991573152087 15.392930030822754\n",
      "145 18.972451437087287 15.391951560974121\n",
      "Epoch   147: reducing learning rate of group 0 to 3.9063e-06.\n",
      "146 19.437838009425572 15.393579483032227\n",
      "147 19.56378110249837 15.393213272094727\n",
      "148 19.44779364267985 15.399827003479004\n",
      "149 19.658711751302082 15.407179832458496\n",
      "mae:  3.068823474941176\n",
      "mse:  14.797966639836307\n"
     ]
    }
   ],
   "source": [
    "# pol + met \n",
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 128\n",
    "ha = 128\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = 32\n",
    "config[\"num_epochs\"] = 150\n",
    "config[\"lr\"] = 0.0005\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = 32\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(train_aggr[:,:,:,0:5], label_train, dev_aggr[:,:,:,0:5], label_dev, test_aggr[:,:,:,0:5], label_test, configs, x_mean_aft_nor[:, 0:5])\n",
    "# train_grud(train_aggr, label_train, dev_aggr, label_dev, test_aggr, label_test, config, x_mean_aft_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 41.124459402901785 33.30825424194336 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type MFN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type GRUD. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type FilterLinear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 40.5195552280971 32.57960891723633 saving model\n",
      "2 39.12946896325974 30.645034790039062 saving model\n",
      "3 35.56536538260324 25.559152603149414 saving model\n",
      "4 29.912216640654066 19.538232803344727 saving model\n",
      "5 26.86618200937907 18.482820510864258 saving model\n",
      "6 27.514121055603027 19.137409210205078\n",
      "7 25.076367786952428 18.531661987304688\n",
      "8 25.83615934281122 18.748781204223633\n",
      "9 26.572116079784575 18.411460876464844 saving model\n",
      "10 25.64906183878581 18.386533737182617 saving model\n",
      "11 24.23189876193092 17.928932189941406 saving model\n",
      "12 24.889502616155717 17.891202926635742 saving model\n",
      "13 26.096662067231676 18.072412490844727\n",
      "14 25.13059679667155 17.75757598876953 saving model\n",
      "15 25.402059055510023 18.022396087646484\n",
      "16 25.79093755994524 17.790239334106445\n",
      "17 25.201192356291273 17.72968101501465 saving model\n",
      "18 24.611724581037247 17.29775619506836 saving model\n",
      "19 24.181691396804084 17.749574661254883\n",
      "20 24.772257486979168 17.565664291381836\n",
      "21 22.954859915233794 17.358978271484375\n",
      "22 24.602683248974028 17.929447174072266\n",
      "23 23.56189051128569 17.251800537109375 saving model\n",
      "24 25.007737023489817 17.27961540222168\n",
      "25 24.741470336914062 17.36103057861328\n",
      "26 24.47867084684826 17.4547119140625\n",
      "27 23.142966679164342 17.303539276123047\n",
      "28 23.899612835475377 17.392763137817383\n",
      "29 24.284215609232586 17.2897891998291\n",
      "30 24.562842823210218 17.165332794189453 saving model\n",
      "31 22.961286998930433 17.056446075439453 saving model\n",
      "32 23.195610318865096 17.24688148498535\n",
      "33 24.30388609568278 17.368528366088867\n",
      "34 23.79854152316139 17.079673767089844\n",
      "35 24.30060795375279 17.353248596191406\n",
      "36 23.828256062098912 17.414133071899414\n",
      "37 23.554139455159504 16.97313117980957 saving model\n",
      "38 22.827372687203543 17.127195358276367\n",
      "39 23.48395115988595 17.06590461730957\n",
      "40 23.221526917957124 17.08652114868164\n",
      "41 23.488334292457218 17.18412971496582\n",
      "42 23.121100244067964 17.20036506652832\n",
      "43 24.159555662245978 17.696922302246094\n",
      "44 22.77284013657343 16.731828689575195 saving model\n",
      "45 23.188800403050013 17.030742645263672\n",
      "46 23.203325634910946 16.69264793395996 saving model\n",
      "47 22.7793946039109 16.70061683654785\n",
      "48 23.07972440265474 17.06167984008789\n",
      "49 22.81265672047933 16.609270095825195 saving model\n",
      "50 22.720981280008953 16.828533172607422\n",
      "51 22.468419710795086 16.519210815429688 saving model\n",
      "52 22.75138518923805 16.53016471862793\n",
      "53 22.011319523765927 16.60893440246582\n",
      "54 22.554506211053756 16.623065948486328\n",
      "55 21.802204904102144 16.2175235748291 saving model\n",
      "56 22.080824125380744 16.39542579650879\n",
      "57 21.673962275187176 16.392141342163086\n",
      "58 22.789177440461657 16.255268096923828\n",
      "59 23.223377908979142 16.213457107543945 saving model\n",
      "60 21.808925537835982 16.20979118347168 saving model\n",
      "61 22.399271510896227 16.06138038635254 saving model\n",
      "62 22.71500723702567 15.848421096801758 saving model\n",
      "63 22.106543223063152 16.060911178588867\n",
      "64 22.076809610639298 16.021575927734375\n",
      "65 21.143496922084264 15.902219772338867\n",
      "66 22.152718180701847 15.81072998046875 saving model\n",
      "67 22.93106156303769 15.61396312713623 saving model\n",
      "68 21.58006000518799 16.103057861328125\n",
      "69 21.883910724094935 15.583955764770508 saving model\n",
      "70 21.985848018101283 15.682981491088867\n",
      "71 22.268335796537855 15.635354042053223\n",
      "72 21.762296676635742 15.418317794799805 saving model\n",
      "73 20.81574753352574 15.494551658630371\n",
      "74 21.075079554603214 15.529537200927734\n",
      "75 21.789382344200497 15.6093111038208\n",
      "76 21.888127099900018 15.33000659942627 saving model\n",
      "77 22.055494717189244 15.77659797668457\n",
      "78 22.06809711456299 15.345670700073242\n",
      "79 21.770098595392135 15.671720504760742\n",
      "mae:  3.0318519705829545\n",
      "mse:  14.356297171220174\n"
     ]
    }
   ],
   "source": [
    "# pol + met \n",
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 128\n",
    "ha = 128\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = 32\n",
    "config[\"num_epochs\"] = 80\n",
    "config[\"lr\"] = 0.0005\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = 32\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(train_aggr[:,:,:,0:5], label_train, dev_aggr[:,:,:,0:5], label_dev, test_aggr[:,:,:,0:5], label_test, configs, x_mean_aft_nor[:, 0:5])\n",
    "# train_grud(train_aggr, label_train, dev_aggr, label_dev, test_aggr, label_test, config, x_mean_aft_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
