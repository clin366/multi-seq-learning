{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/predict-one-day-diff/pol-met-search'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control experiment using last observed value for missing data imputation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 7, 52)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mfn(X_train, y_train, X_valid, y_valid, X_test, y_test, configs):\n",
    "#     p = np.random.permutation(X_train.shape[0])\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "    def swap_axes(nparr):\n",
    "        return nparr.swapaxes(0,1)\n",
    "    X_train = swap_axes(X_train)\n",
    "    X_valid = swap_axes(X_valid)\n",
    "    X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters \n",
    "    input_size = X_train.shape[2]\n",
    "    h = 128\n",
    "    t = X_train.shape[0]\n",
    "    output_dim = 1\n",
    "    dropout = 0.5\n",
    "\n",
    "#     d = X_train.shape[2]\n",
    "#     h = 128\n",
    "#     t = X_train.shape[0]\n",
    "#     output_dim = 1\n",
    "#     dropout = 0.5\n",
    "\n",
    "    [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig] = configs\n",
    "\n",
    "    \n",
    "    #model = EFLSTM(d,h,output_dim,dropout)\n",
    "    model = MFN(config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "    #optimizer = optim.SGD(model.parameters(),lr=config[\"lr\"],momentum=config[\"momentum\"])\n",
    "\n",
    "    # optimizer = optim.SGD([\n",
    "    #                 {'params':model.lstm_l.parameters(), 'lr':config[\"lr\"]},\n",
    "    #                 {'params':model.classifier.parameters(), 'lr':config[\"lr\"]}\n",
    "    #             ], momentum=0.9)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "#     criterion = nn.L1Loss()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer,mode='min',patience=100,factor=0.5,verbose=True)\n",
    "\n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[1]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[:,start:end])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/temp_models/mfn_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "#     print 'model number is:', rand\n",
    "    model = torch.load('models/temp_models/mfn_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFN(nn.Module):\n",
    "    def __init__(self,config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig):\n",
    "        super(MFN, self).__init__()\n",
    "        [self.d_l,self.d_a] = config[\"input_dims\"]\n",
    "        [self.dh_l,self.dh_a] = config[\"h_dims\"]\n",
    "        total_h_dim = self.dh_l+self.dh_a\n",
    "        \n",
    "        self.mem_dim = config[\"memsize\"]\n",
    "        window_dim = config[\"windowsize\"]\n",
    "        output_dim = 1\n",
    "        attInShape = total_h_dim*window_dim\n",
    "        gammaInShape = attInShape+self.mem_dim\n",
    "        final_out = total_h_dim+self.mem_dim\n",
    "        h_att1 = NN1Config[\"shapes\"]\n",
    "        h_att2 = NN2Config[\"shapes\"]\n",
    "        h_gamma1 = gamma1Config[\"shapes\"]\n",
    "        h_gamma2 = gamma2Config[\"shapes\"]\n",
    "        h_out = outConfig[\"shapes\"]\n",
    "        att1_dropout = NN1Config[\"drop\"]\n",
    "        att2_dropout = NN2Config[\"drop\"]\n",
    "        gamma1_dropout = gamma1Config[\"drop\"]\n",
    "        gamma2_dropout = gamma2Config[\"drop\"]\n",
    "        out_dropout = outConfig[\"drop\"]\n",
    "\n",
    "        self.lstm_l = nn.GRUCell(self.d_l, self.dh_l)\n",
    "        self.lstm_a = nn.GRUCell(self.d_a, self.dh_a)\n",
    "\n",
    "        self.att1_fc1 = nn.Linear(attInShape, h_att1)\n",
    "        self.att1_fc2 = nn.Linear(h_att1, attInShape)\n",
    "        self.att1_dropout = nn.Dropout(att1_dropout)\n",
    "\n",
    "        self.att2_fc1 = nn.Linear(attInShape, h_att2)\n",
    "        self.att2_fc2 = nn.Linear(h_att2, self.mem_dim)\n",
    "        self.att2_dropout = nn.Dropout(att2_dropout)\n",
    "\n",
    "        self.gamma1_fc1 = nn.Linear(gammaInShape, h_gamma1)\n",
    "        self.gamma1_fc2 = nn.Linear(h_gamma1, self.mem_dim)\n",
    "        self.gamma1_dropout = nn.Dropout(gamma1_dropout)\n",
    "\n",
    "        self.gamma2_fc1 = nn.Linear(gammaInShape, h_gamma2)\n",
    "        self.gamma2_fc2 = nn.Linear(h_gamma2, self.mem_dim)\n",
    "        self.gamma2_dropout = nn.Dropout(gamma2_dropout)\n",
    "\n",
    "        self.out_fc1 = nn.Linear(final_out, h_out)\n",
    "        self.out_fc2 = nn.Linear(h_out, output_dim)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_l = x[:,:,:self.d_l]\n",
    "        x_a = x[:,:,self.d_l:self.d_l+self.d_a]\n",
    "        # x is t x n x d\n",
    "        n = x.shape[1]\n",
    "        t = x.shape[0]\n",
    "        self.h_l = torch.zeros(n, self.dh_l)\n",
    "        self.h_a = torch.zeros(n, self.dh_a)\n",
    "\n",
    "        self.c_l = torch.zeros(n, self.dh_l)\n",
    "        self.c_a = torch.zeros(n, self.dh_a)\n",
    "        \n",
    "        self.mem = torch.zeros(n, self.mem_dim)\n",
    "        all_h_ls = []\n",
    "        all_h_as = []\n",
    "\n",
    "        all_c_ls = []\n",
    "        all_c_as = []\n",
    "\n",
    "        all_mems = []\n",
    "        for i in range(t):\n",
    "            # prev time step\n",
    "            prev_h_l = self.h_l\n",
    "            prev_h_a = self.h_a\n",
    "\n",
    "            # curr time step\n",
    "#             exit(0)\n",
    "            new_h_l = self.lstm_l(x_l[i], self.h_l)\n",
    "            new_h_a = self.lstm_a(x_a[i], self.h_a)\n",
    "   \n",
    "            # concatenate\n",
    "            prev_cs = torch.cat([prev_h_l,prev_h_a], dim=1)\n",
    "            new_cs = torch.cat([new_h_l,new_h_a], dim=1)\n",
    "            \n",
    "            cStar = torch.cat([prev_cs,new_cs], dim=1)\n",
    "            attention = F.softmax(self.att1_fc2(self.att1_dropout(F.relu(self.att1_fc1(cStar)))),dim=1)\n",
    "            attended = attention*cStar\n",
    "            \n",
    "            cHat = F.tanh(self.att2_fc2(self.att2_dropout(F.relu(self.att2_fc1(attended)))))\n",
    "            \n",
    "            both = torch.cat([attended,self.mem], dim=1)\n",
    "            gamma1 = F.sigmoid(self.gamma1_fc2(self.gamma1_dropout(F.relu(self.gamma1_fc1(both)))))\n",
    "            gamma2 = F.sigmoid(self.gamma2_fc2(self.gamma2_dropout(F.relu(self.gamma2_fc1(both)))))\n",
    "            \n",
    "            self.mem = gamma1*self.mem + gamma2*cHat\n",
    "            all_mems.append(self.mem)\n",
    "            # update\n",
    "            self.h_l = new_h_l\n",
    "            self.h_a = new_h_a\n",
    "\n",
    "            all_h_ls.append(self.h_l)\n",
    "            all_h_as.append(self.h_a)\n",
    "\n",
    "        # last hidden layer last_hs is n x h\n",
    "        last_h_l = all_h_ls[-1]\n",
    "        last_h_a = all_h_as[-1]\n",
    "\n",
    "        last_mem = all_mems[-1]\n",
    "        last_hs = torch.cat([last_h_l,last_h_a,last_mem], dim=1)\n",
    "        output = self.out_fc2(self.out_dropout(F.relu(self.out_fc1(last_hs))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 7, 52)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 40.64848395756313 32.172306060791016 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type MFN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 38.42827447255453 28.981014251708984 saving model\n",
      "2 34.58909429822649 23.673786163330078 saving model\n",
      "3 29.38978844597226 19.39164161682129 saving model\n",
      "4 26.0859858194987 19.015167236328125 saving model\n",
      "5 25.692779041471937 18.97467613220215 saving model\n",
      "6 25.395569438026065 19.07463836669922\n",
      "7 25.333875429062616 18.591068267822266 saving model\n",
      "8 24.80284818013509 18.90890884399414\n",
      "9 23.839193571181525 18.6917724609375\n",
      "10 23.45577398935954 18.540889739990234 saving model\n",
      "11 24.918754804702033 18.686447143554688\n",
      "12 23.612905820210774 18.473556518554688 saving model\n",
      "13 22.95072927929106 18.91614532470703\n",
      "14 22.703480448041642 18.783994674682617\n",
      "15 23.82614703405471 19.088760375976562\n",
      "16 22.4011386235555 18.70659637451172\n",
      "17 23.4459475562686 18.734678268432617\n",
      "18 22.72279911949521 19.22016143798828\n",
      "19 22.593629428318568 18.57927703857422\n",
      "20 22.630849792843772 18.906837463378906\n",
      "21 21.589904467264812 18.87853240966797\n",
      "22 22.329895564488 18.934772491455078\n",
      "Epoch    24: reducing learning rate of group 0 to 2.5000e-04.\n",
      "23 21.344862710861932 18.612499237060547\n",
      "24 22.379260517302015 18.77307891845703\n",
      "25 21.420682044256303 18.86109161376953\n",
      "26 20.698562122526624 18.94015884399414\n",
      "27 20.826336633591424 19.217111587524414\n",
      "28 21.644082659766788 18.693403244018555\n",
      "29 21.648526645842054 18.867361068725586\n",
      "30 21.391981442769367 19.206571578979492\n",
      "31 20.848211969648087 19.164222717285156\n",
      "32 20.983055795942033 18.80923080444336\n",
      "33 20.98212414696103 18.868328094482422\n",
      "Epoch    35: reducing learning rate of group 0 to 1.2500e-04.\n",
      "34 22.406757672627766 19.155630111694336\n",
      "35 21.270483698163712 19.117408752441406\n",
      "36 20.332504408700125 18.998722076416016\n",
      "37 20.3134674344744 19.01634407043457\n",
      "38 19.793425196693057 19.121463775634766\n",
      "39 20.692667779468355 18.906166076660156\n",
      "40 20.127159481956845 19.035432815551758\n",
      "41 20.243763605753582 19.000024795532227\n",
      "42 19.85394850231352 19.12110710144043\n",
      "43 20.486983753386 19.222806930541992\n",
      "44 20.08629848843529 19.1580810546875\n",
      "Epoch    46: reducing learning rate of group 0 to 6.2500e-05.\n",
      "45 20.196207682291668 18.95734214782715\n",
      "46 20.151503108796618 19.00525665283203\n",
      "47 20.331644012814476 19.044082641601562\n",
      "48 20.204532305399578 19.179271697998047\n",
      "49 20.68315179007394 19.1876277923584\n",
      "mae:  3.2553852542742225\n",
      "mse:  15.990876127299167\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 128\n",
    "ha = 128\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = 32\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.0005\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = hl\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train[:,:,0:5], label_train, last_dev[:,:,0:5], label_dev, last_test[:,:,0:5], label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 40.64848395756313 32.172306060791016 saving model\n",
      "1 38.42827447255453 28.981014251708984 saving model\n",
      "2 34.58909429822649 23.673786163330078 saving model\n",
      "3 29.38978844597226 19.39164161682129 saving model\n",
      "4 26.0859858194987 19.015167236328125 saving model\n",
      "5 25.692779041471937 18.97467613220215 saving model\n",
      "6 25.395569438026065 19.07463836669922\n",
      "7 25.333875429062616 18.591068267822266 saving model\n",
      "8 24.80284818013509 18.90890884399414\n",
      "9 23.839193571181525 18.6917724609375\n",
      "10 23.45577398935954 18.540889739990234 saving model\n",
      "11 24.918754804702033 18.686447143554688\n",
      "12 23.612905820210774 18.473556518554688 saving model\n",
      "13 22.95072927929106 18.91614532470703\n",
      "14 22.703480448041642 18.783994674682617\n",
      "15 23.82614703405471 19.088760375976562\n",
      "16 22.4011386235555 18.70659637451172\n",
      "17 23.4459475562686 18.734678268432617\n",
      "18 22.72279911949521 19.22016143798828\n",
      "19 22.593629428318568 18.57927703857422\n",
      "20 22.630849792843772 18.906837463378906\n",
      "21 21.589904467264812 18.87853240966797\n",
      "22 22.329895564488 18.934772491455078\n",
      "Epoch    24: reducing learning rate of group 0 to 2.5000e-04.\n",
      "23 21.344862710861932 18.612499237060547\n",
      "24 22.379260517302015 18.77307891845703\n",
      "25 21.420682044256303 18.86109161376953\n",
      "26 20.698562122526624 18.94015884399414\n",
      "27 20.826336633591424 19.217111587524414\n",
      "28 21.644082659766788 18.693403244018555\n",
      "29 21.648526645842054 18.867361068725586\n",
      "30 21.391981442769367 19.206571578979492\n",
      "31 20.848211969648087 19.164222717285156\n",
      "32 20.983055795942033 18.80923080444336\n",
      "33 20.98212414696103 18.868328094482422\n",
      "Epoch    35: reducing learning rate of group 0 to 1.2500e-04.\n",
      "34 22.406757672627766 19.155630111694336\n",
      "35 21.270483698163712 19.117408752441406\n",
      "36 20.332504408700125 18.998722076416016\n",
      "37 20.3134674344744 19.01634407043457\n",
      "38 19.793425196693057 19.121463775634766\n",
      "39 20.692667779468355 18.906166076660156\n",
      "40 20.127159481956845 19.035432815551758\n",
      "41 20.243763605753582 19.000024795532227\n",
      "42 19.85394850231352 19.12110710144043\n",
      "43 20.486983753386 19.222806930541992\n",
      "44 20.08629848843529 19.1580810546875\n",
      "Epoch    46: reducing learning rate of group 0 to 6.2500e-05.\n",
      "45 20.196207682291668 18.95734214782715\n",
      "46 20.151503108796618 19.00525665283203\n",
      "47 20.331644012814476 19.044082641601562\n",
      "48 20.204532305399578 19.179271697998047\n",
      "49 20.68315179007394 19.1876277923584\n",
      "mae:  3.2553852542742225\n",
      "mse:  15.990876127299167\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 128\n",
    "ha = 128\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = 32\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.0005\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = hl\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train[:,:,0:5], label_train, last_dev[:,:,0:5], label_dev, last_test[:,:,0:5], label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36.76633980160668 22.68921661376953 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type MFN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 26.650658289591473 18.734664916992188 saving model\n",
      "2 27.01508240472703 19.434616088867188\n",
      "3 24.296658470517112 18.99413299560547\n",
      "4 23.259517942156112 18.831459045410156\n",
      "5 23.14715812319801 19.12861442565918\n",
      "6 23.037480127243768 18.802019119262695\n",
      "7 22.624505678812664 19.084245681762695\n",
      "8 20.873754728408088 19.046112060546875\n",
      "9 21.695666131519136 19.361303329467773\n",
      "10 20.86329105922154 18.83840560913086\n",
      "11 20.559358733040945 19.44209098815918\n",
      "12 20.419234048752557 17.910724639892578 saving model\n",
      "13 20.6493467603411 20.808439254760742\n",
      "14 19.174366860162642 19.30905532836914\n",
      "15 20.435554686046782 18.339069366455078\n",
      "16 19.844137146359397 19.49654769897461\n",
      "17 18.848063968476794 21.234594345092773\n",
      "18 19.203963188897994 19.71392059326172\n",
      "19 18.338015011378697 19.369653701782227\n",
      "20 17.59056804293678 20.757963180541992\n",
      "21 18.275400706699916 20.173341751098633\n",
      "22 17.420775231860933 20.804492950439453\n",
      "Epoch    24: reducing learning rate of group 0 to 5.0000e-04.\n",
      "23 17.2085835366022 21.121580123901367\n",
      "24 16.65951955886114 21.005659103393555\n",
      "25 16.334413028898695 21.862939834594727\n",
      "26 15.617996306646438 21.608325958251953\n",
      "27 15.159543718610491 21.835481643676758\n",
      "28 14.644888628096808 22.29857635498047\n",
      "29 15.696639469691686 21.213668823242188\n",
      "30 14.591873691195534 22.78476333618164\n",
      "31 14.571635064624605 22.286624908447266\n",
      "32 14.591531185876756 21.65808868408203\n",
      "33 14.158636388324556 23.130634307861328\n",
      "Epoch    35: reducing learning rate of group 0 to 2.5000e-04.\n",
      "34 13.522816816965738 21.974027633666992\n",
      "35 13.50098351069859 22.900718688964844\n",
      "36 13.314424264998664 22.57052993774414\n",
      "37 13.3667634782337 23.416732788085938\n",
      "38 12.510849861871629 23.611520767211914\n",
      "39 13.241562366485596 23.495697021484375\n",
      "40 12.362538405827113 23.204795837402344\n",
      "41 11.66415800367083 23.315139770507812\n",
      "42 12.274400302342006 23.95343780517578\n",
      "43 12.338026500883556 23.98634910583496\n",
      "44 11.981553100404286 23.923786163330078\n",
      "Epoch    46: reducing learning rate of group 0 to 1.2500e-04.\n",
      "45 12.067000264213199 23.94011116027832\n",
      "46 12.375821635836648 24.16604995727539\n",
      "47 12.235764560245332 24.135225296020508\n",
      "48 11.75409520240057 24.655643463134766\n",
      "49 11.68047285079956 24.398069381713867\n",
      "mae:  2.9849512498979722\n",
      "mse:  13.977365397911518\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 256\n",
    "ha = 256\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = 32\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.001\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = hl\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train[:,:,0:5], label_train, last_dev[:,:,0:5], label_dev, last_test[:,:,0:5], label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFN(nn.Module):\n",
    "    def __init__(self,config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig):\n",
    "        super(MFN, self).__init__()\n",
    "        [self.d_l,self.d_a] = config[\"input_dims\"]\n",
    "        [self.dh_l,self.dh_a] = config[\"h_dims\"]\n",
    "        total_h_dim = self.dh_l+self.dh_a\n",
    "        \n",
    "        self.mem_dim = config[\"memsize\"]\n",
    "        window_dim = config[\"windowsize\"]\n",
    "        output_dim = 1\n",
    "        attInShape = total_h_dim*window_dim\n",
    "        gammaInShape = attInShape+self.mem_dim\n",
    "        final_out = total_h_dim+self.mem_dim\n",
    "        h_att1 = NN1Config[\"shapes\"]\n",
    "        h_att2 = NN2Config[\"shapes\"]\n",
    "        h_gamma1 = gamma1Config[\"shapes\"]\n",
    "        h_gamma2 = gamma2Config[\"shapes\"]\n",
    "        h_out = outConfig[\"shapes\"]\n",
    "        att1_dropout = NN1Config[\"drop\"]\n",
    "        att2_dropout = NN2Config[\"drop\"]\n",
    "        gamma1_dropout = gamma1Config[\"drop\"]\n",
    "        gamma2_dropout = gamma2Config[\"drop\"]\n",
    "        out_dropout = outConfig[\"drop\"]\n",
    "\n",
    "        self.lstm_l = nn.LSTMCell(self.d_l, self.dh_l)\n",
    "        self.lstm_a = nn.LSTMCell(self.d_a, self.dh_a)\n",
    "\n",
    "        self.att1_fc1 = nn.Linear(attInShape, h_att1)\n",
    "        self.att1_fc2 = nn.Linear(h_att1, attInShape)\n",
    "        self.att1_dropout = nn.Dropout(att1_dropout)\n",
    "\n",
    "        self.att2_fc1 = nn.Linear(attInShape, h_att2)\n",
    "        self.att2_fc2 = nn.Linear(h_att2, self.mem_dim)\n",
    "        self.att2_dropout = nn.Dropout(att2_dropout)\n",
    "\n",
    "        self.gamma1_fc1 = nn.Linear(gammaInShape, h_gamma1)\n",
    "        self.gamma1_fc2 = nn.Linear(h_gamma1, self.mem_dim)\n",
    "        self.gamma1_dropout = nn.Dropout(gamma1_dropout)\n",
    "\n",
    "        self.gamma2_fc1 = nn.Linear(gammaInShape, h_gamma2)\n",
    "        self.gamma2_fc2 = nn.Linear(h_gamma2, self.mem_dim)\n",
    "        self.gamma2_dropout = nn.Dropout(gamma2_dropout)\n",
    "\n",
    "        self.out_fc1 = nn.Linear(final_out, h_out)\n",
    "        self.out_fc2 = nn.Linear(h_out, output_dim)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_l = x[:,:,:self.d_l]\n",
    "        x_a = x[:,:,self.d_l:self.d_l+self.d_a]\n",
    "        # x is t x n x d\n",
    "        n = x.shape[1]\n",
    "        t = x.shape[0]\n",
    "        self.h_l = torch.zeros(n, self.dh_l)\n",
    "        self.h_a = torch.zeros(n, self.dh_a)\n",
    "\n",
    "        self.c_l = torch.zeros(n, self.dh_l)\n",
    "        self.c_a = torch.zeros(n, self.dh_a)\n",
    "        \n",
    "        self.mem = torch.zeros(n, self.mem_dim)\n",
    "        all_h_ls = []\n",
    "        all_h_as = []\n",
    "\n",
    "        all_c_ls = []\n",
    "        all_c_as = []\n",
    "\n",
    "        all_mems = []\n",
    "        for i in range(t):\n",
    "            # prev time step\n",
    "            prev_c_l = self.c_l\n",
    "            prev_c_a = self.c_a\n",
    "\n",
    "            # curr time step\n",
    "            new_h_l, new_c_l = self.lstm_l(x_l[i], (self.h_l, self.c_l))\n",
    "            new_h_a, new_c_a = self.lstm_a(x_a[i], (self.h_a, self.c_a))\n",
    "   \n",
    "            # concatenate\n",
    "            prev_cs = torch.cat([prev_c_l,prev_c_a], dim=1)\n",
    "            new_cs = torch.cat([new_c_l,new_c_a], dim=1)\n",
    "            \n",
    "            cStar = torch.cat([prev_cs,new_cs], dim=1)\n",
    "            attention = F.softmax(self.att1_fc2(self.att1_dropout(F.relu(self.att1_fc1(cStar)))),dim=1)\n",
    "            attended = attention*cStar\n",
    "            \n",
    "            cHat = F.tanh(self.att2_fc2(self.att2_dropout(F.relu(self.att2_fc1(attended)))))\n",
    "            \n",
    "            both = torch.cat([attended,self.mem], dim=1)\n",
    "            gamma1 = F.sigmoid(self.gamma1_fc2(self.gamma1_dropout(F.relu(self.gamma1_fc1(both)))))\n",
    "            gamma2 = F.sigmoid(self.gamma2_fc2(self.gamma2_dropout(F.relu(self.gamma2_fc1(both)))))\n",
    "            \n",
    "            self.mem = gamma1*self.mem + gamma2*cHat\n",
    "            all_mems.append(self.mem)\n",
    "            # update\n",
    "            self.h_l, self.c_l = new_h_l, new_c_l\n",
    "            self.h_a, self.c_a = new_h_a, new_c_a\n",
    "\n",
    "            all_h_ls.append(self.h_l)\n",
    "            all_h_as.append(self.h_a)\n",
    " \n",
    "            all_c_ls.append(self.c_l)\n",
    "            all_c_as.append(self.c_a)\n",
    "\n",
    "        # last hidden layer last_hs is n x h\n",
    "        last_h_l = all_h_ls[-1]\n",
    "        last_h_a = all_h_as[-1]\n",
    "\n",
    "        last_mem = all_mems[-1]\n",
    "        last_hs = torch.cat([last_h_l,last_h_a,last_mem], dim=1)\n",
    "        output = self.out_fc2(self.out_dropout(F.relu(self.out_fc1(last_hs))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 40.40646107991537 33.01768493652344 saving model\n",
      "1 39.760721842447914 32.0798225402832 saving model\n",
      "2 38.700338999430336 30.67528533935547 saving model\n",
      "3 37.22062428792318 28.808597564697266 saving model\n",
      "4 35.17368825276693 26.693241119384766 saving model\n",
      "5 33.8869997660319 24.551103591918945 saving model\n",
      "6 30.90236536661784 22.54726791381836 saving model\n",
      "7 28.18731625874837 20.56934928894043 saving model\n",
      "8 25.728047688802082 19.704343795776367 saving model\n",
      "9 24.77847671508789 20.08586311340332\n",
      "10 24.603047053019207 20.113313674926758\n",
      "11 22.889197667439777 19.138946533203125 saving model\n",
      "12 24.11378796895345 18.722599029541016 saving model\n",
      "13 23.237943013509113 18.890501022338867\n",
      "14 23.032106399536133 19.177263259887695\n",
      "15 22.720221837361652 19.596290588378906\n",
      "16 22.287209192911785 19.857763290405273\n",
      "17 22.313144048055012 19.48477554321289\n",
      "18 22.282973607381184 18.978906631469727\n",
      "19 22.483434677124023 18.614791870117188 saving model\n",
      "20 22.58549690246582 18.935199737548828\n",
      "21 22.17371940612793 19.74394989013672\n",
      "22 21.53915532430013 19.987422943115234\n",
      "23 21.953571319580078 19.461830139160156\n",
      "24 21.968538920084637 19.182113647460938\n",
      "25 21.187122980753582 19.30555534362793\n",
      "26 20.994251887003582 19.513612747192383\n",
      "27 21.370228449503582 19.439697265625\n",
      "28 20.34783681233724 19.18587875366211\n",
      "29 20.809308369954426 19.746753692626953\n",
      "Epoch    31: reducing learning rate of group 0 to 5.0000e-04.\n",
      "30 20.341421763102215 20.1191463470459\n",
      "31 20.073538462320965 19.771940231323242\n",
      "32 21.18831952412923 19.445829391479492\n",
      "33 20.67571258544922 19.432598114013672\n",
      "34 19.366129557291668 19.481664657592773\n",
      "35 19.993984858194988 19.733570098876953\n",
      "36 20.197079340616863 19.928401947021484\n",
      "37 19.77045440673828 19.869115829467773\n",
      "38 19.803922653198242 19.613929748535156\n",
      "39 18.749809900919598 19.415632247924805\n",
      "40 19.61031977335612 19.350980758666992\n",
      "Epoch    42: reducing learning rate of group 0 to 2.5000e-04.\n",
      "41 18.851781209309895 19.547197341918945\n",
      "42 19.301376342773438 19.6857967376709\n",
      "43 18.959503173828125 19.72225570678711\n",
      "44 19.313003540039062 19.673677444458008\n",
      "45 19.064193725585938 19.642065048217773\n",
      "46 18.909727732340496 19.686403274536133\n",
      "47 18.41196886698405 19.67043113708496\n",
      "48 18.97603925069173 19.57491111755371\n",
      "49 18.71331850687663 19.605775833129883\n",
      "mae:  3.339955624993429\n",
      "mse:  16.738368989892177\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 256\n",
    "ha = 256\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = 32\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.001\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = hl\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train[:,:,0:5], label_train, last_dev[:,:,0:5], label_dev, last_test[:,:,0:5], label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 7, 52)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
