{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/predict-one-day-diff/pol-met-search'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])\n",
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control experiment using last observed value for missing data imputation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_dim, dropout):\n",
    "        \"\"\"\n",
    "        input_size - the number of expected features in the input x\n",
    "        hidden_size - the number of hidden units in state h\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.h = hidden_size\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (time_step, n_features)\n",
    "        \"\"\"\n",
    "        t = x.shape[0]\n",
    "        n = x.shape[1]\n",
    "        self.hx = torch.zeros(n, self.h)\n",
    "        self.cx = torch.zeros(n, self.h)\n",
    "        all_hs = []\n",
    "        all_cs = []\n",
    "        # iterate through cells \n",
    "        for i in range(t):\n",
    "            self.hx, self.cx = self.lstm(x[i], (self.hx, self.cx))\n",
    "            all_hs.append(self.hx)\n",
    "            all_cs.append(self.cx)\n",
    "        # last hidden layer last_hs is n * h\n",
    "        last_hs = all_hs[-1]\n",
    "        output = F.relu(self.fc1(last_hs))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "def train_lstm(X_train, y_train, X_valid, y_valid, X_test, y_test, config):\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "    def swap_axes(nparr):\n",
    "        return nparr.swapaxes(0,1)\n",
    "    X_train = swap_axes(X_train)\n",
    "    X_valid = swap_axes(X_valid)\n",
    "    X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters\n",
    "    input_size = X_train.shape[2]\n",
    "    h = config[\"h\"]\n",
    "    t = X_train.shape[0]\n",
    "    output_dim = 1\n",
    "    dropout = config[\"drop\"]\n",
    "    \n",
    "    model = LSTM(input_size, h, output_dim, dropout)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[1]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[:, start:end])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches \n",
    "    \n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    # timing\n",
    "#     start_time = time.time()\n",
    "#     predictions = predict(model, X_test)\n",
    "#     print(predictions.shape)\n",
    "#     print(predictions)\n",
    "#     end_time = time.time()\n",
    "#     print(end_time-start_time)\n",
    "#     assert False\n",
    "     \n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/lstm_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "    model = torch.load('models/lstm_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)\n",
    "#     corr = np.corrcoef(predictions,y_test)[0][1]\n",
    "#     print(\"corr: \", corr)\n",
    "#     true_label = (y_test >= 0)\n",
    "#     sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 7, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_train[:,:,1:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 41.23611295790899 33.683197021484375 saving model\n",
      "1 41.24767230805897 33.67363739013672 saving model\n",
      "2 41.266623996552966 33.66393280029297 saving model\n",
      "3 41.19815717424665 33.654537200927734 saving model\n",
      "4 41.26398240952265 33.64583969116211 saving model\n",
      "5 41.23711159115746 33.636905670166016 saving model\n",
      "6 41.21532349359421 33.627437591552734 saving model\n",
      "7 41.1984483628046 33.61874771118164 saving model\n",
      "8 41.2486314319429 33.60905075073242 saving model\n",
      "9 41.16646385192871 33.598541259765625 saving model\n",
      "10 41.18278367178781 33.58843231201172 saving model\n",
      "11 41.15946515401205 33.5783576965332 saving model\n",
      "12 41.1622321719215 33.56774139404297 saving model\n",
      "13 41.168164116995676 33.55644989013672 saving model\n",
      "14 41.14789099920364 33.54535675048828 saving model\n",
      "15 41.12988553728376 33.535030364990234 saving model\n",
      "16 41.14571807498024 33.52366638183594 saving model\n",
      "17 41.14968681335449 33.510929107666016 saving model\n",
      "18 41.16890407743908 33.499019622802734 saving model\n",
      "19 41.09192493983677 33.48691940307617 saving model\n",
      "20 41.095230011712935 33.47355270385742 saving model\n",
      "21 41.14020501999628 33.45820617675781 saving model\n",
      "22 41.113376072474885 33.44331741333008 saving model\n",
      "23 41.09821210588728 33.426658630371094 saving model\n",
      "24 41.086501621064684 33.41117477416992 saving model\n",
      "25 41.067481994628906 33.39449691772461 saving model\n",
      "26 41.102696918305895 33.37846374511719 saving model\n",
      "27 41.1065788269043 33.36325454711914 saving model\n",
      "28 41.081664039975124 33.346614837646484 saving model\n",
      "29 41.00126466296968 33.32908248901367 saving model\n",
      "30 41.01306406656901 33.31079864501953 saving model\n",
      "31 40.99454234895252 33.2899284362793 saving model\n",
      "32 41.00594302586147 33.2675666809082 saving model\n",
      "33 41.00530206589472 33.24504089355469 saving model\n",
      "34 40.98536663963681 33.22228240966797 saving model\n",
      "35 40.969775336129324 33.199031829833984 saving model\n",
      "36 40.89420818147205 33.173458099365234 saving model\n",
      "37 40.95936720711844 33.14906311035156 saving model\n",
      "38 40.92118381318592 33.12378692626953 saving model\n",
      "39 40.94358907427107 33.09822463989258 saving model\n",
      "40 40.888852709815616 33.06806945800781 saving model\n",
      "41 40.89484060378302 33.037845611572266 saving model\n",
      "42 40.80013157072521 33.00244903564453 saving model\n",
      "43 40.812167122250514 32.96323013305664 saving model\n",
      "44 40.76891072591146 32.91606140136719 saving model\n",
      "45 40.864694595336914 32.87493896484375 saving model\n",
      "46 40.81244087219238 32.84107971191406 saving model\n",
      "47 40.74583907354446 32.796775817871094 saving model\n",
      "48 40.647900081816175 32.745147705078125 saving model\n",
      "49 40.6666929154169 32.70012283325195 saving model\n",
      "50 40.55199786594936 32.641544342041016 saving model\n",
      "51 40.589837392171226 32.58057403564453 saving model\n",
      "52 40.46209562392462 32.512447357177734 saving model\n",
      "53 40.51653816586449 32.43950271606445 saving model\n",
      "54 40.387165977841335 32.366703033447266 saving model\n",
      "55 40.40732020423526 32.29187774658203 saving model\n",
      "56 40.265592756725496 32.19565963745117 saving model\n",
      "57 40.1367982228597 32.09584045410156 saving model\n",
      "58 40.14818886348179 32.00248336791992 saving model\n",
      "59 40.05672763642811 31.89325714111328 saving model\n",
      "60 39.97928682963053 31.785587310791016 saving model\n",
      "61 39.89758382524763 31.655078887939453 saving model\n",
      "62 39.76156752450125 31.53437042236328 saving model\n",
      "63 39.60038784572056 31.389484405517578 saving model\n",
      "64 39.71508198692685 31.262418746948242 saving model\n",
      "65 39.30200649443127 31.122997283935547 saving model\n",
      "66 39.003154255094984 30.983346939086914 saving model\n",
      "67 39.0811976932344 30.834951400756836 saving model\n",
      "68 38.873180888947985 30.719669342041016 saving model\n",
      "69 38.772046452476864 30.58995819091797 saving model\n",
      "70 38.44812742869059 30.474416732788086 saving model\n",
      "71 38.32902545020694 30.34136390686035 saving model\n",
      "72 38.074524470738005 30.20743751525879 saving model\n",
      "73 37.91215165456136 30.067567825317383 saving model\n",
      "74 37.9234361194429 29.94576072692871 saving model\n",
      "75 37.78174241383871 29.814186096191406 saving model\n",
      "76 37.62709009079706 29.6983642578125 saving model\n",
      "77 37.43529051826114 29.577232360839844 saving model\n",
      "78 37.4942877633231 29.498193740844727 saving model\n",
      "79 36.8879269191197 29.38150405883789 saving model\n",
      "80 37.06599571591332 29.261350631713867 saving model\n",
      "81 36.92665567852202 29.165424346923828 saving model\n",
      "82 36.37088544028146 29.0477237701416 saving model\n",
      "83 36.30279590969994 28.890987396240234 saving model\n",
      "84 36.62002309163412 28.801315307617188 saving model\n",
      "85 36.539760362534295 28.709644317626953 saving model\n",
      "86 35.90855870928083 28.639249801635742 saving model\n",
      "87 36.238626298450285 28.541566848754883 saving model\n",
      "88 35.656733240400044 28.457538604736328 saving model\n",
      "89 36.249399230593724 28.356094360351562 saving model\n",
      "90 36.355283419291176 28.261940002441406 saving model\n",
      "91 35.305119696117586 28.201936721801758 saving model\n",
      "92 35.56288887205578 28.112640380859375 saving model\n",
      "93 35.52263237181164 28.022600173950195 saving model\n",
      "94 35.413503601437526 27.920818328857422 saving model\n",
      "95 35.40328965868269 27.8238525390625 saving model\n",
      "96 35.65516903286888 27.754295349121094 saving model\n",
      "97 34.9594418661935 27.6636962890625 saving model\n",
      "98 35.46504506610689 27.627910614013672 saving model\n",
      "99 34.54979733058384 27.631948471069336\n",
      "100 35.24223105112711 27.555204391479492 saving model\n",
      "101 34.87209860483805 27.4630126953125 saving model\n",
      "102 34.87826588040306 27.37795066833496 saving model\n",
      "103 35.367133095150905 27.345874786376953 saving model\n",
      "104 35.36357239314488 27.309823989868164 saving model\n",
      "105 35.33594631013416 27.257164001464844 saving model\n",
      "106 34.61223152705601 27.24093246459961 saving model\n",
      "107 35.125910849798295 27.173261642456055 saving model\n",
      "108 35.24072215670631 27.12133026123047 saving model\n",
      "109 35.28107229868571 27.084774017333984 saving model\n",
      "110 34.54810773758661 27.101572036743164\n",
      "111 35.092456000191824 27.080169677734375 saving model\n",
      "112 35.258817854381746 27.034807205200195 saving model\n",
      "113 34.59582124437605 26.97517967224121 saving model\n",
      "114 34.57706033615839 26.91591453552246 saving model\n",
      "115 33.87244710468111 26.904645919799805 saving model\n",
      "116 34.70274534679594 26.88842010498047 saving model\n",
      "117 34.05433050791422 26.840606689453125 saving model\n",
      "118 34.59022453853062 26.83538246154785 saving model\n",
      "119 34.77043851216634 26.797592163085938 saving model\n",
      "120 34.48670092083159 26.827669143676758\n",
      "121 34.02621891385033 26.83604621887207\n",
      "122 33.77184722537086 26.78586769104004 saving model\n",
      "123 34.44125597817557 26.743282318115234 saving model\n",
      "124 34.14359610421317 26.704618453979492 saving model\n",
      "125 34.5710909253075 26.73198890686035\n",
      "126 34.51903815496536 26.714792251586914\n",
      "127 34.226599874950594 26.664108276367188 saving model\n",
      "128 33.58071913037981 26.715166091918945\n",
      "129 34.47037410736084 26.68381690979004\n",
      "130 34.41847805749802 26.677982330322266\n",
      "131 33.556361879621235 26.66497230529785\n",
      "132 33.4544190906343 26.693483352661133\n",
      "133 34.70932633536203 26.67402458190918\n",
      "134 34.64160492306664 26.629823684692383 saving model\n",
      "135 34.51379022144136 26.64927864074707\n",
      "136 34.04435988834926 26.589702606201172 saving model\n",
      "137 33.46220438820975 26.55471420288086 saving model\n",
      "138 33.47726917266846 26.544416427612305 saving model\n",
      "139 33.28295725867862 26.53080940246582 saving model\n",
      "140 34.326031775701615 26.519208908081055 saving model\n",
      "141 33.39214883531843 26.501060485839844 saving model\n",
      "142 33.312547138759065 26.498064041137695 saving model\n",
      "143 33.65035801842099 26.526439666748047\n",
      "144 33.04067493620373 26.54347801208496\n",
      "145 33.6620911189488 26.533329010009766\n",
      "146 33.51325271243141 26.5242977142334\n",
      "147 33.56486629304432 26.546823501586914\n",
      "148 33.55402673993792 26.49811363220215\n",
      "149 33.70799945649647 26.468780517578125 saving model\n",
      "mae:  4.116883722945186\n",
      "mse:  26.700708121981204\n"
     ]
    }
   ],
   "source": [
    "# met \n",
    "config = {'h':32, 'lr':0.0001, 'num_epochs':150, 'batchsize':32, 'drop':0.5}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train[:,:,1:5], label_train, last_dev[:,:,1:5], label_dev, last_test[:,:,1:5], label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 41.329455511910574 33.67776107788086 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 41.22793061392648 33.67509841918945 saving model\n",
      "2 41.21859432402111 33.67281723022461 saving model\n",
      "3 41.24243645440964 33.670467376708984 saving model\n",
      "4 41.19531086512974 33.668338775634766 saving model\n",
      "5 41.17391386486235 33.665775299072266 saving model\n",
      "6 41.20284407479422 33.66217803955078 saving model\n",
      "7 41.211346399216424 33.6595573425293 saving model\n",
      "8 41.2637502579462 33.657501220703125 saving model\n",
      "9 41.18513225373768 33.65495300292969 saving model\n",
      "10 41.19043486458914 33.65225601196289 saving model\n",
      "11 41.226323354811896 33.64967346191406 saving model\n",
      "12 41.2124034336635 33.64716720581055 saving model\n",
      "13 41.16431862967355 33.645179748535156 saving model\n",
      "14 41.215364183698384 33.64227294921875 saving model\n",
      "15 41.14261954171317 33.63984298706055 saving model\n",
      "16 41.07734071640741 33.63759231567383 saving model\n",
      "17 41.1756964183989 33.63459777832031 saving model\n",
      "18 41.16886838277181 33.631126403808594 saving model\n",
      "19 41.135512851533434 33.627906799316406 saving model\n",
      "20 41.13559087117513 33.62614059448242 saving model\n",
      "21 41.1110467456636 33.62430953979492 saving model\n",
      "22 41.15445600237165 33.62173080444336 saving model\n",
      "23 41.11693881806873 33.61873245239258 saving model\n",
      "24 41.12138602847145 33.616397857666016 saving model\n",
      "25 41.14297176542736 33.613731384277344 saving model\n",
      "26 41.13053803216843 33.61089324951172 saving model\n",
      "27 41.131334123157316 33.60707473754883 saving model\n",
      "28 41.096348353794646 33.60316848754883 saving model\n",
      "29 41.0780824025472 33.60028839111328 saving model\n",
      "30 41.075139817737394 33.597042083740234 saving model\n",
      "31 41.01417178199405 33.59291458129883 saving model\n",
      "32 41.076620011102584 33.589080810546875 saving model\n",
      "33 41.06552033197312 33.584716796875 saving model\n",
      "34 41.03397178649902 33.581031799316406 saving model\n",
      "35 40.97735205150786 33.57640075683594 saving model\n",
      "36 41.038700557890394 33.571868896484375 saving model\n",
      "37 41.00666409447079 33.56605911254883 saving model\n",
      "38 40.97810799734933 33.56094741821289 saving model\n",
      "39 40.97640764145624 33.55535888671875 saving model\n",
      "40 40.91669009980701 33.55013656616211 saving model\n",
      "41 40.97609792436872 33.54503631591797 saving model\n",
      "42 41.00704738071987 33.54109191894531 saving model\n",
      "43 40.93947537740072 33.53672790527344 saving model\n",
      "44 40.90692756289528 33.52993392944336 saving model\n",
      "45 40.869204294113885 33.52235794067383 saving model\n",
      "46 40.82459059215727 33.51633071899414 saving model\n",
      "47 40.928535370599654 33.510005950927734 saving model\n",
      "48 40.82496216183617 33.5032844543457 saving model\n",
      "49 40.80458686465309 33.49523162841797 saving model\n",
      "50 40.80885641915457 33.48588562011719 saving model\n",
      "51 40.824243999662855 33.476322174072266 saving model\n",
      "52 40.78841618129185 33.4677848815918 saving model\n",
      "53 40.69351850237165 33.45734405517578 saving model\n",
      "54 40.692200615292506 33.44586181640625 saving model\n",
      "55 40.7001702444894 33.434898376464844 saving model\n",
      "56 40.675938197544646 33.42295837402344 saving model\n",
      "57 40.75918506440662 33.4117431640625 saving model\n",
      "58 40.65701720828102 33.399295806884766 saving model\n",
      "59 40.69999876476469 33.3852653503418 saving model\n",
      "60 40.59533555167062 33.369197845458984 saving model\n",
      "61 40.481849307105655 33.354103088378906 saving model\n",
      "62 40.484414781842915 33.337852478027344 saving model\n",
      "63 40.54724320911226 33.320343017578125 saving model\n",
      "64 40.45938591730027 33.30642318725586 saving model\n",
      "65 40.51374916803269 33.28917694091797 saving model\n",
      "66 40.55084510076614 33.27228546142578 saving model\n",
      "67 40.48025903247652 33.256290435791016 saving model\n",
      "68 40.44624355861119 33.239341735839844 saving model\n",
      "69 40.26933987935384 33.22242736816406 saving model\n",
      "70 40.29246339343843 33.20545959472656 saving model\n",
      "71 40.250763302757626 33.186946868896484 saving model\n",
      "72 40.18348167056129 33.16459274291992 saving model\n",
      "73 40.231036958240324 33.14299011230469 saving model\n",
      "74 40.18069394429525 33.12297058105469 saving model\n",
      "75 39.97689047313872 33.10073471069336 saving model\n",
      "76 40.080018543061755 33.07784652709961 saving model\n",
      "77 39.980712164016 33.0546760559082 saving model\n",
      "78 39.81669734773182 33.027870178222656 saving model\n",
      "79 39.72458830333891 32.99728012084961 saving model\n",
      "80 39.88645798819406 32.96820831298828 saving model\n",
      "81 39.654767990112305 32.93992233276367 saving model\n",
      "82 39.6103401184082 32.90522003173828 saving model\n",
      "83 39.617438725062776 32.87546920776367 saving model\n",
      "84 39.609416870843795 32.84471893310547 saving model\n",
      "85 39.45147232782273 32.81233596801758 saving model\n",
      "86 39.424469993228 32.784027099609375 saving model\n",
      "87 39.16185633341471 32.75279235839844 saving model\n",
      "88 39.167412349155974 32.71416473388672 saving model\n",
      "89 38.989015125093005 32.67448425292969 saving model\n",
      "90 38.90479700905936 32.64002227783203 saving model\n",
      "91 38.65166291736421 32.596797943115234 saving model\n",
      "92 38.57950451260521 32.55429458618164 saving model\n",
      "93 38.61186577024914 32.5133171081543 saving model\n",
      "94 38.309666224888396 32.46513366699219 saving model\n",
      "95 38.47696903773716 32.42145919799805 saving model\n",
      "96 38.2296260652088 32.380836486816406 saving model\n",
      "97 37.78661228361584 32.337825775146484 saving model\n",
      "98 37.8079894837879 32.293216705322266 saving model\n",
      "99 37.73597254071917 32.2445182800293 saving model\n",
      "100 37.51911899021694 32.20000076293945 saving model\n",
      "101 37.54059959593273 32.16170120239258 saving model\n",
      "102 37.129124232700896 32.122806549072266 saving model\n",
      "103 37.12767015184675 32.07345199584961 saving model\n",
      "104 36.81438754853748 32.02314758300781 saving model\n",
      "105 36.364689372834704 31.9647216796875 saving model\n",
      "106 36.31025132678804 31.9102840423584 saving model\n",
      "107 36.26247823806036 31.867109298706055 saving model\n",
      "108 35.871452059064595 31.822629928588867 saving model\n",
      "109 35.36312480199905 31.77059555053711 saving model\n",
      "110 35.58787132444836 31.72032928466797 saving model\n",
      "111 35.269604183378675 31.677499771118164 saving model\n",
      "112 35.08452628907703 31.639638900756836 saving model\n",
      "113 34.61833345322382 31.609384536743164 saving model\n",
      "114 34.38095960162935 31.55446434020996 saving model\n",
      "115 34.01095726376488 31.510616302490234 saving model\n",
      "116 34.09342961084275 31.468164443969727 saving model\n",
      "117 33.72923791976202 31.439491271972656 saving model\n",
      "118 32.79058447338286 31.40331268310547 saving model\n",
      "119 33.255583899361746 31.375503540039062 saving model\n",
      "120 33.057172775268555 31.35947036743164 saving model\n",
      "121 32.88796565646217 31.340890884399414 saving model\n",
      "122 31.873783293224516 31.318716049194336 saving model\n",
      "123 31.890880584716797 31.320894241333008\n",
      "124 31.71591181982131 31.31533432006836 saving model\n",
      "125 30.962212562561035 31.298227310180664 saving model\n",
      "126 30.502746309552872 31.282182693481445 saving model\n",
      "127 30.438404809860955 31.278013229370117 saving model\n",
      "128 30.563287417093914 31.276887893676758 saving model\n",
      "129 29.638096446082706 31.297395706176758\n",
      "130 29.48362536657424 31.3209228515625\n",
      "131 29.14466467357817 31.334754943847656\n",
      "132 28.3268853142148 31.36332893371582\n",
      "133 28.73888451712472 31.400758743286133\n",
      "134 28.21510364895775 31.449174880981445\n",
      "135 26.316203208196733 31.489437103271484\n",
      "136 26.501780237470353 31.520915985107422\n",
      "137 26.399579275222052 31.57115936279297\n",
      "Epoch   139: reducing learning rate of group 0 to 5.0000e-05.\n",
      "138 26.97130657377697 31.6284122467041\n",
      "139 25.550491151355562 31.673276901245117\n",
      "140 25.74337014697847 31.703815460205078\n",
      "141 26.56241834731329 31.73444175720215\n",
      "142 25.82095209757487 31.761856079101562\n",
      "143 24.937072254362562 31.78820037841797\n",
      "144 24.784203801836288 31.801294326782227\n",
      "145 25.264612515767414 31.827863693237305\n",
      "146 25.22950821831113 31.847627639770508\n",
      "147 24.73919477916899 31.87471580505371\n",
      "148 24.764305841355096 31.90522575378418\n",
      "Epoch   150: reducing learning rate of group 0 to 2.5000e-05.\n",
      "149 24.783747082664853 31.95178985595703\n",
      "mae:  4.733872620676729\n",
      "mse:  33.929693199946335\n"
     ]
    }
   ],
   "source": [
    "# met and search \n",
    "config = {'h':32, 'lr':0.0001, 'num_epochs':150, 'batchsize':32, 'drop':0.5}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train[:,:,1:], label_train, last_dev[:,:,1:], label_dev, last_test[:,:,1:], label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 41.33254668826149 33.734954833984375 saving model\n",
      "1 41.28713162740072 33.675148010253906 saving model\n",
      "2 41.2317925407773 33.61861801147461 saving model\n",
      "3 41.19173722040085 33.562835693359375 saving model\n",
      "4 41.1420465196882 33.5089225769043 saving model\n",
      "5 41.031910669235955 33.4533576965332 saving model\n",
      "6 40.97483653113956 33.39518737792969 saving model\n",
      "7 40.908630189441496 33.334381103515625 saving model\n",
      "8 40.86536852518717 33.27157974243164 saving model\n",
      "9 40.79624048868815 33.20006561279297 saving model\n",
      "10 40.72873115539551 33.12499237060547 saving model\n",
      "11 40.57214182899112 33.040409088134766 saving model\n",
      "12 40.52426038469587 32.944793701171875 saving model\n",
      "13 40.43034435453869 32.84569549560547 saving model\n",
      "14 40.299276261102584 32.73468780517578 saving model\n",
      "15 40.1606638772147 32.61459732055664 saving model\n",
      "16 40.10060037885393 32.485809326171875 saving model\n",
      "17 39.98446478162493 32.337337493896484 saving model\n",
      "18 39.78516633169992 32.16497802734375 saving model\n",
      "19 39.61846478780111 31.974319458007812 saving model\n",
      "20 39.49483176640102 31.758949279785156 saving model\n",
      "21 39.04595665704636 31.518476486206055 saving model\n",
      "22 38.8609307607015 31.263362884521484 saving model\n",
      "23 38.74863906133743 30.990264892578125 saving model\n",
      "24 38.35979012080601 30.68472671508789 saving model\n",
      "25 38.08243674323673 30.355304718017578 saving model\n",
      "26 37.59748822166806 29.987947463989258 saving model\n",
      "27 37.35673963455927 29.582714080810547 saving model\n",
      "28 36.7944157010033 29.117748260498047 saving model\n",
      "29 36.29978729429699 28.663860321044922 saving model\n",
      "30 35.87782228560675 28.16059112548828 saving model\n",
      "31 35.50943501790365 27.684085845947266 saving model\n",
      "32 35.123599143255326 27.20596694946289 saving model\n",
      "33 34.22138581957136 26.692020416259766 saving model\n",
      "34 34.035536039443244 26.129106521606445 saving model\n",
      "35 33.68835771651495 25.59828758239746 saving model\n",
      "36 32.466154825119744 25.09389877319336 saving model\n",
      "37 32.33869943164644 24.669050216674805 saving model\n",
      "38 31.831688653855096 24.228191375732422 saving model\n",
      "39 30.90011692047119 23.830209732055664 saving model\n",
      "40 30.824884142194474 23.395465850830078 saving model\n",
      "41 30.553778375898087 23.006330490112305 saving model\n",
      "42 30.507949511210125 22.681678771972656 saving model\n",
      "43 29.14828164236886 22.419832229614258 saving model\n",
      "44 29.464306740533736 22.161285400390625 saving model\n",
      "45 28.639436176845006 21.902734756469727 saving model\n",
      "46 29.572013082958403 21.737594604492188 saving model\n",
      "47 28.689342135474796 21.587905883789062 saving model\n",
      "48 27.418753306070965 21.46538543701172 saving model\n",
      "49 28.023738906497048 21.381446838378906 saving model\n",
      "50 27.422232991173153 21.279874801635742 saving model\n",
      "51 27.15873927161807 21.293691635131836\n",
      "52 27.579223996117 21.195152282714844 saving model\n",
      "53 26.951491628374374 21.105907440185547 saving model\n",
      "54 27.40742878686814 21.057411193847656 saving model\n",
      "55 27.506548200334823 21.054758071899414 saving model\n",
      "56 27.91908209664481 21.013858795166016 saving model\n",
      "57 27.168813705444336 20.9871826171875 saving model\n",
      "58 26.676964850652787 20.94127082824707 saving model\n",
      "59 26.26521060580299 20.954038619995117\n",
      "60 26.779948416210356 20.916112899780273 saving model\n",
      "61 26.789739926656086 20.953887939453125\n",
      "62 26.896005358014786 20.954547882080078\n",
      "63 27.211179188319615 20.902780532836914 saving model\n",
      "64 28.037216004871187 20.988527297973633\n",
      "65 26.795653070722306 21.00560760498047\n",
      "66 26.698253631591797 21.00006675720215\n",
      "67 27.067738487606956 20.96051788330078\n",
      "68 27.029553640456427 20.972984313964844\n",
      "69 26.506019047328405 20.994266510009766\n",
      "70 28.22209135691325 21.063875198364258\n",
      "71 27.58595866248721 21.06814956665039\n",
      "72 27.7152282169887 21.024709701538086\n",
      "73 27.054990541367303 21.01885223388672\n",
      "Epoch    75: reducing learning rate of group 0 to 5.0000e-05.\n",
      "74 27.34669390178862 20.96658706665039\n",
      "75 26.3732883362543 20.95547103881836\n",
      "76 27.784781819298153 20.904714584350586\n",
      "77 27.37691334315709 20.88487434387207 saving model\n",
      "78 27.955295108613512 20.89463233947754\n",
      "79 27.345321337382 20.935590744018555\n",
      "80 27.030022121611097 20.94623565673828\n",
      "81 27.91385319119408 20.951650619506836\n",
      "82 26.15748950413295 20.92035484313965\n",
      "83 27.3443055834089 20.882259368896484 saving model\n",
      "84 26.706651596795943 20.887165069580078\n",
      "85 26.293642770676385 20.878459930419922 saving model\n",
      "86 26.809791337876092 20.905548095703125\n",
      "87 27.070551281883603 20.938549041748047\n",
      "88 27.15646521250407 20.929527282714844\n",
      "89 26.442771957034157 20.927019119262695\n",
      "90 26.786778222946893 20.910600662231445\n",
      "91 27.29440652756464 20.91604995727539\n",
      "92 27.168518293471564 20.923444747924805\n",
      "93 26.93344951811291 20.957290649414062\n",
      "94 27.492863019307453 20.942123413085938\n",
      "95 27.76716223217192 20.913572311401367\n",
      "Epoch    97: reducing learning rate of group 0 to 2.5000e-05.\n",
      "96 26.53080740429106 20.907960891723633\n",
      "97 27.015950066702707 20.88964080810547\n",
      "98 27.51836381639753 20.881031036376953\n",
      "99 27.638931637718564 20.889232635498047\n",
      "100 26.94988950093587 20.904117584228516\n",
      "101 27.599782580421085 20.910484313964844\n",
      "102 27.786432311648415 20.907135009765625\n",
      "103 27.6872741154262 20.903879165649414\n",
      "104 26.04322438012986 20.8955078125\n",
      "105 27.369809241521928 20.898605346679688\n",
      "106 27.55001381465367 20.895334243774414\n",
      "Epoch   108: reducing learning rate of group 0 to 1.2500e-05.\n",
      "107 26.46904350462414 20.900876998901367\n",
      "108 27.556272143409366 20.896318435668945\n",
      "109 26.74683143979027 20.895709991455078\n",
      "110 26.73721004667736 20.89765167236328\n",
      "111 27.189688046773274 20.890878677368164\n",
      "112 27.47836244673956 20.88793182373047\n",
      "113 28.134908222016833 20.886598587036133\n",
      "114 26.47542730967204 20.88747787475586\n",
      "115 26.851977529979887 20.88724708557129\n",
      "116 26.65871910821824 20.887012481689453\n",
      "117 27.643434524536133 20.882291793823242\n",
      "Epoch   119: reducing learning rate of group 0 to 6.2500e-06.\n",
      "118 25.928783870878675 20.8820743560791\n",
      "119 27.22161102294922 20.879093170166016\n",
      "120 26.90663968949091 20.878076553344727 saving model\n",
      "121 27.757928394135973 20.88069725036621\n",
      "122 26.905769075666154 20.88005256652832\n",
      "123 27.818401109604608 20.882902145385742\n",
      "124 26.80073897043864 20.88685417175293\n",
      "125 26.726558730715798 20.888641357421875\n",
      "126 27.45923441932315 20.890825271606445\n",
      "127 27.117824645269486 20.89292335510254\n",
      "128 26.51711636497861 20.89641761779785\n",
      "Epoch   130: reducing learning rate of group 0 to 3.1250e-06.\n",
      "129 26.98127637590681 20.895315170288086\n",
      "130 26.36502556573777 20.893611907958984\n",
      "131 27.42832815079462 20.892112731933594\n",
      "132 26.681185041155135 20.892202377319336\n",
      "133 25.62669495173863 20.889934539794922\n",
      "134 26.351746195838565 20.888248443603516\n",
      "135 26.515454655601864 20.88907241821289\n",
      "136 27.120486350286576 20.88780975341797\n",
      "137 27.328620047796342 20.887231826782227\n",
      "138 26.227384431021555 20.88446617126465\n",
      "139 26.749475297473726 20.88401985168457\n",
      "Epoch   141: reducing learning rate of group 0 to 1.5625e-06.\n",
      "140 27.213441848754883 20.883790969848633\n",
      "141 27.80071208590553 20.883813858032227\n",
      "142 26.885723704383487 20.883665084838867\n",
      "143 26.10718313852946 20.884157180786133\n",
      "144 27.304724420819962 20.884944915771484\n",
      "145 26.87526698339553 20.8846435546875\n",
      "146 26.700777871268137 20.88556671142578\n",
      "147 27.27957684653146 20.885814666748047\n",
      "148 26.161986260187057 20.88595199584961\n",
      "149 26.46156360989525 20.88527488708496\n",
      "mae:  3.7926271640564786\n",
      "mse:  20.9721825917327\n"
     ]
    }
   ],
   "source": [
    "# pol only\n",
    "config = {'h':32, 'lr':0.0001, 'num_epochs':150, 'batchsize':32, 'drop':0.5}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train[:,:,0:1], label_train, last_dev[:,:,0:1], label_dev, last_test[:,:,0:1], label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 40.763465518043155 32.6501350402832 saving model\n",
      "1 39.394938060215544 30.905275344848633 saving model\n",
      "2 36.971732911609465 27.10590362548828 saving model\n",
      "3 32.51917225973947 21.174152374267578 saving model\n",
      "4 27.750157537914458 19.8355655670166 saving model\n",
      "5 25.691828954787482 18.996641159057617 saving model\n",
      "6 25.176917030697776 19.870403289794922\n",
      "7 23.888593719119118 19.426408767700195\n",
      "8 24.50892816271101 19.239788055419922\n",
      "9 23.789821034386044 19.07443618774414\n",
      "10 23.68046696980794 19.18096160888672\n",
      "11 23.88068589710054 19.172433853149414\n",
      "12 22.73766276949928 18.87061882019043 saving model\n",
      "13 23.794909250168573 19.032934188842773\n",
      "14 23.320554369971866 18.814205169677734 saving model\n",
      "15 22.309762273515975 18.818405151367188\n",
      "16 23.535151118323917 18.973939895629883\n",
      "17 23.155351956685383 18.98465347290039\n",
      "18 22.207816169375466 18.78868293762207 saving model\n",
      "19 23.20317486354283 19.060285568237305\n",
      "20 21.98801853543236 19.106218338012695\n",
      "21 22.787301653907413 19.120712280273438\n",
      "22 21.50005531311035 18.509225845336914 saving model\n",
      "23 22.186875570388068 19.348825454711914\n",
      "24 21.685006232488725 18.705598831176758\n",
      "25 21.241948491051083 18.438617706298828 saving model\n",
      "26 21.35456743694487 18.69058609008789\n",
      "27 20.619800476800826 18.64376449584961\n",
      "28 20.855431692940847 18.848236083984375\n",
      "29 21.076663789295015 18.634828567504883\n",
      "30 20.30816432407924 18.34446144104004 saving model\n",
      "31 20.562476112729026 19.15550422668457\n",
      "32 20.594293458121165 18.387582778930664\n",
      "33 20.57376684461321 18.588956832885742\n",
      "34 20.38754113515218 18.477754592895508\n",
      "35 19.99571990966797 18.283470153808594 saving model\n",
      "36 19.506082648322696 18.2858829498291\n",
      "37 19.83734012785412 18.565305709838867\n",
      "38 20.358301843915665 18.062671661376953 saving model\n",
      "39 19.923580033438547 18.560260772705078\n",
      "40 19.614460263933456 18.732364654541016\n",
      "41 18.99138981955392 18.301084518432617\n",
      "42 19.2836339587257 18.202701568603516\n",
      "43 19.04920260111491 18.668991088867188\n",
      "44 19.44784809294201 18.471641540527344\n",
      "45 19.101168450855074 18.328659057617188\n",
      "46 19.503115926470077 18.427562713623047\n",
      "47 18.32989347548712 18.521547317504883\n",
      "48 18.85289516903105 18.794431686401367\n",
      "Epoch    50: reducing learning rate of group 0 to 2.5000e-04.\n",
      "49 18.681194873083207 18.530996322631836\n",
      "50 18.20158050173805 18.719038009643555\n",
      "51 17.88940175374349 19.158863067626953\n",
      "52 18.135713531857444 18.612686157226562\n",
      "53 17.853360176086426 19.099346160888672\n",
      "54 17.692227908543178 18.993389129638672\n",
      "55 17.318777947198775 18.836715698242188\n",
      "56 17.93929181780134 18.670331954956055\n",
      "57 17.537190255664644 18.64591407775879\n",
      "58 17.541935171399796 18.6002140045166\n",
      "59 17.65876833597819 18.662002563476562\n",
      "Epoch    61: reducing learning rate of group 0 to 1.2500e-04.\n",
      "60 16.861608913966588 18.66902732849121\n",
      "61 17.436643759409588 18.981840133666992\n",
      "62 17.106326421101887 18.840232849121094\n",
      "63 17.140530722481863 18.686691284179688\n",
      "64 16.938080742245628 18.929519653320312\n",
      "65 17.59936339514596 18.85576629638672\n",
      "66 17.284658568246023 18.7331600189209\n",
      "67 16.650018056233723 18.65829849243164\n",
      "68 16.88120378766741 18.919021606445312\n",
      "69 17.242883001055038 18.951934814453125\n",
      "70 16.818283194587345 18.843706130981445\n",
      "Epoch    72: reducing learning rate of group 0 to 6.2500e-05.\n",
      "71 17.1352728889102 18.918210983276367\n",
      "72 16.719735122862318 18.94202995300293\n",
      "73 16.862236794971285 19.023338317871094\n",
      "74 16.871855395180837 18.887407302856445\n",
      "75 16.71870417821975 18.951921463012695\n",
      "76 16.986138230278378 18.89618682861328\n",
      "77 16.567064398810977 18.983177185058594\n",
      "78 16.837717714763823 18.96883201599121\n",
      "79 17.021761190323602 18.982257843017578\n",
      "80 16.838950951894123 19.08022689819336\n",
      "81 16.678171657380602 18.927406311035156\n",
      "Epoch    83: reducing learning rate of group 0 to 3.1250e-05.\n",
      "82 16.85138425372896 19.04410743713379\n",
      "83 16.76812203725179 19.077972412109375\n",
      "84 16.04447841644287 19.096704483032227\n",
      "85 16.73371782756987 19.10384178161621\n",
      "86 16.713484718686058 19.103435516357422\n",
      "87 16.3031321707226 19.099769592285156\n",
      "88 16.601423150017148 19.03989601135254\n",
      "89 16.533363115219842 18.95941734313965\n",
      "90 16.47128706886655 18.93326759338379\n",
      "91 16.848269689650763 18.962247848510742\n",
      "92 16.902817249298096 18.92844009399414\n",
      "Epoch    94: reducing learning rate of group 0 to 1.5625e-05.\n",
      "93 16.40429873693557 18.943675994873047\n",
      "94 16.248299348922004 18.987749099731445\n",
      "95 16.441614900316512 19.005657196044922\n",
      "96 16.366295042492094 19.040189743041992\n",
      "97 16.751515138716925 19.079225540161133\n",
      "98 16.442792506445024 19.066251754760742\n",
      "99 16.30192277545021 19.051366806030273\n",
      "100 15.93246721086048 19.05570411682129\n",
      "101 15.981914111546107 19.07860565185547\n",
      "102 16.312834217434837 19.079036712646484\n",
      "103 16.350992429824103 19.0816707611084\n",
      "Epoch   105: reducing learning rate of group 0 to 7.8125e-06.\n",
      "104 16.483453160240536 19.084171295166016\n",
      "105 16.45174201329549 19.082645416259766\n",
      "106 17.16492848169236 19.097257614135742\n",
      "107 16.436081364041282 19.11606216430664\n",
      "108 16.77344848996117 19.131927490234375\n",
      "109 16.04897626241048 19.135419845581055\n",
      "110 16.584252039591473 19.12067413330078\n",
      "111 16.64701470874605 19.121257781982422\n",
      "112 15.960269927978516 19.1312255859375\n",
      "113 16.72106715611049 19.137134552001953\n",
      "114 17.12752714611235 19.123178482055664\n",
      "Epoch   116: reducing learning rate of group 0 to 3.9063e-06.\n",
      "115 15.920904114132835 19.11164093017578\n",
      "116 16.533823263077508 19.114442825317383\n",
      "117 16.30982848576137 19.115097045898438\n",
      "118 16.492013636089506 19.11905860900879\n",
      "119 16.02872333072481 19.121294021606445\n",
      "120 16.264235655466717 19.12750816345215\n",
      "121 16.88275571096511 19.134620666503906\n",
      "122 16.631235985528853 19.125139236450195\n",
      "123 16.51188019343785 19.1196346282959\n",
      "124 16.07447101956322 19.12604522705078\n",
      "125 16.130830605824787 19.13551902770996\n",
      "Epoch   127: reducing learning rate of group 0 to 1.9531e-06.\n",
      "126 16.39397328240531 19.138181686401367\n",
      "127 16.155468577430362 19.133329391479492\n",
      "128 16.480888661884126 19.128787994384766\n",
      "129 16.586388860430038 19.12925910949707\n",
      "130 16.557659762246267 19.13245391845703\n",
      "131 16.553861799694243 19.13191032409668\n",
      "132 16.081311066945393 19.130130767822266\n",
      "133 16.49629309063866 19.125701904296875\n",
      "134 17.032191526322137 19.123918533325195\n",
      "135 16.605705488295783 19.12306785583496\n",
      "136 16.03406084151495 19.12824249267578\n",
      "Epoch   138: reducing learning rate of group 0 to 9.7656e-07.\n",
      "137 16.63322217123849 19.127613067626953\n",
      "138 16.30186396553403 19.125383377075195\n",
      "139 16.358679839542933 19.12311363220215\n",
      "140 15.833884557088217 19.122514724731445\n",
      "141 16.015130065736315 19.12360382080078\n",
      "142 16.101115317571733 19.12352752685547\n",
      "143 15.879003025236583 19.123920440673828\n",
      "144 16.172459942953928 19.124801635742188\n",
      "145 16.44597898210798 19.127012252807617\n",
      "146 16.440205437796457 19.128767013549805\n",
      "147 16.350589366186234 19.128877639770508\n",
      "Epoch   149: reducing learning rate of group 0 to 4.8828e-07.\n",
      "148 16.311909516652424 19.129196166992188\n",
      "149 16.243852592649915 19.129077911376953\n",
      "mae:  3.3079172487224433\n",
      "mse:  17.009053592231822\n"
     ]
    }
   ],
   "source": [
    "# pol and met \n",
    "config = {'h':128, 'lr':0.0005, 'num_epochs':150, 'batchsize':32, 'drop':0.7}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train[:,:,0:5], label_train, last_dev[:,:,0:5], label_dev, last_test[:,:,0:5], label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
