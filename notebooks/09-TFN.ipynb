{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSubNet(nn.Module):\n",
    "    '''\n",
    "    The LSTM-based subnetwork that is used in TFN for text\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_size, hidden_size, out_size, num_layers=1, dropout=0.2, bidirectional=False):\n",
    "        '''\n",
    "        Args:\n",
    "            in_size: input dimension\n",
    "            hidden_size: hidden layer dimension\n",
    "            num_layers: specify the number of layers of LSTMs.\n",
    "            dropout: dropout probability\n",
    "            bidirectional: specify usage of bidirectional LSTM\n",
    "        Output:\n",
    "            (return value in forward) a tensor of shape (batch_size, out_size)\n",
    "        '''\n",
    "        super(TextSubNet, self).__init__()\n",
    "        self.rnn = nn.LSTM(in_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_1 = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x: tensor of shape (batch_size, sequence_len, in_size)\n",
    "        '''\n",
    "        _, final_states = self.rnn(x)\n",
    "        h = self.dropout(final_states[0].squeeze())\n",
    "        y_1 = self.linear_1(h)\n",
    "        return y_1\n",
    "\n",
    "\n",
    "class TFN(nn.Module):\n",
    "    '''\n",
    "    Implements the Tensor Fusion Networks for multimodal sentiment analysis as is described in:\n",
    "    Zadeh, Amir, et al. \"Tensor fusion network for multimodal sentiment analysis.\" EMNLP 2017 Oral.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_dims, hidden_dims, text_out, dropouts, post_fusion_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dims - a length-3 tuple, contains (audio_dim, video_dim, text_dim)\n",
    "            hidden_dims - another length-3 tuple, similar to input_dims\n",
    "            text_out - int, specifying the resulting dimensions of the text subnetwork\n",
    "            dropouts - a length-4 tuple, contains (audio_dropout, video_dropout, text_dropout, post_fusion_dropout)\n",
    "            post_fusion_dim - int, specifying the size of the sub-networks after tensorfusion\n",
    "        Output:\n",
    "            (return value in forward) a scalar value between -3 and 3\n",
    "        '''\n",
    "        super(TFN, self).__init__()\n",
    "\n",
    "        # dimensions are specified in the order of audio, video and text\n",
    "        self.audio_in = input_dims[0]\n",
    "        self.text_in = input_dims[1]\n",
    "\n",
    "        self.audio_hidden = hidden_dims[0]\n",
    "        self.text_hidden = hidden_dims[1]\n",
    "        self.audio_out = text_out[0]\n",
    "        self.text_out= text_out[1]\n",
    "        self.post_fusion_dim = post_fusion_dim\n",
    "\n",
    "        self.audio_prob = dropouts[0]\n",
    "        self.text_prob = dropouts[1]\n",
    "        self.post_fusion_prob = dropouts[2]\n",
    "\n",
    "        # define the pre-fusion subnetworks\n",
    "        self.audio_subnet = TextSubNet(self.audio_in, self.audio_hidden, self.audio_out, dropout=self.audio_prob)\n",
    "        self.text_subnet = TextSubNet(self.text_in, self.text_hidden, self.text_out, dropout=self.text_prob)\n",
    "\n",
    "        # define the post_fusion layers\n",
    "        self.post_fusion_dropout = nn.Dropout(p=self.post_fusion_prob)\n",
    "        self.post_fusion_layer_1 = nn.Linear((self.text_out + 1) * (self.audio_hidden + 1), self.post_fusion_dim)\n",
    "        self.post_fusion_layer_2 = nn.Linear(self.post_fusion_dim, self.post_fusion_dim)\n",
    "        self.post_fusion_layer_3 = nn.Linear(self.post_fusion_dim, 1)\n",
    "\n",
    "        # in TFN we are doing a regression with constrained output range: (-3, 3), hence we'll apply sigmoid to output\n",
    "        # shrink it to (0, 1), and scale\\shift it back to range (-3, 3)\n",
    "#         self.output_range = Parameter(torch.FloatTensor([6]), requires_grad=False)\n",
    "#         self.output_shift = Parameter(torch.FloatTensor([-3]), requires_grad=False)\n",
    "\n",
    "    def forward(self, audio_x, text_x):\n",
    "        '''\n",
    "        Args:\n",
    "            audio_x: tensor of shape (batch_size, sequence_len, audio_in)\n",
    "            video_x: tensor of shape (batch_size, video_in)\n",
    "            text_x: tensor of shape (batch_size, sequence_len, text_in)\n",
    "        '''\n",
    "        audio_h = self.audio_subnet(audio_x)\n",
    "        text_h = self.text_subnet(text_x)\n",
    "        batch_size = audio_h.data.shape[0]\n",
    "\n",
    "        # next we perform \"tensor fusion\", which is essentially appending 1s to the tensors and take Kronecker product\n",
    "        if audio_h.is_cuda:\n",
    "            DTYPE = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            DTYPE = torch.FloatTensor\n",
    "\n",
    "        _audio_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), audio_h), dim=1)\n",
    "        _text_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), text_h), dim=1)\n",
    "\n",
    "        # _audio_h has shape (batch_size, audio_in + 1), _video_h has shape (batch_size, _video_in + 1)\n",
    "        # we want to perform outer product between the two batch, hence we unsqueenze them to get\n",
    "        # (batch_size, audio_in + 1, 1) X (batch_size, 1, video_in + 1)\n",
    "        # fusion_tensor will have shape (batch_size, audio_in + 1, video_in + 1)\n",
    "        fusion_tensor = torch.bmm(_audio_h.unsqueeze(2), _text_h.unsqueeze(1))\n",
    "        \n",
    "        # next we do kronecker product between fusion_tensor and _text_h. This is even trickier\n",
    "        # we have to reshape the fusion tensor during the computation\n",
    "        # in the end we don't keep the 3-D tensor, instead we flatten it\n",
    "        fusion_tensor = fusion_tensor.view(batch_size, -1)\n",
    "\n",
    "        post_fusion_dropped = self.post_fusion_dropout(fusion_tensor)\n",
    "        post_fusion_y_1 = F.relu(self.post_fusion_layer_1(post_fusion_dropped))\n",
    "        post_fusion_y_2 = F.relu(self.post_fusion_layer_2(post_fusion_y_1))\n",
    "        output = self.post_fusion_layer_3(post_fusion_y_2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/pol_temp_rh'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_uniform, xavier_normal, orthogonal\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 7, 5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mfn(X_train, y_train, X_valid, y_valid, X_test, y_test, configs):\n",
    "#     p = np.random.permutation(X_train.shape[0])\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "#     def swap_axes(nparr):\n",
    "#         return nparr.swapaxes(0,1)\n",
    "#     X_train = swap_axes(X_train)\n",
    "#     X_valid = swap_axes(X_valid)\n",
    "#     X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters \n",
    "    input_size = X_train.shape[2]\n",
    "    h = 128\n",
    "    t = X_train.shape[1]\n",
    "    output_dim = 1\n",
    "    dropout = 0.5\n",
    "    \n",
    "    model = TFN(configs[\"input_dims\"], configs[\"h_dims\"], configs[\"text_out\"],\n",
    "               configs[\"dropouts\"], configs[\"post_fusion_dim\"])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "#     criterion = nn.L1Loss()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer,mode='min',patience=100,factor=0.5,verbose=True)\n",
    "\n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[0]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[start:end, :])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/temp_models/mfn_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "#     print 'model number is:', rand\n",
    "    model = torch.load('models/temp_models/mfn_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSubNet(nn.Module):\n",
    "    '''\n",
    "    The LSTM-based subnetwork that is used in TFN for text\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_size, hidden_size, out_size, num_layers=7, dropout=0.2, bidirectional=False):\n",
    "        '''\n",
    "        Args:\n",
    "            in_size: input dimension\n",
    "            hidden_size: hidden layer dimension\n",
    "            num_layers: specify the number of layers of LSTMs.\n",
    "            dropout: dropout probability\n",
    "            bidirectional: specify usage of bidirectional LSTM\n",
    "        Output:\n",
    "            (return value in forward) a tensor of shape (batch_size, out_size)\n",
    "        '''\n",
    "        super(TextSubNet, self).__init__()\n",
    "        self.rnn = nn.LSTM(in_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_1 = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x: tensor of shape (batch_size, sequence_len, in_size)\n",
    "        '''\n",
    "        _, final_states = self.rnn(x)\n",
    "#         print(\"shape of the final_states\")\n",
    "#         print(final_states[0].size())\n",
    "        h = self.dropout(final_states[0][-1].squeeze())\n",
    "        y_1 = self.linear_1(h)\n",
    "        return y_1\n",
    "\n",
    "\n",
    "class TFN(nn.Module):\n",
    "    '''\n",
    "    Implements the Tensor Fusion Networks for multimodal sentiment analysis as is described in:\n",
    "    Zadeh, Amir, et al. \"Tensor fusion network for multimodal sentiment analysis.\" EMNLP 2017 Oral.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_dims, hidden_dims, text_out, dropouts, post_fusion_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dims - a length-3 tuple, contains (audio_dim, video_dim, text_dim)\n",
    "            hidden_dims - another length-3 tuple, similar to input_dims\n",
    "            text_out - int, specifying the resulting dimensions of the text subnetwork\n",
    "            dropouts - a length-4 tuple, contains (audio_dropout, video_dropout, text_dropout, post_fusion_dropout)\n",
    "            post_fusion_dim - int, specifying the size of the sub-networks after tensorfusion\n",
    "        Output:\n",
    "            (return value in forward) a scalar value between -3 and 3\n",
    "        '''\n",
    "        super(TFN, self).__init__()\n",
    "\n",
    "        # dimensions are specified in the order of audio, video and text\n",
    "        self.audio_in = input_dims[0]\n",
    "        self.text_in = input_dims[1]\n",
    "\n",
    "        self.audio_hidden = hidden_dims[0]\n",
    "        self.text_hidden = hidden_dims[1]\n",
    "        self.audio_out = text_out[0]\n",
    "        self.text_out= text_out[1]\n",
    "        self.post_fusion_dim = post_fusion_dim\n",
    "\n",
    "        self.audio_prob = dropouts[0]\n",
    "        self.text_prob = dropouts[1]\n",
    "        self.post_fusion_prob = dropouts[2]\n",
    "\n",
    "        # define the pre-fusion subnetworks\n",
    "        self.audio_subnet = TextSubNet(self.audio_in, self.audio_hidden, self.audio_out, dropout=self.audio_prob)\n",
    "        self.text_subnet = TextSubNet(self.text_in, self.text_hidden, self.text_out, dropout=self.text_prob)\n",
    "\n",
    "        # define the post_fusion layers\n",
    "        self.post_fusion_dropout = nn.Dropout(p=self.post_fusion_prob)\n",
    "        self.post_fusion_layer_1 = nn.Linear((self.text_out + 1) * (self.audio_hidden + 1), self.post_fusion_dim)\n",
    "        self.post_fusion_layer_2 = nn.Linear(self.post_fusion_dim, self.post_fusion_dim)\n",
    "        self.post_fusion_layer_3 = nn.Linear(self.post_fusion_dim, 1)\n",
    "\n",
    "        # in TFN we are doing a regression with constrained output range: (-3, 3), hence we'll apply sigmoid to output\n",
    "        # shrink it to (0, 1), and scale\\shift it back to range (-3, 3)\n",
    "#         self.output_range = Parameter(torch.FloatTensor([6]), requires_grad=False)\n",
    "#         self.output_shift = Parameter(torch.FloatTensor([-3]), requires_grad=False)\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        '''\n",
    "        Args:\n",
    "            audio_x: tensor of shape (batch_size, sequence_len, audio_in)\n",
    "            text_x: tensor of shape (batch_size, sequence_len, text_in)\n",
    "        '''\n",
    "        audio_x = input_x[:,:,:self.audio_in]\n",
    "        text_x = input_x[:,:,self.audio_in:self.audio_in+self.text_in]\n",
    "#         print(audio_x.size())\n",
    "        audio_h = self.audio_subnet(audio_x)\n",
    "#         print(audio_h.size())\n",
    "        text_h = self.text_subnet(text_x)\n",
    "        batch_size = audio_h.data.shape[0]\n",
    "\n",
    "        # next we perform \"tensor fusion\", which is essentially appending 1s to the tensors and take Kronecker product\n",
    "        if audio_h.is_cuda:\n",
    "            DTYPE = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            DTYPE = torch.FloatTensor\n",
    "\n",
    "#         print(\"the size of audio_h\")\n",
    "        _audio_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), audio_h), dim=1)\n",
    "        _text_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), text_h), dim=1)\n",
    "\n",
    "        # _audio_h has shape (batch_size, audio_in + 1), _video_h has shape (batch_size, _video_in + 1)\n",
    "        # we want to perform outer product between the two batch, hence we unsqueenze them to get\n",
    "        # (batch_size, audio_in + 1, 1) X (batch_size, 1, video_in + 1)\n",
    "        # fusion_tensor will have shape (batch_size, audio_in + 1, video_in + 1)\n",
    "        fusion_tensor = torch.bmm(_audio_h.unsqueeze(2), _text_h.unsqueeze(1))\n",
    "        \n",
    "        # next we do kronecker product between fusion_tensor and _text_h. This is even trickier\n",
    "        # we have to reshape the fusion tensor during the computation\n",
    "        # in the end we don't keep the 3-D tensor, instead we flatten it\n",
    "        fusion_tensor = fusion_tensor.view(batch_size, -1)\n",
    "\n",
    "        post_fusion_dropped = self.post_fusion_dropout(fusion_tensor)\n",
    "        post_fusion_y_1 = F.relu(self.post_fusion_layer_1(post_fusion_dropped))\n",
    "        post_fusion_y_2 = F.relu(self.post_fusion_layer_2(post_fusion_y_1))\n",
    "        output = self.post_fusion_layer_3(post_fusion_y_2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 151.12362798055014 93.65888214111328 saving model\n",
      "1 86.34914620717366 44.03561019897461 saving model\n",
      "2 42.66665426890055 20.319063186645508 saving model\n",
      "3 44.92241954803467 17.869155883789062 saving model\n",
      "4 38.23664093017578 18.1877384185791\n",
      "5 36.67391268412272 17.75365447998047 saving model\n",
      "6 37.218139012654625 18.480466842651367\n",
      "7 38.26812203725179 17.94582748413086\n",
      "8 37.21442731221517 18.217092514038086\n",
      "9 35.80371252695719 18.401939392089844\n",
      "10 35.916890144348145 18.30585289001465\n",
      "11 36.411628087361656 18.577512741088867\n",
      "12 35.58442465464274 18.163572311401367\n",
      "13 37.7751522064209 18.312070846557617\n",
      "14 37.28560988108317 18.125551223754883\n",
      "15 35.509450912475586 18.287067413330078\n",
      "Epoch    17: reducing learning rate of group 0 to 5.0000e-04.\n",
      "16 36.22530206044515 18.38686180114746\n",
      "17 34.77285925547282 19.009708404541016\n",
      "18 34.09031836191813 18.618375778198242\n",
      "19 35.99804814656576 19.178516387939453\n",
      "20 33.9280424118042 18.831621170043945\n",
      "21 35.46969827016195 18.819583892822266\n",
      "22 35.415859858194985 18.547557830810547\n",
      "23 34.512577056884766 19.166534423828125\n",
      "24 35.366448720296226 18.812206268310547\n",
      "25 33.5085252126058 18.774545669555664\n",
      "26 35.12739499409994 19.546695709228516\n",
      "Epoch    28: reducing learning rate of group 0 to 2.5000e-04.\n",
      "27 35.74388821919759 19.10423469543457\n",
      "28 35.974672635396324 19.320281982421875\n",
      "29 35.826826095581055 19.96586036682129\n",
      "30 35.25912062327067 20.165794372558594\n",
      "31 34.409043312072754 20.366846084594727\n",
      "32 34.204159100850426 20.62226104736328\n",
      "33 34.245550791422524 20.652725219726562\n",
      "34 33.0400398572286 20.692909240722656\n",
      "35 35.726113637288414 20.83247947692871\n",
      "36 34.28665939966837 20.89116096496582\n",
      "37 33.9139461517334 21.37254524230957\n",
      "Epoch    39: reducing learning rate of group 0 to 1.2500e-04.\n",
      "38 34.00039418538412 21.686920166015625\n",
      "39 33.71172412236532 21.489229202270508\n",
      "40 34.59820810953776 21.21274185180664\n",
      "41 33.92791223526001 21.157649993896484\n",
      "42 34.089646339416504 21.208662033081055\n",
      "43 33.21494070688883 21.157133102416992\n",
      "44 33.27029037475586 21.138525009155273\n",
      "45 33.643442471822105 21.1216983795166\n",
      "46 34.90577030181885 21.232080459594727\n",
      "47 34.13725026448568 21.105512619018555\n",
      "48 33.30201530456543 21.01599884033203\n",
      "Epoch    50: reducing learning rate of group 0 to 6.2500e-05.\n",
      "49 33.498543898264565 21.160057067871094\n",
      "mae:  3.5797610874018395\n",
      "mse:  18.214823584689395\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 128\n",
    "ha = 128\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"text_out\"] = (hl, ha)\n",
    "config[\"dropouts\"] = (0.7, 0.7, 0.7)\n",
    "config[\"post_fusion_dim\"] = hl\n",
    "config[\"batchsize\"] = hl\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.001\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 141.4877210344587 110.12069702148438 saving model\n",
      "1 140.76679011753626 109.37950134277344 saving model\n",
      "2 139.81058029901413 108.35891723632812 saving model\n",
      "3 138.21675836472284 106.33660125732422 saving model\n",
      "4 134.8403607323056 101.5842514038086 saving model\n",
      "5 126.28743816557385 88.8018569946289 saving model\n",
      "6 102.01781717936198 50.39952087402344 saving model\n",
      "7 50.85084006899879 21.110916137695312 saving model\n",
      "8 32.86671420506069 19.08951187133789 saving model\n",
      "9 31.231712068830216 18.765867233276367 saving model\n",
      "10 32.55115191141764 18.407224655151367 saving model\n",
      "11 29.867077373322985 19.61494255065918\n",
      "12 31.41922037942069 18.98175811767578\n",
      "13 31.81729452950614 19.254167556762695\n",
      "14 31.29679570879255 18.660823822021484\n",
      "15 31.533252034868514 18.63995933532715\n",
      "16 30.211222194489977 19.693208694458008\n",
      "17 31.112482252575102 19.11903190612793\n",
      "18 30.11747269403367 19.199750900268555\n",
      "19 29.940886542910622 19.352855682373047\n",
      "20 31.118786721002486 19.423049926757812\n",
      "Epoch    22: reducing learning rate of group 0 to 5.0000e-05.\n",
      "21 32.47978973388672 19.066787719726562\n",
      "22 29.616078649248397 19.087383270263672\n",
      "23 31.127643812270392 19.48153305053711\n",
      "24 31.004994528634207 19.25589370727539\n",
      "25 30.658178238641646 19.16341209411621\n",
      "26 29.57322629292806 19.178226470947266\n",
      "27 30.15700998760405 19.039188385009766\n",
      "28 29.66053127107166 19.18540382385254\n",
      "29 31.483207112266903 19.46042823791504\n",
      "30 30.063406172252837 19.20423126220703\n",
      "31 30.32147371201288 19.416959762573242\n",
      "Epoch    33: reducing learning rate of group 0 to 2.5000e-05.\n",
      "32 30.15276640937442 19.46652603149414\n",
      "33 31.35263488406227 19.505796432495117\n",
      "34 30.124087379092263 19.374624252319336\n",
      "35 31.656148819696334 19.422042846679688\n",
      "36 30.056173052106583 19.22211265563965\n",
      "37 27.981276012602308 19.537057876586914\n",
      "38 30.798158146086195 19.708757400512695\n",
      "39 30.57439963022868 19.56110382080078\n",
      "40 29.903403736296156 19.52427101135254\n",
      "41 30.834389868236723 19.246564865112305\n",
      "42 29.63710943857829 19.155858993530273\n",
      "Epoch    44: reducing learning rate of group 0 to 1.2500e-05.\n",
      "43 29.82164410182408 19.25910186767578\n",
      "44 31.303155944460915 19.289104461669922\n",
      "45 29.026258423214866 19.334999084472656\n",
      "46 30.678893952142623 19.478862762451172\n",
      "47 30.606994946797688 19.45387840270996\n",
      "48 29.292383148556663 19.502140045166016\n",
      "49 30.20730917794364 19.63481330871582\n",
      "mae:  3.8109978242353963\n",
      "mse:  20.570716894799265\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 32\n",
    "ha = 32\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"text_out\"] = (hl, ha)\n",
    "config[\"dropouts\"] = (0.5, 0.5, 0.5)\n",
    "config[\"post_fusion_dim\"] = hl\n",
    "config[\"batchsize\"] = hl\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.0001\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 147.58606234463778 111.87670135498047 saving model\n",
      "1 146.96226570822975 111.15380096435547 saving model\n",
      "2 145.9830884066495 110.06047058105469 saving model\n",
      "3 144.4334820834073 108.15937042236328 saving model\n",
      "4 141.6655606356534 104.71897888183594 saving model\n",
      "5 136.27631933038884 97.34671783447266 saving model\n",
      "6 124.28441897305575 80.16642761230469 saving model\n",
      "7 96.32560244473544 41.57260513305664 saving model\n",
      "8 53.197735873135656 26.51873016357422 saving model\n",
      "9 39.153928236527875 19.488378524780273 saving model\n",
      "10 34.927127838134766 17.92530632019043 saving model\n",
      "11 36.77009235728871 20.452730178833008\n",
      "12 36.341758728027344 19.849782943725586\n",
      "13 32.72726882587779 18.940858840942383\n",
      "14 34.71786932511763 18.81072235107422\n",
      "15 35.562731916254215 18.986814498901367\n",
      "16 37.82662374323065 19.108989715576172\n",
      "17 33.94412508877841 19.782392501831055\n",
      "18 35.02877270091664 19.65970230102539\n",
      "19 34.42843922701749 18.78156852722168\n",
      "20 34.22343652898615 18.547237396240234\n",
      "Epoch    22: reducing learning rate of group 0 to 5.0000e-05.\n",
      "21 33.77676426280629 19.05195426940918\n",
      "22 34.47819215601141 19.1637020111084\n",
      "23 35.55937680331144 19.662900924682617\n",
      "24 33.96130804582076 19.363500595092773\n",
      "25 33.60244829004461 19.46852684020996\n",
      "26 33.09134379300204 19.68158531188965\n",
      "27 34.73756720803001 19.383573532104492\n",
      "28 33.828337062488906 19.15834617614746\n",
      "29 35.662117351185195 19.06853675842285\n",
      "30 34.899157610806554 19.410858154296875\n",
      "31 36.16243275729093 19.499942779541016\n",
      "Epoch    33: reducing learning rate of group 0 to 2.5000e-05.\n",
      "32 35.48839499733665 19.323057174682617\n",
      "33 34.049142143943094 19.244150161743164\n",
      "34 34.66633449901234 19.337766647338867\n",
      "35 34.500769181685015 19.39553451538086\n",
      "36 33.91476267034357 19.36629295349121\n",
      "37 34.74482328241522 19.30963706970215\n",
      "38 33.3327940160578 19.399024963378906\n",
      "39 33.357662894509055 19.438758850097656\n",
      "40 37.031335310502485 19.172178268432617\n",
      "41 37.0491719679399 18.9810848236084\n",
      "42 33.2348088351163 18.88198471069336\n",
      "Epoch    44: reducing learning rate of group 0 to 1.2500e-05.\n",
      "43 33.33218903975053 19.040494918823242\n",
      "44 33.120873191139914 19.075756072998047\n",
      "45 32.179313312877305 19.171066284179688\n",
      "46 34.02298979325728 19.241962432861328\n",
      "47 34.785062616521664 19.261028289794922\n",
      "48 32.440464366566054 19.265079498291016\n",
      "49 35.94100657376376 19.24647331237793\n",
      "mae:  3.6883250843394886\n",
      "mse:  19.28380409213091\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 64\n",
    "ha = 64\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"text_out\"] = (hl, ha)\n",
    "config[\"dropouts\"] = (0.7, 0.7, 0.7)\n",
    "config[\"post_fusion_dim\"] = hl\n",
    "config[\"batchsize\"] = hl\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.0001\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dims' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-a4a7353c7567>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_fusion_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dims' is not defined"
     ]
    }
   ],
   "source": [
    "input_dims, hidden_dims, text_out, dropouts, post_fusion_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFN(input_dims, (32, 32), (32, 32), (0.3, 0.3, 0.3), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor([6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.ones([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
