{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/met-search'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control experiment using last observed value for missing data imputation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_dim, dropout):\n",
    "        \"\"\"\n",
    "        input_size - the number of expected features in the input x\n",
    "        hidden_size - the number of hidden units in state h\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.h = hidden_size\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (time_step, n_features)\n",
    "        \"\"\"\n",
    "        t = x.shape[0]\n",
    "        n = x.shape[1]\n",
    "        self.hx = torch.zeros(n, self.h)\n",
    "        self.cx = torch.zeros(n, self.h)\n",
    "        all_hs = []\n",
    "        all_cs = []\n",
    "        # iterate through cells \n",
    "        for i in range(t):\n",
    "            self.hx, self.cx = self.lstm(x[i], (self.hx, self.cx))\n",
    "            all_hs.append(self.hx)\n",
    "            all_cs.append(self.cx)\n",
    "        # last hidden layer last_hs is n * h\n",
    "        last_hs = all_hs[-1]\n",
    "        output = F.relu(self.fc1(last_hs))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "def train_lstm(X_train, y_train, X_valid, y_valid, X_test, y_test, config):\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "    def swap_axes(nparr):\n",
    "        return nparr.swapaxes(0,1)\n",
    "    X_train = swap_axes(X_train)\n",
    "    X_valid = swap_axes(X_valid)\n",
    "    X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters\n",
    "    input_size = X_train.shape[2]\n",
    "    h = config[\"h\"]\n",
    "    t = X_train.shape[0]\n",
    "    output_dim = 1\n",
    "    dropout = config[\"drop\"]\n",
    "    \n",
    "    model = LSTM(input_size, h, output_dim, dropout)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[1]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[:, start:end])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches \n",
    "    \n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    # timing\n",
    "#     start_time = time.time()\n",
    "#     predictions = predict(model, X_test)\n",
    "#     print(predictions.shape)\n",
    "#     print(predictions)\n",
    "#     end_time = time.time()\n",
    "#     print(end_time-start_time)\n",
    "#     assert False\n",
    "     \n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/lstm_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "    model = torch.load('models/lstm_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)\n",
    "#     corr = np.corrcoef(predictions,y_test)[0][1]\n",
    "#     print(\"corr: \", corr)\n",
    "#     true_label = (y_test >= 0)\n",
    "#     sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 141.50481887090774 110.30804443359375 saving model\n",
      "1 141.34756760370163 110.11189270019531 saving model\n",
      "2 141.01817648751395 109.91075897216797 saving model\n",
      "3 140.76776486351378 109.69828033447266 saving model\n",
      "4 140.55548132033576 109.47544860839844 saving model\n",
      "5 140.23985181535994 109.23992919921875 saving model\n",
      "6 139.93800535656158 108.99242401123047 saving model\n",
      "7 139.54592786516463 108.72837829589844 saving model\n",
      "8 139.09881664457777 108.43704223632812 saving model\n",
      "9 138.8563966296968 108.12977600097656 saving model\n",
      "10 138.43473125639417 107.79744720458984 saving model\n",
      "11 137.93509637741815 107.4321517944336 saving model\n",
      "12 137.3997308640253 107.02713775634766 saving model\n",
      "13 136.84078652518136 106.57701873779297 saving model\n",
      "14 136.28516678583054 106.07667541503906 saving model\n",
      "15 135.63209424700057 105.52751922607422 saving model\n",
      "16 134.6635506039574 104.88420104980469 saving model\n",
      "17 133.6219242640904 104.1524658203125 saving model\n",
      "18 132.55049968901136 103.30713653564453 saving model\n",
      "19 131.2511687505813 102.33670043945312 saving model\n",
      "20 129.49643343970888 101.1863021850586 saving model\n",
      "21 127.80616287958054 99.82447052001953 saving model\n",
      "22 125.45014953613281 98.17716217041016 saving model\n",
      "23 122.33189573742095 96.14901733398438 saving model\n",
      "24 119.18066115606399 93.71194458007812 saving model\n",
      "25 115.25105467296783 90.74832916259766 saving model\n",
      "26 110.08848608107795 87.13672637939453 saving model\n",
      "27 104.56355594453358 82.85031127929688 saving model\n",
      "28 98.07668885730561 77.85702514648438 saving model\n",
      "29 89.9049459184919 72.20652770996094 saving model\n",
      "30 82.6145993187314 66.09024810791016 saving model\n",
      "31 74.55349222819011 59.723331451416016 saving model\n",
      "32 65.69552376156761 53.4228630065918 saving model\n",
      "33 58.18981815519787 47.5175666809082 saving model\n",
      "34 50.27755019778297 42.298343658447266 saving model\n",
      "35 45.41251309712728 37.95105743408203 saving model\n",
      "36 40.235643931797576 34.54605484008789 saving model\n",
      "37 37.11989779699417 31.87035369873047 saving model\n",
      "38 34.79609416780018 29.94545555114746 saving model\n",
      "39 32.91560222989037 28.590486526489258 saving model\n",
      "40 31.29362764812651 27.516023635864258 saving model\n",
      "41 29.244930131094797 26.780202865600586 saving model\n",
      "42 30.297434398106166 26.33147430419922 saving model\n",
      "43 29.39758991059803 26.041105270385742 saving model\n",
      "44 30.441817510695685 25.850860595703125 saving model\n",
      "45 29.643476304553804 25.67383575439453 saving model\n",
      "46 29.7216922215053 25.59128761291504 saving model\n",
      "47 29.445170947483607 25.572206497192383 saving model\n",
      "48 30.27823325565883 25.63620376586914\n",
      "49 30.355778240022204 25.695865631103516\n",
      "50 28.83885992140997 25.69655990600586\n",
      "51 29.699091911315918 25.71956443786621\n",
      "52 28.411716370355514 25.727025985717773\n",
      "53 28.217195964994886 25.74073028564453\n",
      "54 28.85857445853097 25.74734115600586\n",
      "55 28.374501455397834 25.778974533081055\n",
      "56 29.114785739353724 25.872051239013672\n",
      "57 28.865484918866837 25.997346878051758\n",
      "Epoch    59: reducing learning rate of group 0 to 5.0000e-05.\n",
      "58 29.396290688287642 26.099693298339844\n",
      "59 29.60238061632429 26.115934371948242\n",
      "60 28.64177672068278 26.13776397705078\n",
      "61 28.570499965122767 26.126005172729492\n",
      "62 27.919262250264484 26.143510818481445\n",
      "63 29.107892445155553 26.162141799926758\n",
      "64 29.615656444004603 26.19652557373047\n",
      "65 27.86270137060256 26.232301712036133\n",
      "66 28.759332883925666 26.297584533691406\n",
      "67 30.361344292050315 26.335914611816406\n",
      "68 29.075006076267787 26.36408233642578\n",
      "Epoch    70: reducing learning rate of group 0 to 2.5000e-05.\n",
      "69 28.678987593877885 26.407224655151367\n",
      "70 28.534240495590936 26.42119026184082\n",
      "71 27.522685732160294 26.43463897705078\n",
      "72 28.785665557498024 26.458566665649414\n",
      "73 28.298193386622838 26.479827880859375\n",
      "74 27.69657598223005 26.490339279174805\n",
      "75 29.5999566032773 26.497426986694336\n",
      "76 27.93895335424514 26.502878189086914\n",
      "77 26.670669283185685 26.510257720947266\n",
      "78 28.215719359261648 26.525230407714844\n",
      "79 27.682312329610188 26.514009475708008\n",
      "Epoch    81: reducing learning rate of group 0 to 1.2500e-05.\n",
      "80 27.31435335250128 26.522541046142578\n",
      "81 27.99129222688221 26.533374786376953\n",
      "82 27.565734363737562 26.54288101196289\n",
      "83 28.422857602437336 26.544639587402344\n",
      "84 28.743491445268905 26.569385528564453\n",
      "85 26.89367153531029 26.58422088623047\n",
      "86 28.697242373511905 26.58616065979004\n",
      "87 28.241490545726958 26.580209732055664\n",
      "88 27.231571833292644 26.580440521240234\n",
      "89 27.806569190252397 26.58422088623047\n",
      "90 28.021801766895113 26.595548629760742\n",
      "Epoch    92: reducing learning rate of group 0 to 6.2500e-06.\n",
      "91 28.606099673679896 26.60152244567871\n",
      "92 27.651864505949476 26.602874755859375\n",
      "93 30.846601213727677 26.605209350585938\n",
      "94 28.162961732773553 26.60690689086914\n",
      "95 29.988317307971773 26.603864669799805\n",
      "96 28.24088191986084 26.61186408996582\n",
      "97 27.12605399177188 26.616905212402344\n",
      "98 28.915911901564826 26.627492904663086\n",
      "99 28.59647192273821 26.634075164794922\n",
      "100 27.662417320978072 26.638864517211914\n",
      "101 28.223484629676456 26.643386840820312\n",
      "Epoch   103: reducing learning rate of group 0 to 3.1250e-06.\n",
      "102 28.670827320643834 26.655771255493164\n",
      "103 28.79164218902588 26.660959243774414\n",
      "104 28.1942872546968 26.665681838989258\n",
      "105 27.013856252034504 26.66893768310547\n",
      "106 27.810686066037132 26.6710205078125\n",
      "107 28.607157661801292 26.67551612854004\n",
      "108 29.308644748869398 26.679384231567383\n",
      "109 28.167168435596285 26.68293571472168\n",
      "110 28.63167095184326 26.6876277923584\n",
      "111 28.29702023097447 26.689228057861328\n",
      "112 28.239876020522345 26.69125747680664\n",
      "Epoch   114: reducing learning rate of group 0 to 1.5625e-06.\n",
      "113 28.15048617408389 26.697072982788086\n",
      "114 29.902263823009672 26.699975967407227\n",
      "115 28.904914401826403 26.70380210876465\n",
      "116 27.470964613414946 26.70531463623047\n",
      "117 28.158324741181872 26.70481300354004\n",
      "118 29.266895339602517 26.70475196838379\n",
      "119 27.740944408235094 26.705121994018555\n",
      "120 27.345582507905505 26.70596694946289\n",
      "121 27.499711127508256 26.70718765258789\n",
      "122 27.308136622111004 26.70772933959961\n",
      "123 27.73423144930885 26.710756301879883\n",
      "Epoch   125: reducing learning rate of group 0 to 7.8125e-07.\n",
      "124 29.827318100702193 26.71308135986328\n",
      "125 28.875285239446733 26.714176177978516\n",
      "126 29.462057704017276 26.714689254760742\n",
      "127 29.067840212867374 26.71469497680664\n",
      "128 28.25574538821266 26.715534210205078\n",
      "129 27.508403505597794 26.715818405151367\n",
      "130 28.156282742818195 26.716598510742188\n",
      "131 29.197442554292223 26.718624114990234\n",
      "132 27.31655352456229 26.720012664794922\n",
      "133 28.529333296276274 26.7207088470459\n",
      "134 27.909724235534668 26.72248077392578\n",
      "Epoch   136: reducing learning rate of group 0 to 3.9063e-07.\n",
      "135 25.549733071100142 26.722768783569336\n",
      "136 27.745272818065825 26.722829818725586\n",
      "137 28.66632148197719 26.722713470458984\n",
      "138 28.07066281636556 26.723012924194336\n",
      "139 25.72001702444894 26.72327423095703\n",
      "140 27.7614103498913 26.723896026611328\n",
      "141 29.111767178490048 26.723905563354492\n",
      "142 29.093048459007626 26.724342346191406\n",
      "143 27.080629212515696 26.72460174560547\n",
      "144 28.66967451004755 26.724958419799805\n",
      "145 28.670121147519065 26.72534942626953\n",
      "Epoch   147: reducing learning rate of group 0 to 1.9531e-07.\n",
      "146 28.825202351524716 26.725780487060547\n",
      "147 28.67764409383138 26.725934982299805\n",
      "148 26.47029917580741 26.726409912109375\n",
      "149 28.250214758373442 26.7268009185791\n",
      "mae:  3.806851746425156\n",
      "mse:  20.83724820678853\n"
     ]
    }
   ],
   "source": [
    "config = {'h':32, 'lr':0.0001, 'num_epochs':150, 'batchsize':32, 'drop':0.5}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
