{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/pol_temp_rh'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control experiment using last observed value for missing data imputation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_dim, dropout):\n",
    "        \"\"\"\n",
    "        input_size - the number of expected features in the input x\n",
    "        hidden_size - the number of hidden units in state h\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.h = hidden_size\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (time_step, n_features)\n",
    "        \"\"\"\n",
    "        t = x.shape[0]\n",
    "        n = x.shape[1]\n",
    "        self.hx = torch.zeros(n, self.h)\n",
    "        self.cx = torch.zeros(n, self.h)\n",
    "        all_hs = []\n",
    "        all_cs = []\n",
    "        # iterate through cells \n",
    "        for i in range(t):\n",
    "            self.hx, self.cx = self.lstm(x[i], (self.hx, self.cx))\n",
    "            all_hs.append(self.hx)\n",
    "            all_cs.append(self.cx)\n",
    "        # last hidden layer last_hs is n * h\n",
    "        last_hs = all_hs[-1]\n",
    "        output = F.relu(self.fc1(last_hs))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "def train_lstm(X_train, y_train, X_valid, y_valid, X_test, y_test, config):\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "    def swap_axes(nparr):\n",
    "        return nparr.swapaxes(0,1)\n",
    "    X_train = swap_axes(X_train)\n",
    "    X_valid = swap_axes(X_valid)\n",
    "    X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters\n",
    "    input_size = X_train.shape[2]\n",
    "    h = config[\"h\"]\n",
    "    t = X_train.shape[0]\n",
    "    output_dim = 1\n",
    "    dropout = config[\"drop\"]\n",
    "    \n",
    "    model = LSTM(input_size, h, output_dim, dropout)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[1]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[:, start:end])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches \n",
    "    \n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    # timing\n",
    "#     start_time = time.time()\n",
    "#     predictions = predict(model, X_test)\n",
    "#     print(predictions.shape)\n",
    "#     print(predictions)\n",
    "#     end_time = time.time()\n",
    "#     print(end_time-start_time)\n",
    "#     assert False\n",
    "     \n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/lstm_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "    model = torch.load('models/lstm_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)\n",
    "#     corr = np.corrcoef(predictions,y_test)[0][1]\n",
    "#     print(\"corr: \", corr)\n",
    "#     true_label = (y_test >= 0)\n",
    "#     sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_train[:,:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 146.69658733549574 114.76644897460938 saving model\n",
      "1 146.245848156157 114.44473266601562 saving model\n",
      "2 145.9383356003534 114.11944580078125 saving model\n",
      "3 145.57897258940199 113.78263854980469 saving model\n",
      "4 145.13750821068174 113.42277526855469 saving model\n",
      "5 144.74981435139975 113.03050231933594 saving model\n",
      "6 144.3395254952567 112.59529876708984 saving model\n",
      "7 143.816528683617 112.10729217529297 saving model\n",
      "8 143.3779580252511 111.56178283691406 saving model\n",
      "9 142.69454556419737 110.94979095458984 saving model\n",
      "10 141.87390500023253 110.23274993896484 saving model\n",
      "11 141.12169174920945 109.38721466064453 saving model\n",
      "12 140.1775138491676 108.37999725341797 saving model\n",
      "13 138.97293127150763 107.15682220458984 saving model\n",
      "14 137.56603349958147 105.64766693115234 saving model\n",
      "15 135.86024765741257 103.73005676269531 saving model\n",
      "16 133.3707057407924 101.1859359741211 saving model\n",
      "17 129.87890407017298 97.73564147949219 saving model\n",
      "18 126.33740779331752 93.24112701416016 saving model\n",
      "19 120.39346131824311 87.40343475341797 saving model\n",
      "20 113.30886513846261 80.34400177001953 saving model\n",
      "21 104.81627618698846 72.40943145751953 saving model\n",
      "22 95.11116045997257 64.19467163085938 saving model\n",
      "23 86.11235119047619 56.251617431640625 saving model\n",
      "24 75.9703129359654 48.9736442565918 saving model\n",
      "25 69.51894914536248 42.622039794921875 saving model\n",
      "26 62.64531498863583 37.20673751831055 saving model\n",
      "27 55.12443742297945 32.685970306396484 saving model\n",
      "28 52.374864850725444 28.761138916015625 saving model\n",
      "29 44.94936797732399 25.64333724975586 saving model\n",
      "30 41.76570747012184 23.309978485107422 saving model\n",
      "31 41.653636205764045 21.618436813354492 saving model\n",
      "32 39.033377511160715 20.378538131713867 saving model\n",
      "33 36.437125069754465 19.49631690979004 saving model\n",
      "34 34.987315768287296 18.886083602905273 saving model\n",
      "35 35.047047070094514 18.459009170532227 saving model\n",
      "36 34.45899518330892 18.183568954467773 saving model\n",
      "37 32.82682922908238 18.008495330810547 saving model\n",
      "38 33.3347441809518 17.920021057128906 saving model\n",
      "39 32.077327728271484 17.88787078857422 saving model\n",
      "40 32.43661526271275 17.886859893798828 saving model\n",
      "41 31.39550681341262 17.907991409301758\n",
      "42 32.439984094528924 17.945524215698242\n",
      "43 32.894442058744886 17.981155395507812\n",
      "44 31.591160910470144 18.016029357910156\n",
      "45 32.73849019550142 18.05510711669922\n",
      "46 30.641689800080798 18.088485717773438\n",
      "47 32.08628232138498 18.120025634765625\n",
      "48 32.2621495837257 18.1343936920166\n",
      "49 32.29627236865816 18.14187240600586\n",
      "Epoch    51: reducing learning rate of group 0 to 5.0000e-05.\n",
      "50 30.215104784284318 18.166805267333984\n",
      "51 32.575649352300736 18.17782974243164\n",
      "52 30.173822448367165 18.18622398376465\n",
      "53 32.318800608317055 18.198461532592773\n",
      "54 30.811313856215705 18.195241928100586\n",
      "55 30.78186407543364 18.19288444519043\n",
      "56 32.514395986284526 18.19118309020996\n",
      "57 29.84041554587228 18.20077896118164\n",
      "58 31.77622935885475 18.197572708129883\n",
      "59 31.045513334728422 18.186359405517578\n",
      "60 31.942501976376487 18.19741439819336\n",
      "Epoch    62: reducing learning rate of group 0 to 2.5000e-05.\n",
      "61 30.151633943830216 18.204387664794922\n",
      "62 31.968502862112864 18.204252243041992\n",
      "63 30.340274084182013 18.211179733276367\n",
      "64 31.095404307047527 18.218141555786133\n",
      "65 32.109369550432476 18.22237205505371\n",
      "66 30.324167978195916 18.2210636138916\n",
      "67 33.40904072352818 18.219898223876953\n",
      "68 29.178102856590634 18.220109939575195\n",
      "69 32.936404818580264 18.224872589111328\n",
      "70 30.195298649015882 18.222360610961914\n",
      "71 28.68906997499012 18.226314544677734\n",
      "Epoch    73: reducing learning rate of group 0 to 1.2500e-05.\n",
      "72 30.138925279889786 18.228151321411133\n",
      "73 30.634648550124396 18.23262596130371\n",
      "74 30.828032266525994 18.23444366455078\n",
      "75 30.87933195204962 18.236295700073242\n",
      "76 31.39403733753023 18.24047088623047\n",
      "77 30.380487124125164 18.24468231201172\n",
      "78 30.972709610348655 18.252277374267578\n",
      "79 31.141726902553014 18.256540298461914\n",
      "80 31.612050011044456 18.258222579956055\n",
      "81 30.58262339092436 18.262765884399414\n",
      "82 31.393798555646622 18.265188217163086\n",
      "Epoch    84: reducing learning rate of group 0 to 6.2500e-06.\n",
      "83 32.15932246616909 18.26401138305664\n",
      "84 32.85099015917097 18.26338005065918\n",
      "85 33.1643381572905 18.263851165771484\n",
      "86 29.958597410292853 18.26337432861328\n",
      "87 31.866508256821405 18.26336097717285\n",
      "88 30.66274288722447 18.2640438079834\n",
      "89 32.03877603440058 18.265216827392578\n",
      "90 31.178061621529714 18.265365600585938\n",
      "91 30.02411360967727 18.264413833618164\n",
      "92 31.011736052376882 18.2640380859375\n",
      "93 30.474611282348633 18.26398468017578\n",
      "Epoch    95: reducing learning rate of group 0 to 3.1250e-06.\n",
      "94 31.594509397234237 18.264360427856445\n",
      "95 32.321537471952894 18.264310836791992\n",
      "96 30.406780833289737 18.263999938964844\n",
      "97 29.936099461146764 18.26393699645996\n",
      "98 31.435625439598446 18.26430892944336\n",
      "99 28.40873127891904 18.265443801879883\n",
      "100 30.21304443904332 18.266353607177734\n",
      "101 32.131225222633 18.266551971435547\n",
      "102 29.826945668175107 18.265981674194336\n",
      "103 30.479995409647625 18.266128540039062\n",
      "104 32.87448860350109 18.266395568847656\n",
      "Epoch   106: reducing learning rate of group 0 to 1.5625e-06.\n",
      "105 32.79586701166062 18.26559829711914\n",
      "106 29.887895084562757 18.265562057495117\n",
      "107 31.14359646751767 18.2658634185791\n",
      "108 30.54803939092727 18.265830993652344\n",
      "109 31.304363795689174 18.2662353515625\n",
      "110 31.388219470069522 18.26653289794922\n",
      "111 29.249313490731375 18.26637840270996\n",
      "112 31.49118559701102 18.26652717590332\n",
      "113 32.283416112264 18.266433715820312\n",
      "114 31.66404151916504 18.266178131103516\n",
      "115 29.430264609200613 18.26618003845215\n",
      "Epoch   117: reducing learning rate of group 0 to 7.8125e-07.\n",
      "116 31.501731509254093 18.266817092895508\n",
      "117 29.798213686261857 18.26685333251953\n",
      "118 31.886495998927526 18.267013549804688\n",
      "119 31.298611958821613 18.2672176361084\n",
      "120 31.251074200584775 18.267621994018555\n",
      "121 31.152361188616073 18.267871856689453\n",
      "122 31.240193548656645 18.267969131469727\n",
      "123 33.020925158546085 18.26766014099121\n",
      "124 31.844314756847563 18.267438888549805\n",
      "125 31.661719367617653 18.267358779907227\n",
      "126 32.25989750453404 18.2673397064209\n",
      "Epoch   128: reducing learning rate of group 0 to 3.9063e-07.\n",
      "127 28.98822121393113 18.267412185668945\n",
      "128 29.05057584671747 18.267484664916992\n",
      "129 32.1074655623663 18.267515182495117\n",
      "130 29.62195891425723 18.26763153076172\n",
      "131 30.431347165788925 18.26767349243164\n",
      "132 31.16421513330369 18.26763153076172\n",
      "133 31.704864683605376 18.267629623413086\n",
      "134 32.63542892819359 18.26755714416504\n",
      "135 31.71464965457008 18.267623901367188\n",
      "136 31.799558639526367 18.267627716064453\n",
      "137 31.528741745721724 18.26767921447754\n",
      "Epoch   139: reducing learning rate of group 0 to 1.9531e-07.\n",
      "138 30.10852137066069 18.267637252807617\n",
      "139 31.39185315086728 18.26764678955078\n",
      "140 30.350225539434526 18.2677001953125\n",
      "141 30.730538958594913 18.267757415771484\n",
      "142 32.20641917274112 18.267757415771484\n",
      "143 31.445335388183594 18.267778396606445\n",
      "144 31.280763353620255 18.26780128479004\n",
      "145 30.54633376711891 18.267839431762695\n",
      "146 33.9467222122919 18.267887115478516\n",
      "147 30.584072340102423 18.267902374267578\n",
      "148 31.416645322527206 18.267967224121094\n",
      "Epoch   150: reducing learning rate of group 0 to 9.7656e-08.\n",
      "149 30.819217818123953 18.267993927001953\n",
      "mae:  3.604614260965142\n",
      "mse:  18.381199056303046\n"
     ]
    }
   ],
   "source": [
    "config = {'h':32, 'lr':0.0001, 'num_epochs':150, 'batchsize':32, 'drop':0.5}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train[:,:,1:], label_train, last_dev[:,:,1:], label_dev, last_test[:,:,1:], label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 143.39490799676804 112.08809661865234 saving model\n",
      "1 142.88226572672525 111.64618682861328 saving model\n",
      "2 142.45459856305803 111.20196533203125 saving model\n",
      "3 142.03448631649925 110.74302673339844 saving model\n",
      "4 141.53744325183686 110.25714874267578 saving model\n",
      "5 140.9085736955915 109.73889923095703 saving model\n",
      "6 140.32575334821428 109.16926574707031 saving model\n",
      "7 139.70548502604166 108.6005859375 saving model\n",
      "8 139.0303475516183 107.97180938720703 saving model\n",
      "9 138.4253892444429 107.21587371826172 saving model\n",
      "10 137.373413449242 106.23810577392578 saving model\n",
      "11 136.09661356608072 104.85802459716797 saving model\n",
      "12 134.3394041515532 102.77561950683594 saving model\n",
      "13 131.76061321440199 99.80361938476562 saving model\n",
      "14 127.65883273170108 95.62702178955078 saving model\n",
      "15 122.50021798270089 89.80530548095703 saving model\n",
      "16 116.0691379365467 82.45384979248047 saving model\n",
      "17 106.96993310110909 74.1333236694336 saving model\n",
      "18 97.15976987566266 65.7936782836914 saving model\n",
      "19 86.87470735822406 58.12149429321289 saving model\n",
      "20 79.89391617547898 51.424259185791016 saving model\n",
      "21 72.12517456781296 45.75559997558594 saving model\n",
      "22 66.45742988586426 40.903533935546875 saving model\n",
      "23 61.04735837663923 36.80887222290039 saving model\n",
      "24 57.00946090334938 33.344730377197266 saving model\n",
      "25 53.20420991806757 30.469772338867188 saving model\n",
      "26 49.162768500191824 28.0455322265625 saving model\n",
      "27 46.7924428667341 25.999284744262695 saving model\n",
      "28 44.083048411778044 24.300771713256836 saving model\n",
      "29 43.65130079360235 22.889787673950195 saving model\n",
      "30 41.97777157738095 21.751863479614258 saving model\n",
      "31 39.45620936439151 20.809492111206055 saving model\n",
      "32 38.463168462117515 20.023666381835938 saving model\n",
      "33 37.65317898704892 19.39521598815918 saving model\n",
      "34 36.90892700921921 18.962251663208008 saving model\n",
      "35 35.22982642764137 18.62650489807129 saving model\n",
      "36 34.44699151175363 18.3662109375 saving model\n",
      "37 34.49543171837216 18.166915893554688 saving model\n",
      "38 35.43605904352097 18.012784957885742 saving model\n",
      "39 33.71048164367676 17.902618408203125 saving model\n",
      "40 32.608622142246794 17.824478149414062 saving model\n",
      "41 32.43688156491234 17.776042938232422 saving model\n",
      "42 35.37838263738723 17.742849349975586 saving model\n",
      "43 30.749696595328196 17.72698402404785 saving model\n",
      "44 33.88743037269229 17.724496841430664 saving model\n",
      "45 34.532591683524 17.730276107788086\n",
      "46 34.89535949343727 17.739259719848633\n",
      "47 31.9892676671346 17.752044677734375\n",
      "48 31.90916261218843 17.770122528076172\n",
      "49 33.401613962082635 17.776487350463867\n",
      "50 30.643758864629838 17.792213439941406\n",
      "51 33.34971437000093 17.80295753479004\n",
      "52 32.27483413332985 17.81849479675293\n",
      "53 33.8648506346203 17.832075119018555\n",
      "54 33.28266125633603 17.841880798339844\n",
      "Epoch    56: reducing learning rate of group 0 to 5.0000e-05.\n",
      "55 33.62111286889939 17.85178565979004\n",
      "56 32.998094331650506 17.85447883605957\n",
      "57 34.50565692356655 17.854711532592773\n",
      "58 32.60673068818592 17.847579956054688\n",
      "59 33.297953287760414 17.853025436401367\n",
      "60 33.64959544227237 17.855709075927734\n",
      "61 34.598639488220215 17.861175537109375\n",
      "62 33.59176794687907 17.857961654663086\n",
      "63 31.421521595546178 17.86473274230957\n",
      "64 34.01805396307083 17.869613647460938\n",
      "65 31.61739526476179 17.865724563598633\n",
      "Epoch    67: reducing learning rate of group 0 to 2.5000e-05.\n",
      "66 32.75431569417318 17.8730525970459\n",
      "67 34.24209721883138 17.87113380432129\n",
      "68 34.20820535932268 17.868894577026367\n",
      "69 30.448020480927966 17.873023986816406\n",
      "70 33.91342144920712 17.880144119262695\n",
      "71 34.615267799014134 17.881742477416992\n",
      "72 32.83911977495466 17.88319969177246\n",
      "73 34.14129080091204 17.88410758972168\n",
      "74 33.52524907248361 17.881216049194336\n",
      "75 33.8899165562221 17.880184173583984\n",
      "76 34.2968864440918 17.87616539001465\n",
      "Epoch    78: reducing learning rate of group 0 to 1.2500e-05.\n",
      "77 33.782340322222026 17.871774673461914\n",
      "78 35.109557560511995 17.871435165405273\n",
      "79 32.9029879342942 17.8714599609375\n",
      "80 32.585265568324495 17.872722625732422\n",
      "81 33.04365594046457 17.873315811157227\n",
      "82 33.24409893580845 17.872512817382812\n",
      "83 33.41173167455764 17.871416091918945\n",
      "84 32.601868266151065 17.8716983795166\n",
      "85 31.351089704604377 17.87152671813965\n",
      "86 33.58347797393799 17.871519088745117\n",
      "87 34.42132532028925 17.87120819091797\n",
      "Epoch    89: reducing learning rate of group 0 to 6.2500e-06.\n",
      "88 32.32351630074637 17.870267868041992\n",
      "89 32.065186455136256 17.87046241760254\n",
      "90 32.78946386064802 17.871530532836914\n",
      "91 33.94821834564209 17.872053146362305\n",
      "92 34.08655293782552 17.87337875366211\n",
      "93 33.6720556077503 17.873821258544922\n",
      "94 32.93597611926851 17.874618530273438\n",
      "95 33.57436911265055 17.874862670898438\n",
      "96 31.31832368033273 17.875062942504883\n",
      "97 34.43095575060163 17.87582778930664\n",
      "98 33.79627999805269 17.875450134277344\n",
      "Epoch   100: reducing learning rate of group 0 to 3.1250e-06.\n",
      "99 33.938873245602565 17.874923706054688\n",
      "100 31.439453125 17.875125885009766\n",
      "101 36.46998192015148 17.875520706176758\n",
      "102 35.21973546346029 17.875307083129883\n",
      "103 33.108642578125 17.874797821044922\n",
      "104 32.950512749808176 17.874797821044922\n",
      "105 33.863891329084126 17.874555587768555\n",
      "106 34.93669764200846 17.873998641967773\n",
      "107 31.344195411318825 17.87350082397461\n",
      "108 35.63445191156296 17.87333106994629\n",
      "109 33.90101723443894 17.87336540222168\n",
      "Epoch   111: reducing learning rate of group 0 to 1.5625e-06.\n",
      "110 33.18752025422596 17.87388801574707\n",
      "111 32.10870188758487 17.873952865600586\n",
      "112 31.404854002453032 17.873735427856445\n",
      "113 35.68235924130394 17.87336540222168\n",
      "114 35.78351311456589 17.87295913696289\n",
      "115 35.39167876470657 17.87282943725586\n",
      "116 33.32822572617304 17.872623443603516\n",
      "117 32.33637273879278 17.872671127319336\n",
      "118 33.783620788937526 17.87251853942871\n",
      "119 34.24998628525507 17.872318267822266\n",
      "120 34.258693468003045 17.871967315673828\n",
      "Epoch   122: reducing learning rate of group 0 to 7.8125e-07.\n",
      "121 32.542730058942524 17.872135162353516\n",
      "122 34.39134602319626 17.87212371826172\n",
      "123 31.047198250180198 17.872095108032227\n",
      "124 33.97332146054222 17.87222671508789\n",
      "125 31.833275749569847 17.872318267822266\n",
      "126 32.68433507283529 17.87249183654785\n",
      "127 33.41624423435756 17.872556686401367\n",
      "128 31.842130842662993 17.87262725830078\n",
      "129 33.91826956612723 17.87266731262207\n",
      "130 33.974901017688566 17.8725643157959\n",
      "131 32.560239065261115 17.87257194519043\n",
      "Epoch   133: reducing learning rate of group 0 to 3.9063e-07.\n",
      "132 35.17317381359282 17.87264060974121\n",
      "133 31.891158467247372 17.87263298034668\n",
      "134 33.312279928298224 17.87262725830078\n",
      "135 32.7513374146961 17.872676849365234\n",
      "136 34.75723779769171 17.872663497924805\n",
      "137 34.76497146061489 17.8726863861084\n",
      "138 32.696019308907644 17.8726863861084\n",
      "139 34.82247988382975 17.872690200805664\n",
      "140 33.38977405003139 17.872743606567383\n",
      "141 32.473382041567845 17.87272834777832\n",
      "142 32.57033239092146 17.872682571411133\n",
      "Epoch   144: reducing learning rate of group 0 to 1.9531e-07.\n",
      "143 31.94204412187849 17.87262535095215\n",
      "144 33.49922407241095 17.87266731262207\n",
      "145 34.77053442455473 17.87265396118164\n",
      "146 32.35436730157761 17.87266731262207\n",
      "147 34.59954543340774 17.8726749420166\n",
      "148 32.195995421636674 17.872671127319336\n",
      "149 32.69115048363095 17.8726806640625\n",
      "mae:  3.5802070917176807\n",
      "mse:  18.21230118108296\n"
     ]
    }
   ],
   "source": [
    "config = {'h':32, 'lr':0.0001, 'num_epochs':150, 'batchsize':32, 'drop':0.5}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train[:,:,0:1], label_train, last_dev[:,:,0:1], label_dev, last_test[:,:,0:1], label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13. , 10.8, 12.6,  4. ,  8.4, 10.6, 15.3,  3.8, 13.1,  4.5, 12. ,\n",
       "        6.8,  5.1,  6.3,  4.3,  5.8,  9. , 15. ,  5.2,  9.2,  2.8, 12.3,\n",
       "       11.3,  3.4, 13.5, 10.2,  5.1, 13.3,  8.4,  3.2, 18.2,  6.4,  3.9,\n",
       "        8.8, 11.9,  7.1,  5.3,  1.8,  8.3,  4.8, 12.6, 13.8, 10. , 15.5,\n",
       "       17.6,  4.5, 10.6,  3. ,  6.4,  3.1,  5.7,  6.7, 13. , 12.7,  9. ,\n",
       "       10.5, 12.6,  6.7,  8. ,  6.1,  5.9, 12.9,  8.3, 10.9, 12.9,  9.4,\n",
       "       11.9,  7.4,  8.3,  9.6,  9. ,  5. , 10.8,  7.1, 12.5, 18. ,  5.8,\n",
       "        8.2, 12.9, 12.5,  4.6,  4. ,  6.7,  7. ,  5.9,  4.4, 11.2,  8.7,\n",
       "        6.6,  8.4,  8.9, 15.5,  6.2,  3.5,  9.8,  3.6,  9.9,  9.9,  2.3,\n",
       "        6. ,  5.4,  4.7, 10. ,  4.7,  3.5,  7.2, 22.9, 12.8, 10.5,  8.8,\n",
       "        6. ,  3.2, 15.1,  4.8, 12.7,  2.2, 10.5,  6.3, 17.8,  4.2,  4.2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
