{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, filter_square_matrix, bias=True):\n",
    "        '''\n",
    "        filter_square_matrix : filter square matrix, whose each elements is 0 or 1.\n",
    "        '''\n",
    "        super(FilterLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        self.filter_square_matrix = None\n",
    "        if use_gpu:\n",
    "            self.filter_square_matrix = Variable(filter_square_matrix.cuda(), requires_grad=False)\n",
    "        else:\n",
    "            self.filter_square_matrix = Variable(filter_square_matrix, requires_grad=False)\n",
    "        \n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "#         print(self.weight.data)\n",
    "#         print(self.bias.data)\n",
    "\n",
    "    def forward(self, input):\n",
    "#         print(self.filter_square_matrix.mul(self.weight))\n",
    "        return F.linear(input, self.filter_square_matrix.mul(self.weight), self.bias)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) \\\n",
    "            + ', bias=' + str(self.bias is not None) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/met-search'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays\n",
    "\n",
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]\n",
    "\n",
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize delta\n",
    "# we can normlize this because it's know features \n",
    "delta_seq = (delta_seq - np.mean(delta_seq)) / np.std(delta_seq) \n",
    "\n",
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train, masking_train, delta_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq, masking_seq, delta_seq\n",
    "                                                                 ], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev, masking_dev, delta_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq, masking_seq, delta_seq\n",
    "                                                           ], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test, masking_test, delta_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq, masking_seq, delta_seq\n",
    "                                                              ], label_seq, test_index)\n",
    "\n",
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])\n",
    "\n",
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)\n",
    "# get dataset for grud \n",
    "def dataset_aggregation(feature_array, last_obsv, mask, delta):\n",
    "    # expand dimension of array\n",
    "    def expd(arr):\n",
    "        return np.expand_dims(arr, axis=1)\n",
    "    return np.concatenate((expd(feature_array), expd(last_obsv), expd(mask), expd(delta)), axis = 1)\n",
    "\n",
    "# dataset_aggregation for train, dev, test \n",
    "# train_aggr = dataset_aggregation(fea_train, last_train, masking_train, delta_train)\n",
    "train_aggr = dataset_aggregation(last_train, last_train, masking_train, delta_train)\n",
    "# dev_aggr = dataset_aggregation(fea_dev, last_dev, masking_dev, delta_dev)\n",
    "# test_aggr = dataset_aggregation(fea_test, last_test, masking_test, delta_test)\n",
    "dev_aggr = dataset_aggregation(last_dev, last_dev, masking_dev, delta_dev)\n",
    "test_aggr = dataset_aggregation(last_test, last_test, masking_test, delta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 4, 7, 52)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aggr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mfn(X_train, y_train, X_valid, y_valid, X_test, y_test, configs, X_mean):\n",
    "#     p = np.random.permutation(X_train.shape[0])\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "#     def swap_axes(nparr):\n",
    "#         return nparr.swapaxes(0,1)\n",
    "#     X_train = swap_axes(X_train)\n",
    "#     X_valid = swap_axes(X_valid)\n",
    "#     X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters \n",
    "    input_size = X_train.shape[3]\n",
    "    h = 32\n",
    "    t = X_train.shape[2]\n",
    "    output_dim = 1\n",
    "    dropout = 0.5\n",
    "\n",
    "#     d = X_train.shape[2]\n",
    "#     h = 128\n",
    "#     t = X_train.shape[0]\n",
    "#     output_dim = 1\n",
    "#     dropout = 0.5\n",
    "\n",
    "    [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig] = configs\n",
    "\n",
    "    \n",
    "    #model = EFLSTM(d,h,output_dim,dropout)\n",
    "    model = MFN(config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig, X_mean)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "    #optimizer = optim.SGD(model.parameters(),lr=config[\"lr\"],momentum=config[\"momentum\"])\n",
    "\n",
    "    # optimizer = optim.SGD([\n",
    "    #                 {'params':model.lstm_l.parameters(), 'lr':config[\"lr\"]},\n",
    "    #                 {'params':model.classifier.parameters(), 'lr':config[\"lr\"]}\n",
    "    #             ], momentum=0.9)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "#     criterion = nn.L1Loss()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer,mode='min',patience=100,factor=0.5,verbose=True)\n",
    "\n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[0]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[start:end,:])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/temp_models/mfn_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "#     print 'model number is:', rand\n",
    "    model = torch.load('models/temp_models/mfn_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUD(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, X_mean, output_last = True, dropout=0):\n",
    "        \"\"\"\n",
    "        Recurrent Neural Networks for Multivariate Times Series with Missing Values\n",
    "        GRU-D: GRU exploit two representations of informative missingness patterns, i.e., masking and time interval.\n",
    "        cell_size is the size of cell_state.\n",
    "        \n",
    "        GRU-D:\n",
    "            input_size: variable dimension of each time\n",
    "            hidden_size: dimension of hidden_state\n",
    "            mask_size: dimension of masking vector\n",
    "            X_mean: the mean of the historical input data\n",
    "        \"\"\"\n",
    "        super(GRUD, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.delta_size = input_size\n",
    "        self.mask_size = input_size\n",
    "        \n",
    "        self.identity = torch.eye(input_size)\n",
    "        self.zeros = Variable(torch.zeros(input_size))\n",
    "        \n",
    "        self.zl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size)\n",
    "        self.rl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size)\n",
    "        self.hl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size)\n",
    "        \n",
    "        self.gamma_x_l = FilterLinear(self.delta_size, self.delta_size, self.identity)\n",
    "        \n",
    "#         self.gamma_h_l = nn.Linear(self.delta_size, self.delta_size)\n",
    "        self.gamma_h_l = nn.Linear(self.delta_size, hidden_size)\n",
    "        \n",
    "        self.map_hidden_gamma = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, x, x_last_obsv, x_mean, h, mask, delta):\n",
    "    # use input to feed the four dimensions \n",
    "    def forward(self, dataset, X_mean, h):\n",
    "        \"\"\"\n",
    "        dataset: matrix shape (n * c * d)\n",
    "                    n: batchsize; c: class; d number of dims\n",
    "        \"\"\"\n",
    "        x_mean = Variable(torch.Tensor(X_mean))\n",
    "#         print(\"dataset size\")\n",
    "#         print(dataset.size())\n",
    "        batch_size = dataset.size(0)\n",
    "        dim_size = dataset.size(2)\n",
    "    \n",
    "        x = dataset[:,0,:]\n",
    "        x_last_obsv = dataset[:,1,:]\n",
    "        mask = dataset[:,2,:]\n",
    "        delta = dataset[:,3,:]\n",
    "        \n",
    "        delta_x = torch.exp(-torch.max(self.zeros, self.gamma_x_l(delta)))\n",
    "        \n",
    "        delta_h = F.sigmoid(self.map_hidden_gamma(F.relu(self.gamma_h_l(delta))))\n",
    "        \n",
    "        x = mask * x + (1 - mask) * (delta_x * x_last_obsv + (1 - delta_x) * x_mean)\n",
    "        h = delta_h * h \n",
    "    \n",
    "#         print(\"combined size\")\n",
    "#         print(x.size())\n",
    "#         print(h.size())\n",
    "#         print(mask.size())\n",
    "        combined = torch.cat((x, h, mask), 1)\n",
    "        z = torch.sigmoid(self.zl(combined))\n",
    "        r = torch.sigmoid(self.rl(combined))\n",
    "        combined_r = torch.cat((x, r * h, mask), 1)\n",
    "        h_tilde = torch.tanh(self.hl(combined_r))\n",
    "        h = (1 - z) * h + z * h_tilde\n",
    "        \n",
    "        return h\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFN(nn.Module):\n",
    "    def __init__(self,config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig, X_mean):\n",
    "        super(MFN, self).__init__()\n",
    "        [self.d_l,self.d_a] = config[\"input_dims\"]\n",
    "        [self.dh_l,self.dh_a] = config[\"h_dims\"]\n",
    "        total_h_dim = self.dh_l+self.dh_a\n",
    "        \n",
    "        self.mem_dim = config[\"memsize\"]\n",
    "        window_dim = config[\"windowsize\"]\n",
    "        output_dim = 1\n",
    "        attInShape = total_h_dim*window_dim\n",
    "        gammaInShape = attInShape+self.mem_dim\n",
    "        final_out = total_h_dim+self.mem_dim\n",
    "        h_att1 = NN1Config[\"shapes\"]\n",
    "        h_att2 = NN2Config[\"shapes\"]\n",
    "        h_gamma1 = gamma1Config[\"shapes\"]\n",
    "        h_gamma2 = gamma2Config[\"shapes\"]\n",
    "        h_out = outConfig[\"shapes\"]\n",
    "        att1_dropout = NN1Config[\"drop\"]\n",
    "        att2_dropout = NN2Config[\"drop\"]\n",
    "        gamma1_dropout = gamma1Config[\"drop\"]\n",
    "        gamma2_dropout = gamma2Config[\"drop\"]\n",
    "        out_dropout = outConfig[\"drop\"]\n",
    "    \n",
    "        # use grud cell instead of LSTM Cell\n",
    "#         (self, input_size, hidden_size, X_mean, output_last = True, dropout=0):\n",
    "        self.gru_l = GRUD(self.d_l, self.dh_l, X_mean)\n",
    "        self.gru_a = GRUD(self.d_a, self.dh_a, X_mean)\n",
    "\n",
    "        self.att1_fc1 = nn.Linear(attInShape, h_att1)\n",
    "        self.att1_fc2 = nn.Linear(h_att1, attInShape)\n",
    "        self.att1_dropout = nn.Dropout(att1_dropout)\n",
    "\n",
    "        self.att2_fc1 = nn.Linear(attInShape, h_att2)\n",
    "        self.att2_fc2 = nn.Linear(h_att2, self.mem_dim)\n",
    "        self.att2_dropout = nn.Dropout(att2_dropout)\n",
    "\n",
    "        self.gamma1_fc1 = nn.Linear(gammaInShape, h_gamma1)\n",
    "        self.gamma1_fc2 = nn.Linear(h_gamma1, self.mem_dim)\n",
    "        self.gamma1_dropout = nn.Dropout(gamma1_dropout)\n",
    "\n",
    "#         self.gamma2_fc1 = nn.Linear(gammaInShape, h_gamma2)\n",
    "#         self.gamma2_fc2 = nn.Linear(h_gamma2, self.mem_dim)\n",
    "#         self.gamma2_dropout = nn.Dropout(gamma2_dropout)\n",
    "\n",
    "        self.out_fc1 = nn.Linear(final_out, h_out)\n",
    "        self.out_fc2 = nn.Linear(h_out, output_dim)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "        \n",
    "        ## add the part for grud missing imputation \n",
    "        self.x_mean = X_mean \n",
    "        self.hidden_size = self.dh_l\n",
    "        \n",
    "        \n",
    "        ## \n",
    "        self.gamma_decay = nn.Linear(self.d_l+self.d_a, self.hidden_size)\n",
    "        self.map_gamma_decay = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "        \n",
    "    def squeeze_d2(self, matrix):\n",
    "        return torch.squeeze(matrix, dim=2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x_l = x[:,:,:,:self.d_l]\n",
    "        x_a = x[:,:,:,self.d_l:self.d_l+self.d_a]\n",
    "        # x is n x c x t x d\n",
    "        # seperate x_mean_l and x_mean_a \n",
    "        x_mean_l = self.x_mean[:,:self.d_l]\n",
    "        x_mean_a = self.x_mean[:,self.d_l:self.d_l+self.d_a]\n",
    "        \n",
    "        n = x.shape[0]\n",
    "        t = x.shape[1]\n",
    "        self.h_l = self.initHidden(n)\n",
    "        self.h_a = self.initHidden(n)\n",
    "        \n",
    "        self.mem = torch.zeros(n, self.mem_dim)\n",
    "        all_h_ls = []\n",
    "        all_h_as = []\n",
    "\n",
    "        all_mems = []\n",
    "        \n",
    "        for i in range(t):\n",
    "            # prev time step\n",
    "            prev_h_l = self.h_l\n",
    "            prev_h_a = self.h_a\n",
    "#             print(\"x_l size\")\n",
    "#             print(x_l.size())\n",
    "            # curr time step \n",
    "            new_h_l = self.gru_l(self.squeeze_d2(x_l[:,:,i:i+1,:]), x_mean_l[i:i+1,:], prev_h_l)\n",
    "            new_h_a = self.gru_a(self.squeeze_d2(x_a[:,:,i:i+1,:]), x_mean_a[i:i+1,:], prev_h_a)\n",
    "            # curr time step\n",
    "#             new_h_l, new_c_l = self.lstm_l(x_l[i], (self.h_l, self.c_l))\n",
    "#             new_h_a, new_c_a = self.lstm_a(x_a[i], (self.h_a, self.c_a))\n",
    "   \n",
    "            # concatenate\n",
    "            prev_hs = torch.cat([prev_h_l,prev_h_a], dim=1)\n",
    "            new_hs = torch.cat([new_h_l,new_h_a], dim=1)\n",
    "            \n",
    "            cStar = torch.cat([prev_hs,new_hs], dim=1)\n",
    "            attention = F.softmax(self.att1_fc2(self.att1_dropout(F.relu(self.att1_fc1(cStar)))),dim=1)\n",
    "            attended = attention*cStar\n",
    "            \n",
    "            cHat = F.tanh(self.att2_fc2(self.att2_dropout(F.relu(self.att2_fc1(attended)))))\n",
    "            \n",
    "            both = torch.cat([attended,self.mem], dim=1)\n",
    "            gamma1 = F.sigmoid(self.gamma1_fc2(self.gamma1_dropout(F.relu(self.gamma1_fc1(both)))))\n",
    "#             gamma2 = F.sigmoid(self.gamma2_fc2(self.gamma2_dropout(F.relu(self.gamma2_fc1(both)))))\n",
    "        \n",
    "            self.mem = gamma1*self.mem + (1-gamma1)*cHat\n",
    "        \n",
    "            # decay teh memory according to delta\n",
    "            gamma_decay = F.sigmoid(self.map_gamma_decay(F.relu(self.gamma_decay(torch.squeeze(x[:,3,i:i+1,:], dim=1)))))\n",
    "            self.mem = self.mem * gamma_decay \n",
    "        \n",
    "            # record delta condition for memory \n",
    "            \n",
    "            all_mems.append(self.mem)\n",
    "            # update\n",
    "            self.h_l = new_h_l\n",
    "            self.h_a = new_h_a\n",
    "\n",
    "            all_h_ls.append(self.h_l)\n",
    "            all_h_as.append(self.h_a)\n",
    "\n",
    "        # last hidden layer last_hs is n x h\n",
    "        last_h_l = all_h_ls[-1]\n",
    "        last_h_a = all_h_as[-1]\n",
    "\n",
    "        last_mem = all_mems[-1]\n",
    "        last_hs = torch.cat([last_h_l,last_h_a,last_mem], dim=1)\n",
    "        output = self.out_fc2(self.out_dropout(F.relu(self.out_fc1(last_hs))))\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            return Hidden_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            return Hidden_State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [5, 47]\n",
    "hl = 128\n",
    "ha = 128\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = 128\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = 32\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.005\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = 32\n",
    "NN1Config[\"drop\"] = 0.5\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = 0.5\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = 32\n",
    "gamma1Config[\"drop\"] = 0.5\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = 32 \n",
    "gamma2Config[\"drop\"] = 0.5\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = 32\n",
    "outConfig[\"drop\"] = 0.5\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(train_aggr, label_train, dev_aggr, label_dev, test_aggr, label_test, configs, x_mean_aft_nor)\n",
    "# train_grud(train_aggr, label_train, dev_aggr, label_dev, test_aggr, label_test, config, x_mean_aft_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
