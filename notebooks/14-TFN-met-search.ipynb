{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.init import xavier_uniform, xavier_normal, orthogonal\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/met-search'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])\n",
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mfn(X_train, y_train, X_valid, y_valid, X_test, y_test, configs):\n",
    "#     p = np.random.permutation(X_train.shape[0])\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "#     def swap_axes(nparr):\n",
    "#         return nparr.swapaxes(0,1)\n",
    "#     X_train = swap_axes(X_train)\n",
    "#     X_valid = swap_axes(X_valid)\n",
    "#     X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters \n",
    "    input_size = X_train.shape[2]\n",
    "    h = 128\n",
    "    t = X_train.shape[1]\n",
    "    output_dim = 1\n",
    "    dropout = 0.5\n",
    "    \n",
    "    model = TFN(configs[\"input_dims\"], configs[\"h_dims\"], configs[\"text_out\"],\n",
    "               configs[\"dropouts\"], configs[\"post_fusion_dim\"])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "#     criterion = nn.L1Loss()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer,mode='min',patience=100,factor=0.5,verbose=True)\n",
    "\n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[0]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[start:end, :])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/temp_models/mfn_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "#     print 'model number is:', rand\n",
    "    model = torch.load('models/temp_models/mfn_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSubNet(nn.Module):\n",
    "    '''\n",
    "    The LSTM-based subnetwork that is used in TFN for text\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_size, hidden_size, out_size, num_layers=7, dropout=0.2, bidirectional=False):\n",
    "        '''\n",
    "        Args:\n",
    "            in_size: input dimension\n",
    "            hidden_size: hidden layer dimension\n",
    "            num_layers: specify the number of layers of LSTMs.\n",
    "            dropout: dropout probability\n",
    "            bidirectional: specify usage of bidirectional LSTM\n",
    "        Output:\n",
    "            (return value in forward) a tensor of shape (batch_size, out_size)\n",
    "        '''\n",
    "        super(TextSubNet, self).__init__()\n",
    "        self.rnn = nn.LSTM(in_size, hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_1 = nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x: tensor of shape (batch_size, sequence_len, in_size)\n",
    "        '''\n",
    "        _, final_states = self.rnn(x)\n",
    "#         print(\"shape of the final_states\")\n",
    "#         print(final_states[0].size())\n",
    "        h = self.dropout(final_states[0][-1].squeeze())\n",
    "        y_1 = self.linear_1(h)\n",
    "        return y_1\n",
    "\n",
    "\n",
    "class TFN(nn.Module):\n",
    "    '''\n",
    "    Implements the Tensor Fusion Networks for multimodal sentiment analysis as is described in:\n",
    "    Zadeh, Amir, et al. \"Tensor fusion network for multimodal sentiment analysis.\" EMNLP 2017 Oral.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_dims, hidden_dims, text_out, dropouts, post_fusion_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dims - a length-3 tuple, contains (audio_dim, video_dim, text_dim)\n",
    "            hidden_dims - another length-3 tuple, similar to input_dims\n",
    "            text_out - int, specifying the resulting dimensions of the text subnetwork\n",
    "            dropouts - a length-4 tuple, contains (audio_dropout, video_dropout, text_dropout, post_fusion_dropout)\n",
    "            post_fusion_dim - int, specifying the size of the sub-networks after tensorfusion\n",
    "        Output:\n",
    "            (return value in forward) a scalar value between -3 and 3\n",
    "        '''\n",
    "        super(TFN, self).__init__()\n",
    "\n",
    "        # dimensions are specified in the order of audio, video and text\n",
    "        self.audio_in = input_dims[0]\n",
    "        self.text_in = input_dims[1]\n",
    "\n",
    "        self.audio_hidden = hidden_dims[0]\n",
    "        self.text_hidden = hidden_dims[1]\n",
    "        self.audio_out = text_out[0]\n",
    "        self.text_out= text_out[1]\n",
    "        self.post_fusion_dim = post_fusion_dim\n",
    "\n",
    "        self.audio_prob = dropouts[0]\n",
    "        self.text_prob = dropouts[1]\n",
    "        self.post_fusion_prob = dropouts[2]\n",
    "\n",
    "        # define the pre-fusion subnetworks\n",
    "        self.audio_subnet = TextSubNet(self.audio_in, self.audio_hidden, self.audio_out, dropout=self.audio_prob)\n",
    "        self.text_subnet = TextSubNet(self.text_in, self.text_hidden, self.text_out, dropout=self.text_prob)\n",
    "\n",
    "        # define the post_fusion layers\n",
    "        self.post_fusion_dropout = nn.Dropout(p=self.post_fusion_prob)\n",
    "        self.post_fusion_layer_1 = nn.Linear((self.text_out + 1) * (self.audio_hidden + 1), self.post_fusion_dim)\n",
    "        self.post_fusion_layer_2 = nn.Linear(self.post_fusion_dim, self.post_fusion_dim)\n",
    "        self.post_fusion_layer_3 = nn.Linear(self.post_fusion_dim, 1)\n",
    "\n",
    "        # in TFN we are doing a regression with constrained output range: (-3, 3), hence we'll apply sigmoid to output\n",
    "        # shrink it to (0, 1), and scale\\shift it back to range (-3, 3)\n",
    "#         self.output_range = Parameter(torch.FloatTensor([6]), requires_grad=False)\n",
    "#         self.output_shift = Parameter(torch.FloatTensor([-3]), requires_grad=False)\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        '''\n",
    "        Args:\n",
    "            audio_x: tensor of shape (batch_size, sequence_len, audio_in)\n",
    "            text_x: tensor of shape (batch_size, sequence_len, text_in)\n",
    "        '''\n",
    "        audio_x = input_x[:,:,:self.audio_in]\n",
    "        text_x = input_x[:,:,self.audio_in:self.audio_in+self.text_in]\n",
    "#         print(audio_x.size())\n",
    "        audio_h = self.audio_subnet(audio_x)\n",
    "#         print(audio_h.size())\n",
    "        text_h = self.text_subnet(text_x)\n",
    "        batch_size = audio_h.data.shape[0]\n",
    "\n",
    "        # next we perform \"tensor fusion\", which is essentially appending 1s to the tensors and take Kronecker product\n",
    "        if audio_h.is_cuda:\n",
    "            DTYPE = torch.cuda.FloatTensor\n",
    "        else:\n",
    "            DTYPE = torch.FloatTensor\n",
    "\n",
    "#         print(\"the size of audio_h\")\n",
    "        _audio_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), audio_h), dim=1)\n",
    "        _text_h = torch.cat((Variable(torch.ones(batch_size, 1).type(DTYPE), requires_grad=False), text_h), dim=1)\n",
    "\n",
    "        # _audio_h has shape (batch_size, audio_in + 1), _video_h has shape (batch_size, _video_in + 1)\n",
    "        # we want to perform outer product between the two batch, hence we unsqueenze them to get\n",
    "        # (batch_size, audio_in + 1, 1) X (batch_size, 1, video_in + 1)\n",
    "        # fusion_tensor will have shape (batch_size, audio_in + 1, video_in + 1)\n",
    "        fusion_tensor = torch.bmm(_audio_h.unsqueeze(2), _text_h.unsqueeze(1))\n",
    "        \n",
    "        # next we do kronecker product between fusion_tensor and _text_h. This is even trickier\n",
    "        # we have to reshape the fusion tensor during the computation\n",
    "        # in the end we don't keep the 3-D tensor, instead we flatten it\n",
    "        fusion_tensor = fusion_tensor.view(batch_size, -1)\n",
    "\n",
    "        post_fusion_dropped = self.post_fusion_dropout(fusion_tensor)\n",
    "        post_fusion_y_1 = F.relu(self.post_fusion_layer_1(post_fusion_dropped))\n",
    "        post_fusion_y_2 = F.relu(self.post_fusion_layer_2(post_fusion_y_1))\n",
    "        output = self.post_fusion_layer_3(post_fusion_y_2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 151.84854634602866 103.98616790771484 saving model\n",
      "1 133.5666643778483 66.28587341308594 saving model\n",
      "2 69.48322327931722 46.3281364440918 saving model\n",
      "3 40.065612157185875 21.371387481689453 saving model\n",
      "4 40.93886184692383 18.947879791259766 saving model\n",
      "5 41.11260414123535 19.045286178588867\n",
      "6 37.68639691670736 17.832937240600586 saving model\n",
      "7 37.26254081726074 20.074464797973633\n",
      "8 37.98599656422933 18.21662139892578\n",
      "9 35.11983426411947 18.573688507080078\n",
      "10 35.78106180826823 19.402132034301758\n",
      "11 36.040683110555015 18.502826690673828\n",
      "12 37.1142381032308 18.26140594482422\n",
      "13 35.903045336405434 18.619611740112305\n",
      "14 35.3189328511556 18.789703369140625\n",
      "15 34.80059019724528 18.913475036621094\n",
      "16 35.562243143717446 18.92255973815918\n",
      "Epoch    18: reducing learning rate of group 0 to 2.5000e-04.\n",
      "17 34.67629623413086 18.30867576599121\n",
      "18 38.35009765625 18.67597007751465\n",
      "19 36.48952992757162 18.79534339904785\n",
      "20 35.41590277353922 18.716506958007812\n",
      "21 34.0154832204183 18.768871307373047\n",
      "22 33.97651354471842 18.9824275970459\n",
      "23 35.534841537475586 19.2144718170166\n",
      "24 33.232114473978676 18.933391571044922\n",
      "25 35.202168782552086 19.3038387298584\n",
      "26 32.84220600128174 19.548147201538086\n",
      "27 31.778331120808918 19.624711990356445\n",
      "Epoch    29: reducing learning rate of group 0 to 1.2500e-04.\n",
      "28 34.77861404418945 20.73603630065918\n",
      "29 31.321674664815266 21.244173049926758\n",
      "30 32.056962966918945 21.755224227905273\n",
      "31 31.556941668192547 22.078134536743164\n",
      "32 30.238537152608234 22.633590698242188\n",
      "33 31.657963116963703 23.326356887817383\n",
      "34 30.17343870798747 23.854801177978516\n",
      "35 32.297169049580894 23.85546112060547\n",
      "36 29.63271204630534 23.922561645507812\n",
      "37 30.85763963063558 24.662620544433594\n",
      "38 31.80191485087077 24.596588134765625\n",
      "Epoch    40: reducing learning rate of group 0 to 6.2500e-05.\n",
      "39 31.812387466430664 24.614200592041016\n",
      "40 28.479005336761475 24.662845611572266\n",
      "41 28.732410113016766 24.56197738647461\n",
      "42 29.858908971150715 24.450864791870117\n",
      "43 29.03410307566325 24.532949447631836\n",
      "44 28.401760419209797 24.365436553955078\n",
      "45 29.426132520039875 24.332103729248047\n",
      "46 28.67468802134196 24.171236038208008\n",
      "47 28.699057420094807 23.906118392944336\n",
      "48 27.570281346638996 23.788301467895508\n",
      "49 29.003686904907227 23.3966007232666\n",
      "mae:  3.5143003558324386\n",
      "mse:  17.601897841008665\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [5, 47]\n",
    "hl = 128\n",
    "ha = 128\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"text_out\"] = (hl, ha)\n",
    "config[\"dropouts\"] = (0.7, 0.7, 0.7)\n",
    "config[\"post_fusion_dim\"] = hl\n",
    "config[\"batchsize\"] = hl\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.0005\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
