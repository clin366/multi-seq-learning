{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.3],\n",
       "       [14.3],\n",
       "       [14.3],\n",
       "       [ 9.7],\n",
       "       [ 9.7],\n",
       "       [ 9.7],\n",
       "       [ 7.4]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_observation_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.3],\n",
       "       [ nan],\n",
       "       [ nan],\n",
       "       [ 9.7],\n",
       "       [ nan],\n",
       "       [ nan],\n",
       "       [ 7.4]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [9.7],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [7.4],\n",
       "       [nan],\n",
       "       [nan]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control experiment using last observed value for missing data imputation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_dim, dropout):\n",
    "        \"\"\"\n",
    "        input_size - the number of expected features in the input x\n",
    "        hidden_size - the number of hidden units in state h\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.h = hidden_size\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (time_step, n_features)\n",
    "        \"\"\"\n",
    "        t = x.shape[0]\n",
    "        n = x.shape[1]\n",
    "        self.hx = torch.zeros(n, self.h)\n",
    "        self.cx = torch.zeros(n, self.h)\n",
    "        all_hs = []\n",
    "        all_cs = []\n",
    "        # iterate through cells \n",
    "        for i in range(t):\n",
    "            self.hx, self.cx = self.lstm(x[i], (self.hx, self.cx))\n",
    "            all_hs.append(self.hx)\n",
    "            all_cs.append(self.cx)\n",
    "        # last hidden layer last_hs is n * h\n",
    "        last_hs = all_hs[-1]\n",
    "        output = F.relu(self.fc1(last_hs))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "def train_lstm(X_train, y_train, X_valid, y_valid, X_test, y_test, config):\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "    def swap_axes(nparr):\n",
    "        return nparr.swapaxes(0,1)\n",
    "    X_train = swap_axes(X_train)\n",
    "    X_valid = swap_axes(X_valid)\n",
    "    X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters\n",
    "    input_size = X_train.shape[2]\n",
    "    h = config[\"h\"]\n",
    "    t = X_train.shape[0]\n",
    "    output_dim = 1\n",
    "    dropout = config[\"drop\"]\n",
    "    \n",
    "    model = LSTM(input_size, h, output_dim, dropout)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[1]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[:, start:end])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches \n",
    "    \n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    # timing\n",
    "#     start_time = time.time()\n",
    "#     predictions = predict(model, X_test)\n",
    "#     print(predictions.shape)\n",
    "#     print(predictions)\n",
    "#     end_time = time.time()\n",
    "#     print(end_time-start_time)\n",
    "#     assert False\n",
    "     \n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/lstm_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "    model = torch.load('models/lstm_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)\n",
    "#     corr = np.corrcoef(predictions,y_test)[0][1]\n",
    "#     print(\"corr: \", corr)\n",
    "#     true_label = (y_test >= 0)\n",
    "#     sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 142.87357802618118 111.47429656982422 saving model\n",
      "1 142.34687877836683 110.97308349609375 saving model\n",
      "2 141.76615397135416 110.4107666015625 saving model\n",
      "3 141.09373728434244 109.73616027832031 saving model\n",
      "4 140.27906181698754 108.87891387939453 saving model\n",
      "5 139.21773928687685 107.75979614257812 saving model\n",
      "6 137.77382478259858 106.08363342285156 saving model\n",
      "7 135.49786921909876 103.17031860351562 saving model\n",
      "8 131.27459099179222 97.47034454345703 saving model\n",
      "9 123.0447983514695 86.34819793701172 saving model\n",
      "10 107.89436521984283 67.90489959716797 saving model\n",
      "11 85.38680884951637 46.99235153198242 saving model\n",
      "12 63.24389203389486 31.502910614013672 saving model\n",
      "13 45.89570054553804 22.629810333251953 saving model\n",
      "14 36.13741179874965 18.74222183227539 saving model\n",
      "15 30.911359469095867 17.6124324798584 saving model\n",
      "16 28.188276835850306 17.65468406677246\n",
      "17 26.426051684788295 18.0338191986084\n",
      "18 26.41553565434047 18.366445541381836\n",
      "19 26.640444028945197 18.593599319458008\n",
      "20 26.938133421398344 18.681640625\n",
      "21 26.077002434503463 18.72795295715332\n",
      "22 26.315269288562593 18.76917839050293\n",
      "23 26.005496252150763 18.822322845458984\n",
      "24 26.30325798761277 18.851558685302734\n",
      "25 26.350298745291575 18.815107345581055\n",
      "Epoch    27: reducing learning rate of group 0 to 5.0000e-05.\n",
      "26 26.245274135044642 18.80156135559082\n",
      "27 25.891522680010116 18.787954330444336\n",
      "28 26.110540662493026 18.80799674987793\n",
      "29 26.040364946637833 18.841968536376953\n",
      "30 25.76162292843773 18.84723663330078\n",
      "31 26.547721499488468 18.82515525817871\n",
      "32 25.746790568033855 18.835844039916992\n",
      "33 26.275509788876487 18.83858871459961\n",
      "34 25.972048986525763 18.85178565979004\n",
      "35 25.972391219366166 18.837528228759766\n",
      "36 25.69975317092169 18.835311889648438\n",
      "Epoch    38: reducing learning rate of group 0 to 2.5000e-05.\n",
      "37 26.07852922167097 18.84005355834961\n",
      "38 26.088646934145974 18.840503692626953\n",
      "39 25.750659942626953 18.840898513793945\n",
      "40 26.28896054767427 18.860004425048828\n",
      "41 25.27906994592576 18.874162673950195\n",
      "42 25.87892282576788 18.877212524414062\n",
      "43 25.440721829732258 18.86991310119629\n",
      "44 26.4629062016805 18.872846603393555\n",
      "45 26.289589927310036 18.857540130615234\n",
      "46 26.005786259969074 18.858890533447266\n",
      "47 25.892920448666526 18.85947036743164\n",
      "Epoch    49: reducing learning rate of group 0 to 1.2500e-05.\n",
      "48 26.01624634152367 18.857810974121094\n",
      "49 25.47728220621745 18.862138748168945\n",
      "mae:  3.533576296971849\n",
      "mse:  17.73277031163212\n"
     ]
    }
   ],
   "source": [
    "config = {'h':64, 'lr':0.0001, 'num_epochs':50, 'batchsize':32, 'drop':0.2}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 143.43412453787667 112.07876586914062 saving model\n",
      "1 142.94505600702195 111.62542724609375 saving model\n",
      "2 142.4531231834775 111.1673812866211 saving model\n",
      "3 141.94688996814546 110.69254302978516 saving model\n",
      "4 141.41649482363746 110.18985748291016 saving model\n",
      "5 140.84891146705263 109.6414794921875 saving model\n",
      "6 140.23351542154947 109.06795501708984 saving model\n",
      "7 139.60275195893786 108.4738540649414 saving model\n",
      "8 138.90728142147972 107.7906494140625 saving model\n",
      "9 138.0716073172433 106.92293548583984 saving model\n",
      "10 136.97825513567244 105.73148345947266 saving model\n",
      "11 135.4612750098819 104.03018188476562 saving model\n",
      "12 133.2705096290225 101.46720886230469 saving model\n",
      "13 130.0471885317848 97.89236450195312 saving model\n",
      "14 125.5413803827195 92.89722442626953 saving model\n",
      "15 119.30120159330822 86.22345733642578 saving model\n",
      "16 111.24217551095145 78.21641540527344 saving model\n",
      "17 101.92916924612862 69.75833892822266 saving model\n",
      "18 92.32010741460891 61.698726654052734 saving model\n",
      "19 83.20459384009952 54.47452926635742 saving model\n",
      "20 74.9636584690639 48.18671798706055 saving model\n",
      "21 67.68599637349446 42.7866325378418 saving model\n",
      "22 61.32862908499582 38.181514739990234 saving model\n",
      "23 55.805513109479634 34.27492904663086 saving model\n",
      "24 51.023923510596866 30.97869110107422 saving model\n",
      "25 46.89636112394787 28.214698791503906 saving model\n",
      "26 43.34379550388881 25.913972854614258 saving model\n",
      "27 40.295900208609446 24.015527725219727 saving model\n",
      "28 37.6903353645688 22.4652042388916 saving model\n",
      "29 35.47175888788132 21.214818954467773 saving model\n",
      "30 33.59095446268717 20.22142791748047 saving model\n",
      "31 32.004096349080406 19.44672203063965 saving model\n",
      "32 30.67209089370001 18.856563568115234 saving model\n",
      "33 29.560060046968005 18.42061996459961 saving model\n",
      "34 28.636922609238397 18.11202621459961 saving model\n",
      "35 27.875044504801433 17.907161712646484 saving model\n",
      "36 27.249953360784623 17.78541374206543 saving model\n",
      "37 26.74011384873163 17.729001998901367 saving model\n",
      "38 26.32668445223854 17.722736358642578 saving model\n",
      "39 25.99332977476574 17.753847122192383\n",
      "40 25.72598425547282 17.81172752380371\n",
      "41 25.512665249052503 17.887752532958984\n",
      "42 25.343242236546107 17.97500991821289\n",
      "43 25.209242820739746 18.068073272705078\n",
      "44 25.103637422834122 18.162813186645508\n",
      "45 25.020646549406507 18.256160736083984\n",
      "46 24.9555630002703 18.345922470092773\n",
      "47 24.90458193279448 18.43062400817871\n",
      "48 24.864646957034157 18.50934600830078\n",
      "Epoch    50: reducing learning rate of group 0 to 5.0000e-05.\n",
      "49 24.83333297002883 18.58159065246582\n",
      "mae:  3.5876834175803443\n",
      "mse:  18.289545819615302\n"
     ]
    }
   ],
   "source": [
    "config = {'h':32, 'lr':0.0001, 'num_epochs':50, 'batchsize':32, 'drop':0.0}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 144.39079393659318 112.57483673095703 saving model\n",
      "1 143.39836956205824 111.5570068359375 saving model\n",
      "2 142.06606510707311 110.19802856445312 saving model\n",
      "3 140.3621092296782 108.07511138916016 saving model\n",
      "4 137.22879972912017 103.65140533447266 saving model\n",
      "5 129.07548159644716 90.4023666381836 saving model\n",
      "6 104.15223403204055 51.921749114990234 saving model\n",
      "7 58.56558318365188 19.55475425720215 saving model\n",
      "8 34.08881414504278 18.185840606689453 saving model\n",
      "9 29.982911200750443 18.70934295654297\n",
      "10 32.714118866693404 18.23284149169922\n",
      "11 30.1445411046346 17.961305618286133 saving model\n",
      "12 31.42852011181059 17.957353591918945 saving model\n",
      "13 30.38989580245245 18.134645462036133\n",
      "14 30.47426137470064 18.018064498901367\n",
      "15 29.4111457098098 18.10401725769043\n",
      "16 31.926031067257835 18.09047508239746\n",
      "17 29.365579514276412 18.22480010986328\n",
      "18 30.566774731590634 18.208377838134766\n",
      "19 28.990326018560502 18.11907196044922\n",
      "20 30.41956919715518 18.101903915405273\n",
      "21 29.685240972609748 18.31279945373535\n",
      "22 29.644873074122838 18.241302490234375\n",
      "Epoch    24: reducing learning rate of group 0 to 5.0000e-05.\n",
      "23 29.399210067022416 18.239755630493164\n",
      "24 29.21584837777274 18.248559951782227\n",
      "25 28.861175082978747 18.368318557739258\n",
      "26 28.85400236220587 18.463010787963867\n",
      "27 28.011560394650413 18.49148941040039\n",
      "28 30.103480202811106 18.537445068359375\n",
      "29 28.610805284409295 18.50565528869629\n",
      "30 28.389419056120374 18.508544921875\n",
      "31 30.060243424915132 18.548198699951172\n",
      "32 29.721255257016136 18.538908004760742\n",
      "33 26.607196035839262 18.6334285736084\n",
      "Epoch    35: reducing learning rate of group 0 to 2.5000e-05.\n",
      "34 28.032900083632697 18.649829864501953\n",
      "35 27.132590566362655 18.650907516479492\n",
      "36 29.165023849124 18.64756202697754\n",
      "37 28.084724789574032 18.65262222290039\n",
      "38 29.588111968267533 18.636167526245117\n",
      "39 28.287796837942942 18.625532150268555\n",
      "40 27.627485820225306 18.659202575683594\n",
      "41 28.421526091439382 18.715307235717773\n",
      "42 29.385702541896276 18.727767944335938\n",
      "43 29.608029274713424 18.769336700439453\n",
      "44 29.171139853341238 18.788349151611328\n",
      "Epoch    46: reducing learning rate of group 0 to 1.2500e-05.\n",
      "45 28.072829019455682 18.75119400024414\n",
      "46 29.719966888427734 18.74588394165039\n",
      "47 29.4350589570545 18.761756896972656\n",
      "48 28.753538585844495 18.76633644104004\n",
      "49 29.689652488345192 18.753175735473633\n",
      "mae:  3.7613045117086616\n",
      "mse:  19.959367612291906\n"
     ]
    }
   ],
   "source": [
    "config = {'h':128, 'lr':0.0001, 'num_epochs':50, 'batchsize':32, 'drop':0.7}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 142.89171346028647 111.48150634765625 saving model\n",
      "1 142.3996865408761 110.99354553222656 saving model\n",
      "2 141.8175266810826 110.44922637939453 saving model\n",
      "3 141.1881615774972 109.79469299316406 saving model\n",
      "4 140.42027864002046 108.98526763916016 saving model\n",
      "5 139.38258071172805 107.94928741455078 saving model\n",
      "6 138.0432891845703 106.4462661743164 saving model\n",
      "7 136.12773931594123 103.93438720703125 saving model\n",
      "8 132.54222942533949 99.1956787109375 saving model\n",
      "9 125.57864198230561 89.82416534423828 saving model\n",
      "10 112.49674388340541 73.36267852783203 saving model\n",
      "11 92.31576610746838 52.74055862426758 saving model\n",
      "12 70.4735354468936 35.652896881103516 saving model\n",
      "13 52.00707590012323 25.145174026489258 saving model\n",
      "14 40.096625736781526 20.009614944458008 saving model\n",
      "15 36.333033107575915 18.082090377807617 saving model\n",
      "16 31.548081761314755 17.58267593383789 saving model\n",
      "17 30.50357264564151 17.658864974975586\n",
      "18 30.03938184465681 17.91042709350586\n",
      "19 30.52170403798421 18.079391479492188\n",
      "20 30.858175186883834 18.12523651123047\n",
      "21 29.444756326221285 18.17877960205078\n",
      "22 29.841130302065896 18.257183074951172\n",
      "23 28.947618075779506 18.353181838989258\n",
      "24 28.82380721682594 18.429731369018555\n",
      "25 30.139989398774645 18.37154769897461\n",
      "26 29.436698504856654 18.337141036987305\n",
      "Epoch    28: reducing learning rate of group 0 to 5.0000e-05.\n",
      "27 28.974755695887975 18.29113006591797\n",
      "28 28.564815385001047 18.322025299072266\n",
      "29 29.856363932291668 18.3756046295166\n",
      "30 28.08708840324765 18.413179397583008\n",
      "31 29.321242968241375 18.407148361206055\n",
      "32 28.880183901105607 18.413785934448242\n",
      "33 29.506932031540643 18.412626266479492\n",
      "34 28.905521347409202 18.408905029296875\n",
      "35 28.99656563713437 18.38892936706543\n",
      "36 30.249334562392463 18.365713119506836\n",
      "37 29.216829617818195 18.36360740661621\n",
      "Epoch    39: reducing learning rate of group 0 to 2.5000e-05.\n",
      "38 27.793037596203032 18.351274490356445\n",
      "39 28.767093022664387 18.34974479675293\n",
      "40 28.266482943580264 18.38163185119629\n",
      "41 26.984396889096214 18.40363883972168\n",
      "42 30.547351700919016 18.416015625\n",
      "43 28.135153906685964 18.409080505371094\n",
      "44 30.043860526311967 18.42636489868164\n",
      "45 29.843419165838334 18.416484832763672\n",
      "46 28.490155628749303 18.416593551635742\n",
      "47 29.657277606782458 18.409212112426758\n",
      "48 29.378176325843448 18.410058975219727\n",
      "Epoch    50: reducing learning rate of group 0 to 1.2500e-05.\n",
      "49 28.677568117777508 18.42286491394043\n",
      "mae:  3.5611491006267957\n",
      "mse:  17.98590800406819\n"
     ]
    }
   ],
   "source": [
    "config = {'h':64, 'lr':0.0001, 'num_epochs':50, 'batchsize':32, 'drop':0.5}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 142.8925276256743 111.47087097167969 saving model\n",
      "1 142.3457725161598 110.96311950683594 saving model\n",
      "2 141.757325671968 110.38832092285156 saving model\n",
      "3 141.07856823149183 109.6970443725586 saving model\n",
      "4 140.239629473005 108.81123352050781 saving model\n",
      "5 139.15494065057663 107.64851379394531 saving model\n",
      "6 137.6426511492048 105.87740325927734 saving model\n",
      "7 135.18729872930618 102.74186706542969 saving model\n",
      "8 130.65362694149925 96.54956817626953 saving model\n",
      "9 121.77558026994977 84.55403900146484 saving model\n",
      "10 105.54756655011859 65.30238342285156 saving model\n",
      "11 82.60005478631882 44.67216110229492 saving model\n",
      "12 60.39279065813337 29.97088050842285 saving model\n",
      "13 44.318261419023784 21.850908279418945 saving model\n",
      "14 34.68320910135905 18.452030181884766 saving model\n",
      "15 29.637947037106468 17.56721305847168 saving model\n",
      "16 27.225909959702264 17.71426773071289\n",
      "17 26.13494346255348 18.11783218383789\n",
      "18 25.642766180492583 18.48337173461914\n",
      "19 25.402154241289413 18.735126495361328\n",
      "20 25.26978974115281 18.890493392944336\n",
      "21 25.18511136372884 18.98021697998047\n",
      "22 25.124088423592703 19.02967071533203\n",
      "23 25.076409612383163 19.05569839477539\n",
      "24 25.037197158450173 19.068513870239258\n",
      "25 25.003878457205637 19.074016571044922\n",
      "Epoch    27: reducing learning rate of group 0 to 5.0000e-05.\n",
      "26 24.97493471418108 19.075542449951172\n",
      "27 24.936605408078147 19.079816818237305\n",
      "28 24.92547116960798 19.085912704467773\n",
      "29 24.914130074637278 19.090848922729492\n",
      "30 24.903238796052477 19.09449577331543\n",
      "31 24.89284547170003 19.09708595275879\n",
      "32 24.88291835784912 19.098827362060547\n",
      "33 24.873425483703613 19.09990119934082\n",
      "34 24.864331109183176 19.100446701049805\n",
      "35 24.855609348842076 19.1005916595459\n",
      "36 24.847235588800338 19.10041046142578\n",
      "Epoch    38: reducing learning rate of group 0 to 2.5000e-05.\n",
      "37 24.83918267204648 19.099977493286133\n",
      "38 24.824239322117396 19.100839614868164\n",
      "39 24.820634206136067 19.102333068847656\n",
      "40 24.81688949040004 19.10370445251465\n",
      "41 24.813171659197128 19.104881286621094\n",
      "42 24.80950441814604 19.105873107910156\n",
      "43 24.80588168189639 19.106693267822266\n",
      "44 24.802309081667946 19.107362747192383\n",
      "45 24.798782575698127 19.107894897460938\n",
      "46 24.795302345639183 19.10829734802246\n",
      "47 24.7918674378168 19.108591079711914\n",
      "Epoch    49: reducing learning rate of group 0 to 1.2500e-05.\n",
      "48 24.788476943969727 19.10878562927246\n",
      "49 24.781327111380442 19.109146118164062\n",
      "mae:  3.558328313275802\n",
      "mse:  17.955803028807292\n"
     ]
    }
   ],
   "source": [
    "config = {'h':64, 'lr':0.0001, 'num_epochs':50, 'batchsize':32, 'drop':0.0}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 144.3682334536598 112.52589416503906 saving model\n",
      "1 143.26119595482237 111.4061279296875 saving model\n",
      "2 141.8498996552967 109.84474182128906 saving model\n",
      "3 139.6847679501488 107.14594268798828 saving model\n",
      "4 135.3698225475493 100.72005462646484 saving model\n",
      "5 123.05448986235119 79.72200775146484 saving model\n",
      "6 87.02027366274879 34.20840835571289 saving model\n",
      "7 41.48923669542585 17.409717559814453 saving model\n",
      "8 27.530917213076638 19.884645462036133\n",
      "9 25.840191977364675 19.44789695739746\n",
      "10 25.427451587858656 18.974510192871094\n",
      "11 25.319975807553245 18.907392501831055\n",
      "12 25.20515977768671 18.928972244262695\n",
      "13 25.093356768290203 18.93532943725586\n",
      "14 25.002363068716868 18.930465698242188\n",
      "15 24.928135009039018 18.9260196685791\n",
      "16 24.864187558492024 18.924219131469727\n",
      "17 24.806933902558825 18.924285888671875\n",
      "Epoch    19: reducing learning rate of group 0 to 5.0000e-05.\n",
      "18 24.754547028314498 18.925783157348633\n",
      "19 24.674888656252907 18.95649528503418\n",
      "20 24.652789479210263 18.992366790771484\n",
      "21 24.62577283950079 19.012248992919922\n",
      "22 24.600630306062243 19.021671295166016\n",
      "23 24.57727795555478 19.02663230895996\n",
      "24 24.554966563270206 19.0302791595459\n",
      "25 24.533221653529576 19.0339412689209\n",
      "26 24.511824698675248 19.038089752197266\n",
      "27 24.49069681621733 19.042917251586914\n",
      "28 24.469810712905158 19.048492431640625\n",
      "Epoch    30: reducing learning rate of group 0 to 2.5000e-05.\n",
      "29 24.449170839218866 19.05488395690918\n",
      "30 24.411789394560316 19.066560745239258\n",
      "31 24.402456374395463 19.0815372467041\n",
      "32 24.39160714830671 19.09392738342285\n",
      "33 24.380855196998233 19.10382843017578\n",
      "34 24.37035406203497 19.112030029296875\n",
      "35 24.360070137750533 19.119159698486328\n",
      "36 24.349957239060174 19.12567901611328\n",
      "37 24.33998707362584 19.131893157958984\n",
      "38 24.330143973940896 19.138011932373047\n",
      "39 24.32042035602388 19.144166946411133\n",
      "Epoch    41: reducing learning rate of group 0 to 1.2500e-05.\n",
      "40 24.310816129048664 19.15045928955078\n",
      "41 24.29197338649205 19.155941009521484\n",
      "42 24.287559145972843 19.162626266479492\n",
      "43 24.28275003887358 19.168970108032227\n",
      "44 24.277933620271227 19.17486000061035\n",
      "45 24.273163159688313 19.180370330810547\n",
      "46 24.268436976841517 19.18557357788086\n",
      "47 24.263756706601097 19.19054412841797\n",
      "48 24.259117308117094 19.195327758789062\n",
      "49 24.254520870390394 19.199981689453125\n",
      "mae:  3.6825685374992942\n",
      "mse:  19.03583056731524\n"
     ]
    }
   ],
   "source": [
    "config = {'h':128, 'lr':0.0001, 'num_epochs':50, 'batchsize':32, 'drop':0.7}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 144.36276354108537 112.53226470947266 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 143.28201184953963 111.42823791503906 saving model\n",
      "2 141.90025765555245 109.90135955810547 saving model\n",
      "3 139.80965241931733 107.2958984375 saving model\n",
      "4 135.58907717750185 101.19510650634766 saving model\n",
      "5 124.14143480573382 81.43133544921875 saving model\n",
      "6 89.72457522437686 36.430763244628906 saving model\n",
      "7 43.93294266292027 17.380149841308594 saving model\n",
      "8 28.500378336225236 19.599977493286133\n",
      "9 26.739432244073775 19.336875915527344\n",
      "10 25.75014064425514 18.891464233398438\n",
      "11 25.6573364621117 18.85200309753418\n",
      "12 26.068444297427224 18.799102783203125\n",
      "13 25.555141040257045 18.78846549987793\n",
      "14 25.792273203531902 18.735254287719727\n",
      "15 25.41020375206357 18.80600929260254\n",
      "16 25.3397159576416 18.889617919921875\n",
      "17 25.647798946925572 18.902498245239258\n",
      "Epoch    19: reducing learning rate of group 0 to 5.0000e-05.\n",
      "18 25.452332996186755 18.81488800048828\n",
      "19 24.807841800508044 18.847740173339844\n",
      "20 25.465374719528924 18.854116439819336\n",
      "21 24.91729495638893 18.908771514892578\n",
      "22 25.40786379859561 18.958858489990234\n",
      "23 25.44126456124442 18.975814819335938\n",
      "24 25.63830852508545 18.95440101623535\n",
      "25 24.909565971011208 18.968482971191406\n",
      "26 25.373504457019624 18.98737907409668\n",
      "27 24.84309723263695 18.976463317871094\n",
      "28 25.485645294189453 19.001386642456055\n",
      "Epoch    30: reducing learning rate of group 0 to 2.5000e-05.\n",
      "29 24.41067850022089 19.05009651184082\n",
      "30 25.55655134291876 19.07176971435547\n",
      "31 25.51866881052653 19.054588317871094\n",
      "32 25.612730662027996 19.04340934753418\n",
      "33 24.42374887920561 19.047029495239258\n",
      "34 25.0275544211978 19.07040786743164\n",
      "35 25.01173073904855 19.070255279541016\n",
      "36 25.31161644345238 19.058427810668945\n",
      "37 24.9454406556629 19.017051696777344\n",
      "38 24.530264718191965 19.014480590820312\n",
      "39 25.79855001540411 19.021543502807617\n",
      "Epoch    41: reducing learning rate of group 0 to 1.2500e-05.\n",
      "40 24.97695100875128 19.02788734436035\n",
      "41 24.784542992001487 19.05744171142578\n",
      "42 25.136372611636208 19.06869125366211\n",
      "43 24.932086354210263 19.081836700439453\n",
      "44 24.828488576979865 19.085857391357422\n",
      "45 24.695806457882835 19.0888729095459\n",
      "46 25.12298020862398 19.090551376342773\n",
      "47 25.2537777310326 19.10160255432129\n",
      "48 24.406858080909366 19.121713638305664\n",
      "49 24.53383631933303 19.124191284179688\n",
      "mae:  3.6235372038912184\n",
      "mse:  18.498219889810887\n"
     ]
    }
   ],
   "source": [
    "config = {'h':128, 'lr':0.0001, 'num_epochs':50, 'batchsize':32, 'drop':0.2}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[        nan],\n",
       "       [-0.24856643],\n",
       "       [        nan],\n",
       "       [        nan],\n",
       "       [-0.72005529],\n",
       "       [        nan],\n",
       "       [        nan]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21854222],\n",
       "       [-0.24856643],\n",
       "       [-0.71349436],\n",
       "       [-0.53756466],\n",
       "       [-0.72005529],\n",
       "       [-2.0939877 ],\n",
       "       [-1.21070575]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, fea_dev, fea_test = normalize_feature(fea_train, fea_dev, fea_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean_after_norm in train set \n",
    "mean_after_norm = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m         = np.nanmean(X_train, axis=0)\n",
    "s         = np.nanstd(X_train, axis=0)\n",
    "X_train_z = (X_train - m)/s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fea_seq[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
