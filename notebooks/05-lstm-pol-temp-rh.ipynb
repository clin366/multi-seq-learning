{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/pol_temp_rh'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control experiment using last observed value for missing data imputation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_dim, dropout):\n",
    "        \"\"\"\n",
    "        input_size - the number of expected features in the input x\n",
    "        hidden_size - the number of hidden units in state h\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.h = hidden_size\n",
    "        self.lstm = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (time_step, n_features)\n",
    "        \"\"\"\n",
    "        t = x.shape[0]\n",
    "        n = x.shape[1]\n",
    "        self.hx = torch.zeros(n, self.h)\n",
    "        self.cx = torch.zeros(n, self.h)\n",
    "        all_hs = []\n",
    "        all_cs = []\n",
    "        # iterate through cells \n",
    "        for i in range(t):\n",
    "            self.hx, self.cx = self.lstm(x[i], (self.hx, self.cx))\n",
    "            all_hs.append(self.hx)\n",
    "            all_cs.append(self.cx)\n",
    "        # last hidden layer last_hs is n * h\n",
    "        last_hs = all_hs[-1]\n",
    "        output = F.relu(self.fc1(last_hs))\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "    \n",
    "def train_lstm(X_train, y_train, X_valid, y_valid, X_test, y_test, config):\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "    def swap_axes(nparr):\n",
    "        return nparr.swapaxes(0,1)\n",
    "    X_train = swap_axes(X_train)\n",
    "    X_valid = swap_axes(X_valid)\n",
    "    X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters\n",
    "    input_size = X_train.shape[2]\n",
    "    h = config[\"h\"]\n",
    "    t = X_train.shape[0]\n",
    "    output_dim = 1\n",
    "    dropout = config[\"drop\"]\n",
    "    \n",
    "    model = LSTM(input_size, h, output_dim, dropout)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[1]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[:, start:end])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches \n",
    "    \n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    # timing\n",
    "#     start_time = time.time()\n",
    "#     predictions = predict(model, X_test)\n",
    "#     print(predictions.shape)\n",
    "#     print(predictions)\n",
    "#     end_time = time.time()\n",
    "#     print(end_time-start_time)\n",
    "#     assert False\n",
    "     \n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/lstm_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "    model = torch.load('models/lstm_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)\n",
    "#     corr = np.corrcoef(predictions,y_test)[0][1]\n",
    "#     print(\"corr: \", corr)\n",
    "#     true_label = (y_test >= 0)\n",
    "#     sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 148.51093546549478 116.60440063476562 saving model\n",
      "1 148.29742358979723 116.32820129394531 saving model\n",
      "2 147.93506440662202 116.04935455322266 saving model\n",
      "3 147.6263173421224 115.76716613769531 saving model\n",
      "4 147.3431425548735 115.47895050048828 saving model\n",
      "5 147.0048344930013 115.17292022705078 saving model\n",
      "6 146.71435219900948 114.84748077392578 saving model\n",
      "7 146.2334703717913 114.49068450927734 saving model\n",
      "8 145.88888004847936 114.0997314453125 saving model\n",
      "9 145.56317283993675 113.64879608154297 saving model\n",
      "10 144.92964317685082 113.11221313476562 saving model\n",
      "11 144.3488039289202 112.47896575927734 saving model\n",
      "12 143.73105802990142 111.7404556274414 saving model\n",
      "13 143.0209448678153 110.87417602539062 saving model\n",
      "14 141.92852892194475 109.79824829101562 saving model\n",
      "15 140.88487352643693 108.43879699707031 saving model\n",
      "16 139.21480996268136 106.72991180419922 saving model\n",
      "17 137.60929470970518 104.52569580078125 saving model\n",
      "18 135.1795370919364 101.53296661376953 saving model\n",
      "19 131.7304175240653 97.4034194946289 saving model\n",
      "20 127.64450218563988 91.8545913696289 saving model\n",
      "21 121.47214253743489 84.94231414794922 saving model\n",
      "22 114.67541740054176 77.09312438964844 saving model\n",
      "23 105.45605777558826 68.63016510009766 saving model\n",
      "24 95.6661878313337 60.24098205566406 saving model\n",
      "25 87.24634588332404 52.52318572998047 saving model\n",
      "26 77.257262638637 45.68603515625 saving model\n",
      "27 71.17737661089215 39.96129608154297 saving model\n",
      "28 63.43642507280622 35.19750213623047 saving model\n",
      "29 58.23645410083589 31.35171890258789 saving model\n",
      "30 54.43607757205055 28.326108932495117 saving model\n",
      "31 49.624113991147 25.864200592041016 saving model\n",
      "32 44.90098508199056 23.879514694213867 saving model\n",
      "33 42.18792452130999 22.284568786621094 saving model\n",
      "34 44.158931732177734 21.04601287841797 saving model\n",
      "35 39.23444112141927 20.13750457763672 saving model\n",
      "36 37.04251552763439 19.40327262878418 saving model\n",
      "37 37.373644965035574 18.84479331970215 saving model\n",
      "38 35.50823783874512 18.454551696777344 saving model\n",
      "39 35.98739714849563 18.163990020751953 saving model\n",
      "40 37.96875817435129 17.97435188293457 saving model\n",
      "41 33.45204144432431 17.854827880859375 saving model\n",
      "42 35.03678203764416 17.7879695892334 saving model\n",
      "43 35.975681486583895 17.763553619384766 saving model\n",
      "44 36.144369397844585 17.754316329956055 saving model\n",
      "45 35.186628704979306 17.750587463378906 saving model\n",
      "46 33.88705825805664 17.759326934814453\n",
      "47 33.94655600048247 17.775617599487305\n",
      "48 34.18241069430397 17.794923782348633\n",
      "49 34.020570618765696 17.812721252441406\n",
      "mae:  3.5909030803964157\n",
      "mse:  18.247215584844646\n"
     ]
    }
   ],
   "source": [
    "config = {'h':32, 'lr':0.0001, 'num_epochs':50, 'batchsize':32, 'drop':0.5}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 148.5558355422247 116.59574127197266 saving model\n",
      "1 148.21519942510696 116.31136322021484 saving model\n",
      "2 147.90960003080824 116.02476501464844 saving model\n",
      "3 147.5915716262091 115.7329330444336 saving model\n",
      "4 147.309083484468 115.42832946777344 saving model\n",
      "5 146.9377688453311 115.10294342041016 saving model\n",
      "6 146.59620848156158 114.75729370117188 saving model\n",
      "7 146.1917495727539 114.37591552734375 saving model\n",
      "8 145.75304049537294 113.94268035888672 saving model\n",
      "9 145.31788925897507 113.43444061279297 saving model\n",
      "10 144.7667995634533 112.83334350585938 saving model\n",
      "11 144.1587346394857 112.13513946533203 saving model\n",
      "12 143.35314360119048 111.29439544677734 saving model\n",
      "13 142.49611009870256 110.27655029296875 saving model\n",
      "14 141.31615193684897 109.0082778930664 saving model\n",
      "15 140.0712429228283 107.42472076416016 saving model\n",
      "16 138.28850846063523 105.38589477539062 saving model\n",
      "17 136.15179298037575 102.67196655273438 saving model\n",
      "18 133.11468178885323 98.93394470214844 saving model\n",
      "19 128.99281855991907 93.81553649902344 saving model\n",
      "20 123.4830923534575 87.22886657714844 saving model\n",
      "21 116.32693590436664 79.51065063476562 saving model\n",
      "22 107.97839664277576 71.11361694335938 saving model\n",
      "23 98.2638190133231 62.61460494995117 saving model\n",
      "24 88.52336265927269 54.56159973144531 saving model\n",
      "25 80.01879791986374 47.40982437133789 saving model\n",
      "26 70.51850773039318 41.28013610839844 saving model\n",
      "27 63.225544974917455 36.085601806640625 saving model\n",
      "28 56.44010789053781 31.801509857177734 saving model\n",
      "29 50.84010841732933 28.34876823425293 saving model\n",
      "30 46.87638033004034 25.62332534790039 saving model\n",
      "31 43.29049700782413 23.479341506958008 saving model\n",
      "32 39.33300981067476 21.796396255493164 saving model\n",
      "33 36.47107751028879 20.5126953125 saving model\n",
      "34 36.58096086411249 19.555200576782227 saving model\n",
      "35 32.88134629385812 18.861526489257812 saving model\n",
      "36 31.677963574727375 18.369504928588867 saving model\n",
      "37 31.89380745660691 18.04698371887207 saving model\n",
      "38 29.956957998729887 17.860132217407227 saving model\n",
      "39 30.03520765758696 17.77091407775879 saving model\n",
      "40 28.99836540222168 17.74505043029785 saving model\n",
      "41 29.06191598801386 17.76787567138672\n",
      "42 29.038218044099352 17.823854446411133\n",
      "43 28.189202808198473 17.89373207092285\n",
      "44 28.005430630275182 17.96715545654297\n",
      "45 27.150846890040807 18.052104949951172\n",
      "46 27.20267472948347 18.139896392822266\n",
      "47 28.19670867919922 18.226301193237305\n",
      "48 27.007130668276833 18.284822463989258\n",
      "49 27.21617839449928 18.34246063232422\n",
      "mae:  3.593312818353826\n",
      "mse:  18.258901629012314\n"
     ]
    }
   ],
   "source": [
    "config = {'h':32, 'lr':0.0001, 'num_epochs':50, 'batchsize':32, 'drop':0.2}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 145.86148470924013 113.95573425292969 saving model\n",
      "1 145.37368956066314 113.41845703125 saving model\n",
      "2 144.76148441859655 112.84800720214844 saving model\n",
      "3 144.02312869117372 112.22523498535156 saving model\n",
      "4 143.5881878080822 111.51858520507812 saving model\n",
      "5 142.65486689976282 110.6669692993164 saving model\n",
      "6 141.84251912434897 109.58648681640625 saving model\n",
      "7 140.47974504743303 108.09255981445312 saving model\n",
      "8 138.62728046235583 105.80789184570312 saving model\n",
      "9 135.8606422061012 102.10462188720703 saving model\n",
      "10 131.5003851027716 95.84728240966797 saving model\n",
      "11 123.5914768037342 85.19403076171875 saving model\n",
      "12 109.90644491286506 69.47418975830078 saving model\n",
      "13 92.20011175246466 51.453765869140625 saving model\n",
      "14 72.81600225539435 35.94103240966797 saving model\n",
      "15 56.37120810009184 25.953584671020508 saving model\n",
      "16 46.952756518409366 20.673221588134766 saving model\n",
      "17 41.3256456284296 18.431781768798828 saving model\n",
      "18 39.91825566973005 17.769189834594727 saving model\n",
      "19 34.58584449404762 17.71009635925293 saving model\n",
      "20 35.701925186883834 17.777116775512695\n",
      "21 33.27239304497128 17.85004234313965\n",
      "22 34.41054026285807 17.8839111328125\n",
      "23 33.41301754542759 17.920047760009766\n",
      "24 34.327637717837376 17.93276596069336\n",
      "25 35.69115466163272 17.933015823364258\n",
      "26 34.49920531681606 17.932706832885742\n",
      "27 32.247615360078356 17.95567512512207\n",
      "28 33.706601006644114 17.97934913635254\n",
      "29 34.95437286013649 17.991992950439453\n",
      "Epoch    31: reducing learning rate of group 0 to 5.0000e-05.\n",
      "30 32.00371696835472 18.036043167114258\n",
      "31 32.91344270252046 18.02099609375\n",
      "32 33.91020711263021 18.032350540161133\n",
      "33 35.71268435886928 18.00695037841797\n",
      "34 34.8243537176223 17.986661911010742\n",
      "35 31.8731043225243 18.001461029052734\n",
      "36 32.9875796181815 18.020658493041992\n",
      "37 32.67007864089239 18.009336471557617\n",
      "38 32.41550804319836 18.0012149810791\n",
      "39 33.005225408644904 17.99385643005371\n",
      "40 33.61125500996908 17.95374870300293\n",
      "Epoch    42: reducing learning rate of group 0 to 2.5000e-05.\n",
      "41 33.371256873721165 17.95897102355957\n",
      "42 35.613326299758185 17.965885162353516\n",
      "43 35.95340583437965 17.955318450927734\n",
      "44 31.370181492396764 17.95907211303711\n",
      "45 36.461242403302876 17.95131492614746\n",
      "46 32.63057963053385 17.94278907775879\n",
      "47 34.69248267582485 17.944869995117188\n",
      "48 33.08033756982712 17.936304092407227\n",
      "49 34.60971532549177 17.944108963012695\n",
      "mae:  3.658305367556485\n",
      "mse:  18.72777492975782\n"
     ]
    }
   ],
   "source": [
    "config = {'h':64, 'lr':0.0001, 'num_epochs':50, 'batchsize':32, 'drop':0.7}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 145.8687250046503 113.93003845214844 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 145.2564250401088 113.3586196899414 saving model\n",
      "2 144.67695762997582 112.74771118164062 saving model\n",
      "3 143.9765813918341 112.06239318847656 saving model\n",
      "4 143.2735868181501 111.26143646240234 saving model\n",
      "5 142.30578104654947 110.25984954833984 saving model\n",
      "6 141.22912379673548 108.90699005126953 saving model\n",
      "7 139.58508882068452 106.89680480957031 saving model\n",
      "8 137.15960729689826 103.6801986694336 saving model\n",
      "9 133.09564862932478 98.20018768310547 saving model\n",
      "10 125.97628566196987 88.63983154296875 saving model\n",
      "11 113.95888991582962 73.68605041503906 saving model\n",
      "12 96.03153610229492 55.171356201171875 saving model\n",
      "13 74.71212423415412 38.2042121887207 saving model\n",
      "14 55.68081038338797 26.46379280090332 saving model\n",
      "15 42.18146242414202 20.259355545043945 saving model\n",
      "16 34.25321951366606 18.00419044494629 saving model\n",
      "17 30.38073171888079 17.69477081298828 saving model\n",
      "18 28.301763307480584 18.02503776550293\n",
      "19 27.380786214556014 18.455280303955078\n",
      "20 26.239308947608585 18.75347328186035\n",
      "21 26.224538984752837 18.971473693847656\n",
      "22 26.48231551760719 19.064212799072266\n",
      "23 26.47852834065755 19.112346649169922\n",
      "24 26.079340571448917 19.099407196044922\n",
      "25 26.502458754039946 19.11887550354004\n",
      "26 26.163097654070175 19.086509704589844\n",
      "27 25.71325479234968 19.10115623474121\n",
      "Epoch    29: reducing learning rate of group 0 to 5.0000e-05.\n",
      "28 25.383718263535272 19.0825252532959\n",
      "29 26.263402348472958 19.074960708618164\n",
      "30 26.21966316586449 19.076004028320312\n",
      "31 25.446334929693315 19.077077865600586\n",
      "32 25.81830070132301 19.100847244262695\n",
      "33 25.535495621817454 19.117155075073242\n",
      "34 25.985354696001327 19.124019622802734\n",
      "35 25.866367294674827 19.131710052490234\n",
      "36 25.680497941516695 19.135944366455078\n",
      "37 25.879937898545037 19.108610153198242\n",
      "38 26.381238347008114 19.081642150878906\n",
      "Epoch    40: reducing learning rate of group 0 to 2.5000e-05.\n",
      "39 26.03518772125244 19.07384490966797\n",
      "40 25.599763824826194 19.05744743347168\n",
      "41 25.905641147068568 19.06879425048828\n",
      "42 25.94308598836263 19.079265594482422\n",
      "43 26.156055314200266 19.074525833129883\n",
      "44 25.88981701078869 19.067285537719727\n",
      "45 26.826666332426527 19.053672790527344\n",
      "46 25.7825798761277 19.03799819946289\n",
      "47 25.547855013892764 19.048274993896484\n",
      "48 25.475420543125697 19.040374755859375\n",
      "49 25.82800088609968 19.04644012451172\n",
      "mae:  3.687686536331807\n",
      "mse:  18.96986845432093\n"
     ]
    }
   ],
   "source": [
    "config = {'h':64, 'lr':0.0001, 'num_epochs':50, 'batchsize':32, 'drop':0.2}\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_lstm(last_train, label_train, last_dev, label_dev, last_test, label_test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14.3      ,  90.       ,  62.8125   ,  64.       ,  51.5416665],\n",
       "       [ 14.3      ,  83.       ,  68.       ,  70.       ,  57.75     ],\n",
       "       [ 14.3      ,  98.       ,  73.875    ,  67.       ,  58.       ],\n",
       "       [  9.7      , 100.       ,  68.7083335,  67.       ,  49.416667 ],\n",
       "       [  9.7      ,  98.       ,  71.6041665,  75.       ,  60.645833 ],\n",
       "       [  9.7      ,  99.       ,  78.1875   ,  74.       ,  64.5833335],\n",
       "       [  7.4      ,  99.       ,  71.2083335,  65.       ,  53.4375   ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_observation_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14.3      ,  90.       ,  62.8125   ,  64.       ,  51.5416665],\n",
       "       [        nan,  83.       ,  68.       ,  70.       ,  57.75     ],\n",
       "       [        nan,  98.       ,  73.875    ,  67.       ,  58.       ],\n",
       "       [  9.7      , 100.       ,  68.7083335,  67.       ,  49.416667 ],\n",
       "       [        nan,  98.       ,  71.6041665,  75.       ,  60.645833 ],\n",
       "       [        nan,  99.       ,  78.1875   ,  74.       ,  64.5833335],\n",
       "       [  7.4      ,  99.       ,  71.2083335,  65.       ,  53.4375   ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
