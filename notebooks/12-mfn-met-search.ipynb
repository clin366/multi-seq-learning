{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/met-search'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control experiment using last observed value for missing data imputation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFN(nn.Module):\n",
    "    def __init__(self,config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig):\n",
    "        super(MFN, self).__init__()\n",
    "        [self.d_l,self.d_a] = config[\"input_dims\"]\n",
    "        [self.dh_l,self.dh_a] = config[\"h_dims\"]\n",
    "        total_h_dim = self.dh_l+self.dh_a\n",
    "        \n",
    "        self.mem_dim = config[\"memsize\"]\n",
    "        window_dim = config[\"windowsize\"]\n",
    "        output_dim = 1\n",
    "        attInShape = total_h_dim*window_dim\n",
    "        gammaInShape = attInShape+self.mem_dim\n",
    "        final_out = total_h_dim+self.mem_dim\n",
    "        h_att1 = NN1Config[\"shapes\"]\n",
    "        h_att2 = NN2Config[\"shapes\"]\n",
    "        h_gamma1 = gamma1Config[\"shapes\"]\n",
    "        h_gamma2 = gamma2Config[\"shapes\"]\n",
    "        h_out = outConfig[\"shapes\"]\n",
    "        att1_dropout = NN1Config[\"drop\"]\n",
    "        att2_dropout = NN2Config[\"drop\"]\n",
    "        gamma1_dropout = gamma1Config[\"drop\"]\n",
    "        gamma2_dropout = gamma2Config[\"drop\"]\n",
    "        out_dropout = outConfig[\"drop\"]\n",
    "\n",
    "        self.lstm_l = nn.LSTMCell(self.d_l, self.dh_l)\n",
    "        self.lstm_a = nn.LSTMCell(self.d_a, self.dh_a)\n",
    "\n",
    "        self.att1_fc1 = nn.Linear(attInShape, h_att1)\n",
    "        self.att1_fc2 = nn.Linear(h_att1, attInShape)\n",
    "        self.att1_dropout = nn.Dropout(att1_dropout)\n",
    "\n",
    "        self.att2_fc1 = nn.Linear(attInShape, h_att2)\n",
    "        self.att2_fc2 = nn.Linear(h_att2, self.mem_dim)\n",
    "        self.att2_dropout = nn.Dropout(att2_dropout)\n",
    "\n",
    "        self.gamma1_fc1 = nn.Linear(gammaInShape, h_gamma1)\n",
    "        self.gamma1_fc2 = nn.Linear(h_gamma1, self.mem_dim)\n",
    "        self.gamma1_dropout = nn.Dropout(gamma1_dropout)\n",
    "\n",
    "        self.gamma2_fc1 = nn.Linear(gammaInShape, h_gamma2)\n",
    "        self.gamma2_fc2 = nn.Linear(h_gamma2, self.mem_dim)\n",
    "        self.gamma2_dropout = nn.Dropout(gamma2_dropout)\n",
    "\n",
    "        self.out_fc1 = nn.Linear(final_out, h_out)\n",
    "        self.out_fc2 = nn.Linear(h_out, output_dim)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_l = x[:,:,:self.d_l]\n",
    "        x_a = x[:,:,self.d_l:self.d_l+self.d_a]\n",
    "        # x is t x n x d\n",
    "        n = x.shape[1]\n",
    "        t = x.shape[0]\n",
    "        self.h_l = torch.zeros(n, self.dh_l)\n",
    "        self.h_a = torch.zeros(n, self.dh_a)\n",
    "\n",
    "        self.c_l = torch.zeros(n, self.dh_l)\n",
    "        self.c_a = torch.zeros(n, self.dh_a)\n",
    "        \n",
    "        self.mem = torch.zeros(n, self.mem_dim)\n",
    "        all_h_ls = []\n",
    "        all_h_as = []\n",
    "\n",
    "        all_c_ls = []\n",
    "        all_c_as = []\n",
    "\n",
    "        all_mems = []\n",
    "        for i in range(t):\n",
    "            # prev time step\n",
    "            prev_c_l = self.c_l\n",
    "            prev_c_a = self.c_a\n",
    "\n",
    "            # curr time step\n",
    "            new_h_l, new_c_l = self.lstm_l(x_l[i], (self.h_l, self.c_l))\n",
    "            new_h_a, new_c_a = self.lstm_a(x_a[i], (self.h_a, self.c_a))\n",
    "   \n",
    "            # concatenate\n",
    "            prev_cs = torch.cat([prev_c_l,prev_c_a], dim=1)\n",
    "            new_cs = torch.cat([new_c_l,new_c_a], dim=1)\n",
    "            \n",
    "            cStar = torch.cat([prev_cs,new_cs], dim=1)\n",
    "            attention = F.softmax(self.att1_fc2(self.att1_dropout(F.relu(self.att1_fc1(cStar)))),dim=1)\n",
    "            attended = attention*cStar\n",
    "            \n",
    "            cHat = F.tanh(self.att2_fc2(self.att2_dropout(F.relu(self.att2_fc1(attended)))))\n",
    "            \n",
    "            both = torch.cat([attended,self.mem], dim=1)\n",
    "            gamma1 = F.sigmoid(self.gamma1_fc2(self.gamma1_dropout(F.relu(self.gamma1_fc1(both)))))\n",
    "            gamma2 = F.sigmoid(self.gamma2_fc2(self.gamma2_dropout(F.relu(self.gamma2_fc1(both)))))\n",
    "            \n",
    "            self.mem = gamma1*self.mem + gamma2*cHat\n",
    "            all_mems.append(self.mem)\n",
    "            # update\n",
    "            self.h_l, self.c_l = new_h_l, new_c_l\n",
    "            self.h_a, self.c_a = new_h_a, new_c_a\n",
    "\n",
    "            all_h_ls.append(self.h_l)\n",
    "            all_h_as.append(self.h_a)\n",
    " \n",
    "            all_c_ls.append(self.c_l)\n",
    "            all_c_as.append(self.c_a)\n",
    "\n",
    "        # last hidden layer last_hs is n x h\n",
    "        last_h_l = all_h_ls[-1]\n",
    "        last_h_a = all_h_as[-1]\n",
    "\n",
    "        last_mem = all_mems[-1]\n",
    "        last_hs = torch.cat([last_h_l,last_h_a,last_mem], dim=1)\n",
    "        output = self.out_fc2(self.out_dropout(F.relu(self.out_fc1(last_hs))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mfn(X_train, y_train, X_valid, y_valid, X_test, y_test, configs):\n",
    "#     p = np.random.permutation(X_train.shape[0])\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "    def swap_axes(nparr):\n",
    "        return nparr.swapaxes(0,1)\n",
    "    X_train = swap_axes(X_train)\n",
    "    X_valid = swap_axes(X_valid)\n",
    "    X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters \n",
    "    input_size = X_train.shape[2]\n",
    "    h = 128\n",
    "    t = X_train.shape[0]\n",
    "    output_dim = 1\n",
    "    dropout = 0.5\n",
    "\n",
    "#     d = X_train.shape[2]\n",
    "#     h = 128\n",
    "#     t = X_train.shape[0]\n",
    "#     output_dim = 1\n",
    "#     dropout = 0.5\n",
    "\n",
    "    [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig] = configs\n",
    "\n",
    "    \n",
    "    #model = EFLSTM(d,h,output_dim,dropout)\n",
    "    model = MFN(config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "    #optimizer = optim.SGD(model.parameters(),lr=config[\"lr\"],momentum=config[\"momentum\"])\n",
    "\n",
    "    # optimizer = optim.SGD([\n",
    "    #                 {'params':model.lstm_l.parameters(), 'lr':config[\"lr\"]},\n",
    "    #                 {'params':model.classifier.parameters(), 'lr':config[\"lr\"]}\n",
    "    #             ], momentum=0.9)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "#     criterion = nn.L1Loss()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer,mode='min',patience=100,factor=0.5,verbose=True)\n",
    "\n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[1]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[:,start:end])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss test_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        test_loss = evaluate(model, X_test, y_test, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, test_loss, 'saving model')\n",
    "            torch.save(model, 'models/temp_models/mfn_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss, test_loss)\n",
    "\n",
    "#     print 'model number is:', rand\n",
    "    model = torch.load('models/temp_models/mfn_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss test_loss\n",
      "0 140.88166046142578 111.26560974121094 89.40806579589844 saving model\n",
      "1 136.88693491617838 107.45386505126953 86.0100326538086 saving model\n",
      "2 132.5866724650065 102.74201965332031 81.72261047363281 saving model\n",
      "3 127.05015055338542 96.2884521484375 75.81144714355469 saving model\n",
      "4 118.77826182047527 86.91610717773438 67.29901123046875 saving model\n",
      "5 107.15186564127605 72.70096588134766 54.58787536621094 saving model\n",
      "6 89.29688262939453 51.30961227416992 36.29313659667969 saving model\n",
      "7 62.16184743245443 25.88995933532715 18.863813400268555 saving model\n",
      "8 37.9138126373291 26.237396240234375 37.65473937988281\n",
      "9 39.766815185546875 36.205902099609375 54.5194206237793\n",
      "10 35.47192128499349 21.870532989501953 29.35205841064453 saving model\n",
      "11 26.16613833109538 18.805349349975586 18.300262451171875 saving model\n",
      "12 26.37489954630534 20.234281539916992 16.827665328979492\n",
      "13 28.813406626383465 19.88355255126953 16.761201858520508\n",
      "14 28.116995493570965 18.765304565429688 17.762672424316406 saving model\n",
      "15 25.777580897013348 19.043792724609375 20.875072479248047\n",
      "16 26.626960118611652 20.130741119384766 23.509479522705078\n",
      "17 24.77455012003581 19.865619659423828 22.233623504638672\n",
      "18 25.07058588663737 19.240266799926758 19.28412437438965\n",
      "19 24.890544891357422 19.42141342163086 17.580188751220703\n",
      "20 24.599183400472004 19.665796279907227 17.2666015625\n",
      "21 23.592547098795574 19.59370994567871 17.640226364135742\n",
      "22 24.043593724568684 19.61277961730957 18.7198429107666\n",
      "23 24.171785354614258 19.944082260131836 19.993967056274414\n",
      "24 23.48270034790039 20.005367279052734 19.92412757873535\n",
      "Epoch    26: reducing learning rate of group 0 to 2.5000e-04.\n",
      "25 24.580439885457356 19.753225326538086 18.538421630859375\n",
      "26 22.33120282491048 19.73420524597168 18.07735824584961\n",
      "27 23.174415588378906 19.773591995239258 17.788053512573242\n",
      "28 23.066396077473957 19.852020263671875 17.73133087158203\n",
      "29 23.162696202596027 19.92127799987793 17.878368377685547\n",
      "30 21.581586201985676 19.997041702270508 18.198997497558594\n",
      "31 23.048095703125 20.076763153076172 18.4202823638916\n",
      "32 23.4560063680013 20.12278175354004 18.473751068115234\n",
      "33 22.522005081176758 20.128677368164062 18.251108169555664\n",
      "34 22.717865626017254 20.14628791809082 17.96680450439453\n",
      "35 22.75336201985677 20.209016799926758 17.848526000976562\n",
      "Epoch    37: reducing learning rate of group 0 to 1.2500e-04.\n",
      "36 22.38567352294922 20.22795867919922 17.812349319458008\n",
      "37 21.372355779012043 20.23087501525879 17.80124282836914\n",
      "38 21.724557876586914 20.229978561401367 17.855403900146484\n",
      "39 22.75051752726237 20.23712921142578 17.937744140625\n",
      "40 22.427910486857098 20.236167907714844 17.954524993896484\n",
      "41 22.35654067993164 20.243114471435547 17.983396530151367\n",
      "42 21.04034487406413 20.25626564025879 18.01117706298828\n",
      "43 22.216590245564777 20.27625846862793 18.023847579956055\n",
      "44 23.216787974039715 20.312265396118164 18.033809661865234\n",
      "45 21.98039372762044 20.328001022338867 17.979408264160156\n",
      "46 22.45233980814616 20.327632904052734 17.875444412231445\n",
      "Epoch    48: reducing learning rate of group 0 to 6.2500e-05.\n",
      "47 22.901148478190105 20.300617218017578 17.726411819458008\n",
      "48 21.98589833577474 20.299909591674805 17.70292854309082\n",
      "49 21.70565668741862 20.29514503479004 17.716407775878906\n",
      "mae:  3.5132328191079387\n",
      "mse:  17.762673242642045\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [5, 47]\n",
    "hl = 256\n",
    "ha = 256\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = hl\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.0005\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = hl\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train, label_train, last_dev, label_dev, last_test, label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss test_loss\n",
      "0 139.72117360432944 107.54705810546875 86.13550567626953 saving model\n",
      "1 131.24529774983725 97.54459381103516 76.99809265136719 saving model\n",
      "2 118.00240580240886 79.36834716796875 60.548675537109375 saving model\n",
      "3 93.41127522786458 45.00913619995117 31.297805786132812 saving model\n",
      "4 50.83070500691732 23.95153045654297 33.15581130981445 saving model\n",
      "5 47.50562349955241 36.789405822753906 54.63804244995117\n",
      "6 30.936551411946613 19.174989700317383 19.545486450195312 saving model\n",
      "7 27.02692222595215 22.424211502075195 17.118722915649414\n",
      "8 32.8072083791097 21.796279907226562 16.806575775146484\n",
      "9 29.349951426188152 18.90011215209961 17.3961124420166 saving model\n",
      "10 25.77461878458659 20.06362533569336 23.018266677856445\n",
      "11 26.367183685302734 21.279848098754883 25.095792770385742\n",
      "12 24.068212509155273 19.32607078552246 19.757123947143555\n",
      "13 23.675694783528645 19.46296501159668 16.933671951293945\n",
      "14 24.992013931274414 20.04755401611328 16.514951705932617\n",
      "15 24.89605967203776 19.78915023803711 16.966136932373047\n",
      "16 25.064217885335285 19.91752052307129 18.478620529174805\n",
      "17 22.890396118164062 20.342605590820312 19.448152542114258\n",
      "18 23.806041081746418 20.133739471435547 18.29754638671875\n",
      "19 23.821678161621094 20.165884017944336 17.091161727905273\n",
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-04.\n",
      "20 23.063982645670574 20.27153778076172 16.9800968170166\n",
      "21 22.147506713867188 20.237611770629883 17.213504791259766\n",
      "22 22.629340489705402 20.286161422729492 17.7664737701416\n",
      "23 22.835277557373047 20.53243637084961 18.517614364624023\n",
      "24 22.258358001708984 20.66733169555664 18.717514038085938\n",
      "25 23.403252919514973 20.49980354309082 18.055858612060547\n",
      "26 20.95709228515625 20.438432693481445 17.527299880981445\n",
      "27 21.938875198364258 20.48932456970215 17.184555053710938\n",
      "28 21.688886006673176 20.63894271850586 17.19274139404297\n",
      "29 21.65112050374349 20.802091598510742 17.468429565429688\n",
      "30 20.088505427042644 20.97102165222168 17.858623504638672\n",
      "Epoch    32: reducing learning rate of group 0 to 2.5000e-04.\n",
      "31 21.483135223388672 20.980571746826172 17.837276458740234\n",
      "32 21.560603459676106 20.91975975036621 17.66291618347168\n",
      "33 20.567320505777996 20.840476989746094 17.399768829345703\n",
      "34 20.58046531677246 20.819263458251953 17.248624801635742\n",
      "35 20.621930440266926 20.884920120239258 17.29197120666504\n",
      "36 20.682180404663086 20.913257598876953 17.358020782470703\n",
      "37 19.377556482950848 20.92728042602539 17.387908935546875\n",
      "38 19.311412811279297 20.944046020507812 17.489803314208984\n",
      "39 20.422351201375324 20.979337692260742 17.57822036743164\n",
      "40 20.167595545450848 20.928024291992188 17.462055206298828\n",
      "41 19.694852828979492 20.90729331970215 17.401464462280273\n",
      "Epoch    43: reducing learning rate of group 0 to 1.2500e-04.\n",
      "42 18.45750554402669 20.926057815551758 17.38983726501465\n",
      "43 19.51467005411784 20.947235107421875 17.39792823791504\n",
      "44 20.445375442504883 21.004291534423828 17.443195343017578\n",
      "45 19.086096445719402 21.022499084472656 17.426557540893555\n",
      "46 19.22216288248698 21.023813247680664 17.369831085205078\n",
      "47 19.900549570719402 20.987876892089844 17.256816864013672\n",
      "48 18.637630462646484 20.986225128173828 17.2348690032959\n",
      "49 18.73996416727702 20.98580551147461 17.25471305847168\n",
      "mae:  3.4840516224380367\n",
      "mse:  17.396110987775508\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [5, 47]\n",
    "hl = 256\n",
    "ha = 256\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = hl\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.001\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = hl\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train, label_train, last_dev, label_dev, last_test, label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 139.72117360432944 107.54705810546875 saving model\n",
      "1 131.24529774983725 97.54459381103516 saving model\n",
      "2 118.00240580240886 79.36834716796875 saving model\n",
      "3 93.41127522786458 45.00913619995117 saving model\n",
      "4 50.83070500691732 23.95153045654297 saving model\n",
      "5 47.50562349955241 36.789405822753906\n",
      "6 30.936551411946613 19.174989700317383 saving model\n",
      "7 27.02692222595215 22.424211502075195\n",
      "8 32.8072083791097 21.796279907226562\n",
      "9 29.349951426188152 18.90011215209961 saving model\n",
      "10 25.77461878458659 20.06362533569336\n",
      "11 26.367183685302734 21.279848098754883\n",
      "12 24.068212509155273 19.32607078552246\n",
      "13 23.675694783528645 19.46296501159668\n",
      "14 24.992013931274414 20.04755401611328\n",
      "15 24.89605967203776 19.78915023803711\n",
      "16 25.064217885335285 19.91752052307129\n",
      "17 22.890396118164062 20.342605590820312\n",
      "18 23.806041081746418 20.133739471435547\n",
      "19 23.821678161621094 20.165884017944336\n",
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-04.\n",
      "20 23.063982645670574 20.27153778076172\n",
      "21 22.147506713867188 20.237611770629883\n",
      "22 22.629340489705402 20.286161422729492\n",
      "23 22.835277557373047 20.53243637084961\n",
      "24 22.258358001708984 20.66733169555664\n",
      "25 23.403252919514973 20.49980354309082\n",
      "26 20.95709228515625 20.438432693481445\n",
      "27 21.938875198364258 20.48932456970215\n",
      "28 21.688886006673176 20.63894271850586\n",
      "29 21.65112050374349 20.802091598510742\n",
      "30 20.088505427042644 20.97102165222168\n",
      "Epoch    32: reducing learning rate of group 0 to 2.5000e-04.\n",
      "31 21.483135223388672 20.980571746826172\n",
      "32 21.560603459676106 20.91975975036621\n",
      "33 20.567320505777996 20.840476989746094\n",
      "34 20.58046531677246 20.819263458251953\n",
      "35 20.621930440266926 20.884920120239258\n",
      "36 20.682180404663086 20.913257598876953\n",
      "37 19.377556482950848 20.92728042602539\n",
      "38 19.311412811279297 20.944046020507812\n",
      "39 20.422351201375324 20.979337692260742\n",
      "40 20.167595545450848 20.928024291992188\n",
      "41 19.694852828979492 20.90729331970215\n",
      "Epoch    43: reducing learning rate of group 0 to 1.2500e-04.\n",
      "42 18.45750554402669 20.926057815551758\n",
      "43 19.51467005411784 20.947235107421875\n",
      "44 20.445375442504883 21.004291534423828\n",
      "45 19.086096445719402 21.022499084472656\n",
      "46 19.22216288248698 21.023813247680664\n",
      "47 19.900549570719402 20.987876892089844\n",
      "48 18.637630462646484 20.986225128173828\n",
      "49 18.73996416727702 20.98580551147461\n",
      "50 19.167829513549805 20.95602798461914\n",
      "51 17.99499003092448 20.938995361328125\n",
      "52 17.963626861572266 20.95981788635254\n",
      "Epoch    54: reducing learning rate of group 0 to 6.2500e-05.\n",
      "53 18.356726964314777 21.001270294189453\n",
      "54 17.568782170613606 21.01921844482422\n",
      "55 17.44099489847819 21.03489112854004\n",
      "56 17.775749842325848 21.050443649291992\n",
      "57 16.717051823933918 21.078397750854492\n",
      "58 18.1200688680013 21.09149932861328\n",
      "59 18.543293635050457 21.094751358032227\n",
      "60 16.368826866149902 21.11170768737793\n",
      "61 17.570077260335285 21.143747329711914\n",
      "62 17.683612823486328 21.18895149230957\n",
      "63 17.358239491780598 21.22615623474121\n",
      "Epoch    65: reducing learning rate of group 0 to 3.1250e-05.\n",
      "64 16.765793164571125 21.255817413330078\n",
      "65 15.998339970906576 21.27223777770996\n",
      "66 17.52616818745931 21.282649993896484\n",
      "67 16.421637852986652 21.279037475585938\n",
      "68 15.935373306274414 21.270353317260742\n",
      "69 15.795608838399252 21.268877029418945\n",
      "70 16.762490272521973 21.263023376464844\n",
      "71 17.151262919108074 21.26580047607422\n",
      "72 16.52981185913086 21.26247215270996\n",
      "73 15.184050877888998 21.25255012512207\n",
      "74 16.941020965576172 21.243227005004883\n",
      "Epoch    76: reducing learning rate of group 0 to 1.5625e-05.\n",
      "75 16.524128595987957 21.239439010620117\n",
      "76 17.176303227742512 21.23816680908203\n",
      "77 15.742613156636557 21.245254516601562\n",
      "78 17.130266189575195 21.258548736572266\n",
      "79 16.96885617574056 21.269733428955078\n",
      "80 16.852627754211426 21.27803611755371\n",
      "81 16.39997164408366 21.28174591064453\n",
      "82 16.346479098002117 21.287315368652344\n",
      "83 16.052526156107586 21.29244613647461\n",
      "84 16.975258509318035 21.297590255737305\n",
      "85 16.967287063598633 21.300243377685547\n",
      "Epoch    87: reducing learning rate of group 0 to 7.8125e-06.\n",
      "86 17.054950714111328 21.299776077270508\n",
      "87 16.600855191548664 21.301950454711914\n",
      "88 16.79460112253825 21.302104949951172\n",
      "89 15.900181770324707 21.302349090576172\n",
      "90 17.430532455444336 21.30341339111328\n",
      "91 17.006566365559895 21.307889938354492\n",
      "92 15.607935587565104 21.314374923706055\n",
      "93 16.073835690816242 21.319454193115234\n",
      "94 16.346268971761067 21.324905395507812\n",
      "95 16.305720965067547 21.328418731689453\n",
      "96 15.426581064860025 21.331411361694336\n",
      "Epoch    98: reducing learning rate of group 0 to 3.9063e-06.\n",
      "97 16.44299379984538 21.33523941040039\n",
      "98 16.27662181854248 21.337329864501953\n",
      "99 16.233010292053223 21.339540481567383\n",
      "100 16.062303225199383 21.340839385986328\n",
      "101 16.179701805114746 21.341894149780273\n",
      "102 16.663289388020832 21.342323303222656\n",
      "103 16.01409848531087 21.342119216918945\n",
      "104 17.06493314107259 21.34029769897461\n",
      "105 16.51308822631836 21.339982986450195\n",
      "106 15.694793701171875 21.34049415588379\n",
      "107 16.498865763346355 21.340682983398438\n",
      "Epoch   109: reducing learning rate of group 0 to 1.9531e-06.\n",
      "108 16.772288004557293 21.340635299682617\n",
      "109 15.591990152994791 21.341014862060547\n",
      "110 16.21387831370036 21.340560913085938\n",
      "111 15.493776003519693 21.340351104736328\n",
      "112 17.67879803975423 21.340328216552734\n",
      "113 16.254743258158367 21.340957641601562\n",
      "114 16.922336260477703 21.34221076965332\n",
      "115 16.640122095743816 21.34299087524414\n",
      "116 17.506327946980793 21.34337615966797\n",
      "117 16.574318250020344 21.343427658081055\n",
      "118 17.129810969034832 21.34364891052246\n",
      "Epoch   120: reducing learning rate of group 0 to 9.7656e-07.\n",
      "119 16.051589330037434 21.343772888183594\n",
      "120 15.71385924021403 21.344091415405273\n",
      "121 17.53468195597331 21.344633102416992\n",
      "122 16.868907928466797 21.345256805419922\n",
      "123 15.865885416666666 21.34589385986328\n",
      "124 17.11298370361328 21.346824645996094\n",
      "125 16.45844300587972 21.347625732421875\n",
      "126 17.065021514892578 21.348478317260742\n",
      "127 16.03770573933919 21.34905433654785\n",
      "128 16.034561475118 21.349592208862305\n",
      "129 17.00102170308431 21.349884033203125\n",
      "Epoch   131: reducing learning rate of group 0 to 4.8828e-07.\n",
      "130 15.896890004475912 21.349782943725586\n",
      "131 16.088897387186687 21.349777221679688\n",
      "132 15.991805076599121 21.349721908569336\n",
      "133 17.417496999104817 21.349794387817383\n",
      "134 16.245402018229168 21.34995460510254\n",
      "135 16.035897572835285 21.350154876708984\n",
      "136 17.652210871378582 21.350324630737305\n",
      "137 15.702462514241537 21.35055923461914\n",
      "138 17.14345868428548 21.35066032409668\n",
      "139 17.57009760538737 21.35047721862793\n",
      "140 17.9394048055013 21.350257873535156\n",
      "Epoch   142: reducing learning rate of group 0 to 2.4414e-07.\n",
      "141 15.859453201293945 21.350006103515625\n",
      "142 17.41794268290202 21.349931716918945\n",
      "143 16.910254796346027 21.349878311157227\n",
      "144 15.23918628692627 21.34987449645996\n",
      "145 16.799754460652668 21.34992790222168\n",
      "146 17.14846642812093 21.349916458129883\n",
      "147 15.832428614298502 21.34986114501953\n",
      "148 15.332603454589844 21.349782943725586\n",
      "149 16.323239008585613 21.349775314331055\n",
      "mae:  3.4840516224380367\n",
      "mse:  17.396110987775508\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [5, 47]\n",
    "hl = 256\n",
    "ha = 256\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = hl\n",
    "config[\"num_epochs\"] = 150\n",
    "config[\"lr\"] = 0.001\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = hl\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train, label_train, last_dev, label_dev, last_test, label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 7, 52)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
