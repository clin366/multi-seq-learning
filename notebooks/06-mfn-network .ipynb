{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(os.path.pardir)\n",
    "# load data from file \n",
    "import numpy as np \n",
    "save_file_name = ['fea_seq.npy', 'last_observation_seq.npy', 'label_seq.npy', 'masking_seq.npy',\n",
    "                   'delta_seq.npy', 'train_valid_test_split.npy']\n",
    "save_folder = 'data/raw/pol_temp_rh'\n",
    "saved_arrays = []\n",
    "for file_name in save_file_name:\n",
    "    saved_arrays.append(np.load(os.path.join(save_folder, file_name)))\n",
    "[fea_seq, last_observation_seq, label_seq, masking_seq, delta_seq, train_valid_test_split] = saved_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split \n",
    "train_index = [k for k in range(train_valid_test_split[0])]\n",
    "dev_index = [k for k in range(train_valid_test_split[0], \n",
    "                               train_valid_test_split[0] + train_valid_test_split[1])]\n",
    "test_index = [k for k in range(train_valid_test_split[0] + train_valid_test_split[1],\n",
    "              train_valid_test_split[0] + train_valid_test_split[1] + train_valid_test_split[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_by_index_range(nparray_list, label_array, index_range):\n",
    "    '''\n",
    "    nparray_list: list of nparrays to select according to index range \n",
    "    label_array: select the labels from label array\n",
    "    '''\n",
    "    # get non-na index\n",
    "    non_na_index = []\n",
    "    for index in index_range:\n",
    "        if not np.isnan(label_array[index]):\n",
    "            non_na_index.append(index)\n",
    "    \n",
    "    return [k[non_na_index] for k in nparray_list], label_array[non_na_index].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split set to train, test and dev sets \n",
    "# train set\n",
    "[fea_train, last_train], label_train =  get_array_by_index_range([fea_seq,last_observation_seq], label_seq, train_index)\n",
    "# dev set \n",
    "[fea_dev, last_dev], label_dev =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, dev_index)\n",
    "# test set \n",
    "[fea_test, last_test], label_test =  get_array_by_index_range([fea_seq, last_observation_seq], label_seq, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(fea_train, array_list):\n",
    "    \"\"\"\n",
    "    array_list: [fea_dev, fea_test, last_train, last_dev, last_test] to normalize \n",
    "    \"\"\"\n",
    "    train_mean = np.nanmean(fea_train, axis=0)\n",
    "    train_std = np.nanstd(fea_train, axis=0)\n",
    "    def norm_arr(nparr):\n",
    "        return(nparr - train_mean)/train_std\n",
    "    return (norm_arr(fea_train), [norm_arr(k) for k in array_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_train, [fea_dev, fea_test, last_train, last_dev, last_test] = normalize_feature(fea_train,\n",
    "                                                                                   [fea_dev, fea_test, \n",
    "                                                                                    last_train, last_dev,\n",
    "                                                                                    last_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record mean after normalization \n",
    "x_mean_aft_nor = np.nanmean(fea_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control experiment using last observed value for missing data imputation \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFN(nn.Module):\n",
    "    def __init__(self,config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig):\n",
    "        super(MFN, self).__init__()\n",
    "        [self.d_l,self.d_a] = config[\"input_dims\"]\n",
    "        [self.dh_l,self.dh_a] = config[\"h_dims\"]\n",
    "        total_h_dim = self.dh_l+self.dh_a\n",
    "        \n",
    "        self.mem_dim = config[\"memsize\"]\n",
    "        window_dim = config[\"windowsize\"]\n",
    "        output_dim = 1\n",
    "        attInShape = total_h_dim*window_dim\n",
    "        gammaInShape = attInShape+self.mem_dim\n",
    "        final_out = total_h_dim+self.mem_dim\n",
    "        h_att1 = NN1Config[\"shapes\"]\n",
    "        h_att2 = NN2Config[\"shapes\"]\n",
    "        h_gamma1 = gamma1Config[\"shapes\"]\n",
    "        h_gamma2 = gamma2Config[\"shapes\"]\n",
    "        h_out = outConfig[\"shapes\"]\n",
    "        att1_dropout = NN1Config[\"drop\"]\n",
    "        att2_dropout = NN2Config[\"drop\"]\n",
    "        gamma1_dropout = gamma1Config[\"drop\"]\n",
    "        gamma2_dropout = gamma2Config[\"drop\"]\n",
    "        out_dropout = outConfig[\"drop\"]\n",
    "\n",
    "        self.lstm_l = nn.LSTMCell(self.d_l, self.dh_l)\n",
    "        self.lstm_a = nn.LSTMCell(self.d_a, self.dh_a)\n",
    "\n",
    "        self.att1_fc1 = nn.Linear(attInShape, h_att1)\n",
    "        self.att1_fc2 = nn.Linear(h_att1, attInShape)\n",
    "        self.att1_dropout = nn.Dropout(att1_dropout)\n",
    "\n",
    "        self.att2_fc1 = nn.Linear(attInShape, h_att2)\n",
    "        self.att2_fc2 = nn.Linear(h_att2, self.mem_dim)\n",
    "        self.att2_dropout = nn.Dropout(att2_dropout)\n",
    "\n",
    "        self.gamma1_fc1 = nn.Linear(gammaInShape, h_gamma1)\n",
    "        self.gamma1_fc2 = nn.Linear(h_gamma1, self.mem_dim)\n",
    "        self.gamma1_dropout = nn.Dropout(gamma1_dropout)\n",
    "\n",
    "        self.gamma2_fc1 = nn.Linear(gammaInShape, h_gamma2)\n",
    "        self.gamma2_fc2 = nn.Linear(h_gamma2, self.mem_dim)\n",
    "        self.gamma2_dropout = nn.Dropout(gamma2_dropout)\n",
    "\n",
    "        self.out_fc1 = nn.Linear(final_out, h_out)\n",
    "        self.out_fc2 = nn.Linear(h_out, output_dim)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_l = x[:,:,:self.d_l]\n",
    "        x_a = x[:,:,self.d_l:self.d_l+self.d_a]\n",
    "        # x is t x n x d\n",
    "        n = x.shape[1]\n",
    "        t = x.shape[0]\n",
    "        self.h_l = torch.zeros(n, self.dh_l)\n",
    "        self.h_a = torch.zeros(n, self.dh_a)\n",
    "\n",
    "        self.c_l = torch.zeros(n, self.dh_l)\n",
    "        self.c_a = torch.zeros(n, self.dh_a)\n",
    "        \n",
    "        self.mem = torch.zeros(n, self.mem_dim)\n",
    "        all_h_ls = []\n",
    "        all_h_as = []\n",
    "\n",
    "        all_c_ls = []\n",
    "        all_c_as = []\n",
    "\n",
    "        all_mems = []\n",
    "        for i in range(t):\n",
    "            # prev time step\n",
    "            prev_c_l = self.c_l\n",
    "            prev_c_a = self.c_a\n",
    "\n",
    "            # curr time step\n",
    "            new_h_l, new_c_l = self.lstm_l(x_l[i], (self.h_l, self.c_l))\n",
    "            new_h_a, new_c_a = self.lstm_a(x_a[i], (self.h_a, self.c_a))\n",
    "   \n",
    "            # concatenate\n",
    "            prev_cs = torch.cat([prev_c_l,prev_c_a], dim=1)\n",
    "            new_cs = torch.cat([new_c_l,new_c_a], dim=1)\n",
    "            \n",
    "            cStar = torch.cat([prev_cs,new_cs], dim=1)\n",
    "            attention = F.softmax(self.att1_fc2(self.att1_dropout(F.relu(self.att1_fc1(cStar)))),dim=1)\n",
    "            attended = attention*cStar\n",
    "            \n",
    "            cHat = F.tanh(self.att2_fc2(self.att2_dropout(F.relu(self.att2_fc1(attended)))))\n",
    "            \n",
    "            both = torch.cat([attended,self.mem], dim=1)\n",
    "            gamma1 = F.sigmoid(self.gamma1_fc2(self.gamma1_dropout(F.relu(self.gamma1_fc1(both)))))\n",
    "            gamma2 = F.sigmoid(self.gamma2_fc2(self.gamma2_dropout(F.relu(self.gamma2_fc1(both)))))\n",
    "            \n",
    "            self.mem = gamma1*self.mem + gamma2*cHat\n",
    "            all_mems.append(self.mem)\n",
    "            # update\n",
    "            self.h_l, self.c_l = new_h_l, new_c_l\n",
    "            self.h_a, self.c_a = new_h_a, new_c_a\n",
    "\n",
    "            all_h_ls.append(self.h_l)\n",
    "            all_h_as.append(self.h_a)\n",
    " \n",
    "            all_c_ls.append(self.c_l)\n",
    "            all_c_as.append(self.c_a)\n",
    "\n",
    "        # last hidden layer last_hs is n x h\n",
    "        last_h_l = all_h_ls[-1]\n",
    "        last_h_a = all_h_as[-1]\n",
    "\n",
    "        last_mem = all_mems[-1]\n",
    "        last_hs = torch.cat([last_h_l,last_h_a,last_mem], dim=1)\n",
    "        output = self.out_fc2(self.out_dropout(F.relu(self.out_fc1(last_hs))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(664, 7, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mfn(X_train, y_train, X_valid, y_valid, X_test, y_test, configs):\n",
    "#     p = np.random.permutation(X_train.shape[0])\n",
    "    # no shuffle, keep original order \n",
    "    # swap axes for back propagation \n",
    "    def swap_axes(nparr):\n",
    "        return nparr.swapaxes(0,1)\n",
    "    X_train = swap_axes(X_train)\n",
    "    X_valid = swap_axes(X_valid)\n",
    "    X_test = swap_axes(X_test)\n",
    "    \n",
    "    # model parameters \n",
    "    input_size = X_train.shape[2]\n",
    "    h = 128\n",
    "    t = X_train.shape[0]\n",
    "    output_dim = 1\n",
    "    dropout = 0.5\n",
    "\n",
    "#     d = X_train.shape[2]\n",
    "#     h = 128\n",
    "#     t = X_train.shape[0]\n",
    "#     output_dim = 1\n",
    "#     dropout = 0.5\n",
    "\n",
    "    [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig] = configs\n",
    "\n",
    "    \n",
    "    #model = EFLSTM(d,h,output_dim,dropout)\n",
    "    model = MFN(config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=config[\"lr\"])\n",
    "    #optimizer = optim.SGD(model.parameters(),lr=config[\"lr\"],momentum=config[\"momentum\"])\n",
    "\n",
    "    # optimizer = optim.SGD([\n",
    "    #                 {'params':model.lstm_l.parameters(), 'lr':config[\"lr\"]},\n",
    "    #                 {'params':model.classifier.parameters(), 'lr':config[\"lr\"]}\n",
    "    #             ], momentum=0.9)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=10, factor=0.5, verbose=True)\n",
    "    \n",
    "#     criterion = nn.L1Loss()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "#     criterion = criterion.to(device)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer,mode='min',patience=100,factor=0.5,verbose=True)\n",
    "\n",
    "    def train(model, batchsize, X_train, y_train, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        total_n = X_train.shape[1]\n",
    "        num_batches = math.ceil(total_n / batchsize)\n",
    "        for batch in range(num_batches):\n",
    "            start = batch*batchsize\n",
    "            end = (batch+1)*batchsize\n",
    "            optimizer.zero_grad()\n",
    "            batch_X = torch.Tensor(X_train[:,start:end])\n",
    "            batch_y = torch.Tensor(y_train[start:end])\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / num_batches\n",
    "\n",
    "    def evaluate(model, X_valid, y_valid, criterion):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_valid)\n",
    "            batch_y = torch.Tensor(y_valid)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            epoch_loss = criterion(predictions, batch_y).item()\n",
    "        return epoch_loss\n",
    "\n",
    "    def predict(model, X_test):\n",
    "        epoch_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_X = torch.Tensor(X_test)\n",
    "            predictions = model.forward(batch_X).squeeze(1)\n",
    "            predictions = predictions.cpu().data.numpy()\n",
    "        return predictions\n",
    "\n",
    "    best_valid = 999999.0\n",
    "    rand = random.randint(0,100000)\n",
    "    print('epoch train_loss valid_loss')\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        train_loss = train(model, config[\"batchsize\"], X_train, y_train, optimizer, criterion)\n",
    "        valid_loss = evaluate(model, X_valid, y_valid, criterion)\n",
    "        scheduler.step(valid_loss)\n",
    "        if valid_loss <= best_valid:\n",
    "            # save model\n",
    "            best_valid = valid_loss\n",
    "            print(epoch, train_loss, valid_loss, 'saving model')\n",
    "            torch.save(model, 'models/temp_models/mfn_%d.pt' %rand)\n",
    "        else:\n",
    "            print(epoch, train_loss, valid_loss)\n",
    "\n",
    "#     print 'model number is:', rand\n",
    "    model = torch.load('models/temp_models/mfn_%d.pt' %rand)\n",
    "\n",
    "    predictions = predict(model, X_test)\n",
    "    mae = np.mean(np.absolute(predictions-y_test))\n",
    "    print(\"mae: \", mae)\n",
    "    mse = np.mean((predictions - y_test)**2)\n",
    "    print(\"mse: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 7, 5)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 7, 5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 138.94296010335287 111.36531829833984 saving model\n",
      "1 137.82977294921875 110.49248504638672 saving model\n",
      "2 137.01726786295572 109.66050720214844 saving model\n",
      "3 136.0467071533203 108.86024475097656 saving model\n",
      "4 135.12869771321616 108.07041931152344 saving model\n",
      "5 134.25636291503906 107.2625961303711 saving model\n",
      "6 133.40024058024088 106.4128189086914 saving model\n",
      "7 132.40770975748697 105.5063705444336 saving model\n",
      "8 131.41480509440103 104.52982330322266 saving model\n",
      "9 130.3074213663737 103.48682403564453 saving model\n",
      "10 129.0336430867513 102.34229278564453 saving model\n",
      "11 127.77091979980469 101.06938934326172 saving model\n",
      "12 126.33504740397136 99.64335632324219 saving model\n",
      "13 124.54395548502605 98.02600860595703 saving model\n",
      "14 122.75349934895833 96.17029571533203 saving model\n",
      "15 120.59367116292317 94.01270294189453 saving model\n",
      "16 118.17789967854817 91.48675537109375 saving model\n",
      "17 114.96636199951172 88.4975814819336 saving model\n",
      "18 111.62861887613933 84.93543243408203 saving model\n",
      "19 107.22671254475911 80.6561508178711 saving model\n",
      "20 102.38775634765625 75.48716735839844 saving model\n",
      "21 96.64222717285156 69.22349548339844 saving model\n",
      "22 89.37521870930989 61.6657829284668 saving model\n",
      "23 80.3665377298991 52.7357063293457 saving model\n",
      "24 69.59941101074219 42.607582092285156 saving model\n",
      "25 57.312826792399086 32.07731628417969 saving model\n",
      "26 46.404073079427086 23.0091552734375 saving model\n",
      "27 37.542924880981445 18.291913986206055 saving model\n",
      "28 31.78167661031087 19.767518997192383\n",
      "29 30.498308817545574 24.415672302246094\n",
      "30 34.198865254720054 26.038095474243164\n",
      "31 31.933732986450195 24.027873992919922\n",
      "32 29.813552220662434 21.136455535888672\n",
      "33 29.578296661376953 19.242685317993164\n",
      "34 29.387778600056965 18.470388412475586\n",
      "35 28.895479838053387 18.329296112060547\n",
      "36 28.74001185099284 18.351346969604492\n",
      "37 29.211820602416992 18.393157958984375\n",
      "Epoch    39: reducing learning rate of group 0 to 5.0000e-05.\n",
      "38 29.03318913777669 18.500490188598633\n",
      "39 27.53736623128255 18.60832977294922\n",
      "40 28.95950762430827 18.76158905029297\n",
      "41 28.44602330525716 18.93896484375\n",
      "42 28.39764404296875 19.12883186340332\n",
      "43 27.742191950480144 19.294816970825195\n",
      "44 28.301881790161133 19.433502197265625\n",
      "45 27.915600458780926 19.526426315307617\n",
      "46 27.847049713134766 19.562236785888672\n",
      "47 27.494252522786457 19.539642333984375\n",
      "48 27.846471150716145 19.468488693237305\n",
      "Epoch    50: reducing learning rate of group 0 to 2.5000e-05.\n",
      "49 27.084513982137043 19.387582778930664\n",
      "mae:  3.5601947343053895\n",
      "mse:  17.795466422818965\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 256\n",
    "ha = 256\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = hl\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.0001\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = hl\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train, label_train, last_dev, label_dev, last_test, label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 156.11405563354492 112.65887451171875 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type MFN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 155.20471954345703 111.86585235595703 saving model\n",
      "2 154.27276102701822 111.05146026611328 saving model\n",
      "3 153.34173075358072 110.19532775878906 saving model\n",
      "4 152.46337381998697 109.27770233154297 saving model\n",
      "5 151.3701604207357 108.2803726196289 saving model\n",
      "6 150.23250579833984 107.18980407714844 saving model\n",
      "7 148.7600466410319 105.98526000976562 saving model\n",
      "8 147.48532740275064 104.63453674316406 saving model\n",
      "9 145.8607610066732 103.10432434082031 saving model\n",
      "10 143.8507563273112 101.32836151123047 saving model\n",
      "11 141.74614461263022 99.2261962890625 saving model\n",
      "12 139.60418319702148 96.70524597167969 saving model\n",
      "13 135.94170888264975 93.63931274414062 saving model\n",
      "14 132.91644287109375 89.83513641357422 saving model\n",
      "15 128.44332122802734 85.05644226074219 saving model\n",
      "16 121.9296391805013 78.93667602539062 saving model\n",
      "17 113.12656720479329 71.0453872680664 saving model\n",
      "18 104.2041409810384 61.046142578125 saving model\n",
      "19 92.30810991923015 49.000938415527344 saving model\n",
      "20 77.70903968811035 35.91673278808594 saving model\n",
      "21 63.87449232737223 24.42972755432129 saving model\n",
      "22 49.597131411234535 18.109312057495117 saving model\n",
      "23 41.95189698537191 18.223752975463867\n",
      "24 42.251280784606934 21.180744171142578\n",
      "25 40.602545420328774 21.79020881652832\n",
      "26 43.07428868611654 20.537229537963867\n",
      "27 38.93658192952474 19.257144927978516\n",
      "28 39.506248474121094 18.80607795715332\n",
      "29 41.402894020080566 18.813085556030273\n",
      "30 40.59805679321289 18.886306762695312\n",
      "31 40.96233717600504 19.01270294189453\n",
      "32 41.36229674021403 19.009685516357422\n",
      "Epoch    34: reducing learning rate of group 0 to 5.0000e-05.\n",
      "33 37.42450968424479 18.99268913269043\n",
      "34 34.94084644317627 19.0839900970459\n",
      "35 39.64996306101481 19.1924991607666\n",
      "36 36.431121826171875 19.22957992553711\n",
      "37 39.93675708770752 19.242481231689453\n",
      "38 33.91209411621094 19.222606658935547\n",
      "39 40.03019269307455 19.240306854248047\n",
      "40 41.2510986328125 19.175416946411133\n",
      "41 37.35567569732666 19.123023986816406\n",
      "42 37.64503828684489 19.11286735534668\n",
      "43 39.10742155710856 19.07604217529297\n",
      "Epoch    45: reducing learning rate of group 0 to 2.5000e-05.\n",
      "44 37.62091986338297 19.102529525756836\n",
      "45 39.496315002441406 19.127338409423828\n",
      "46 37.756927808125816 19.1340274810791\n",
      "47 38.648733139038086 19.167377471923828\n",
      "48 39.32019964853922 19.20580291748047\n",
      "49 39.885996182759605 19.2198543548584\n",
      "mae:  3.478289631772632\n",
      "mse:  17.183259739695785\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 128\n",
    "ha = 128\n",
    "drop = 0.7\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = hl\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = hl\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.0001\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = hl\n",
    "NN1Config[\"drop\"] = drop\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = drop\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = hl\n",
    "gamma1Config[\"drop\"] = drop\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = hl \n",
    "gamma2Config[\"drop\"] = drop\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = hl\n",
    "outConfig[\"drop\"] = drop\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train, label_train, last_dev, label_dev, last_test, label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n",
      "0 141.46016729445685 111.05241394042969 saving model\n",
      "1 139.5429970877511 109.15363311767578 saving model\n",
      "2 137.1174102056594 106.47740936279297 saving model\n",
      "3 133.2586486453102 102.0577163696289 saving model\n",
      "4 126.23620950608026 92.49103546142578 saving model\n",
      "5 109.34628759111676 65.91061401367188 saving model\n",
      "6 75.19069113050189 32.051841735839844 saving model\n",
      "7 47.68678733280727 18.42310905456543 saving model\n",
      "8 40.83152021680559 17.621511459350586 saving model\n",
      "9 39.16866366068522 17.819971084594727\n",
      "10 38.2137383052281 17.699769973754883\n",
      "11 40.398299512409025 17.7195987701416\n",
      "12 39.65765592030117 17.71005630493164\n",
      "13 39.91312195005871 17.810375213623047\n",
      "14 40.267848764147075 17.874692916870117\n",
      "15 39.71537649063837 17.826465606689453\n",
      "16 37.678475879487536 18.002872467041016\n",
      "17 38.408209755307155 17.90630531311035\n",
      "18 38.586224260784334 17.899261474609375\n",
      "Epoch    20: reducing learning rate of group 0 to 5.0000e-05.\n",
      "19 39.1255780401684 17.900102615356445\n",
      "20 37.00560288202195 17.926668167114258\n",
      "21 37.26020472390311 17.94246482849121\n",
      "22 38.776448249816895 17.944263458251953\n",
      "23 41.44891552698044 17.96026039123535\n",
      "24 36.725998923892064 18.01007843017578\n",
      "25 40.34772664024716 17.985715866088867\n",
      "26 37.25746726989746 17.98213768005371\n",
      "27 37.077976839882986 17.98834991455078\n",
      "28 39.35584231785366 18.02006721496582\n",
      "29 38.22949089322771 18.03048324584961\n",
      "Epoch    31: reducing learning rate of group 0 to 2.5000e-05.\n",
      "30 37.8877539180574 18.0456600189209\n",
      "31 36.222836335500084 18.058443069458008\n",
      "32 40.56285656066168 18.060100555419922\n",
      "33 37.4381761323838 18.08802032470703\n",
      "34 35.14192013513474 18.106613159179688\n",
      "35 35.547738188789005 18.11886215209961\n",
      "36 38.13244790122623 18.12938117980957\n",
      "37 38.03604961576916 18.130226135253906\n",
      "38 38.60046899886358 18.145498275756836\n",
      "39 38.55636405944824 18.14045524597168\n",
      "40 37.67160165877569 18.14348793029785\n",
      "Epoch    42: reducing learning rate of group 0 to 1.2500e-05.\n",
      "41 36.735329173860094 18.1468505859375\n",
      "42 36.88072322663807 18.15289878845215\n",
      "43 36.90115167981102 18.16529083251953\n",
      "44 38.229702586219425 18.169580459594727\n",
      "45 36.97989863441104 18.184335708618164\n",
      "46 36.26669207073393 18.19936752319336\n",
      "47 38.27374944232759 18.2003231048584\n",
      "48 37.153700828552246 18.200477600097656\n",
      "49 41.54048908324469 18.202611923217773\n",
      "mae:  3.664320944163425\n",
      "mse:  18.87775124618758\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 64\n",
    "ha = 32\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = 64\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = 16\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.0001\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = 32\n",
    "NN1Config[\"drop\"] = 0.7\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = 0.7\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = 32\n",
    "gamma1Config[\"drop\"] = 0.7\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = 32 \n",
    "gamma2Config[\"drop\"] = 0.7\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = 32\n",
    "outConfig[\"drop\"] = 0.7\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train, label_train, last_dev, label_dev, last_test, label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 143.4269805181594 111.73497772216797 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type MFN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 142.4031746273949 110.84442901611328 saving model\n",
      "2 141.34705861409506 109.79991912841797 saving model\n",
      "3 140.01563444591704 108.49638366699219 saving model\n",
      "4 138.56229727608817 106.8376235961914 saving model\n",
      "5 136.42176419212706 104.7589111328125 saving model\n",
      "6 133.8928749447777 102.01949310302734 saving model\n",
      "7 130.19739786783853 97.88046264648438 saving model\n",
      "8 124.62183053152901 91.02835083007812 saving model\n",
      "9 114.91846320742653 79.38703918457031 saving model\n",
      "10 99.70678529285249 61.57096862792969 saving model\n",
      "11 79.85024107070197 41.91075134277344 saving model\n",
      "12 57.072988146827335 26.158111572265625 saving model\n",
      "13 42.631982440040225 18.200637817382812 saving model\n",
      "14 35.34261085873558 17.902345657348633 saving model\n",
      "15 32.36609377179827 18.72093963623047\n",
      "16 31.483746755690802 18.558666229248047\n",
      "17 33.00557127453032 18.6918888092041\n",
      "18 33.97688620431082 18.692129135131836\n",
      "19 31.181169237409318 18.46806526184082\n",
      "20 30.53945055462065 18.52680015563965\n",
      "21 31.976759365626744 18.634584426879883\n",
      "22 32.99534184592111 18.390117645263672\n",
      "23 31.088438034057617 18.641740798950195\n",
      "24 31.87835048493885 18.668018341064453\n",
      "Epoch    26: reducing learning rate of group 0 to 5.0000e-05.\n",
      "25 30.179817926316034 18.73569679260254\n",
      "26 32.04582972753616 18.6866455078125\n",
      "27 30.356357619875954 18.656906127929688\n",
      "28 33.70395106361026 18.668216705322266\n",
      "29 29.92553910754976 18.52768325805664\n",
      "30 32.365830466860814 18.529325485229492\n",
      "31 30.653296743120467 18.625411987304688\n",
      "32 31.300321351914178 18.697290420532227\n",
      "33 30.30149659656343 18.62642478942871\n",
      "34 31.39113408043271 18.657352447509766\n",
      "35 32.227743693760466 18.655595779418945\n",
      "Epoch    37: reducing learning rate of group 0 to 2.5000e-05.\n",
      "36 30.946135838826496 18.668264389038086\n",
      "37 31.05788839431036 18.63735008239746\n",
      "38 32.45229062579927 18.654495239257812\n",
      "39 31.50154804048084 18.63388442993164\n",
      "40 29.772437141055153 18.693222045898438\n",
      "41 31.45062337602888 18.700847625732422\n",
      "42 31.029608408610027 18.64781951904297\n",
      "43 31.7592617670695 18.616573333740234\n",
      "44 31.45417962755476 18.661670684814453\n",
      "45 32.08871923174177 18.729196548461914\n",
      "46 31.586013703119185 18.75480079650879\n",
      "Epoch    48: reducing learning rate of group 0 to 1.2500e-05.\n",
      "47 28.146083059765044 18.757429122924805\n",
      "48 31.993355342320033 18.742515563964844\n",
      "49 31.980899265834264 18.74898338317871\n",
      "mae:  3.8061485873766183\n",
      "mse:  20.284504868850487\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 64\n",
    "ha = 32\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = 64\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = 32\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.0001\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = 32\n",
    "NN1Config[\"drop\"] = 0.5\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = 0.5\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = 32\n",
    "gamma1Config[\"drop\"] = 0.5\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = 32 \n",
    "gamma2Config[\"drop\"] = 0.5\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = 32\n",
    "outConfig[\"drop\"] = 0.5\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train, label_train, last_dev, label_dev, last_test, label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21854222,  0.49295122,  0.12868645, -0.46155388, -0.30762506],\n",
       "       [-0.24856643,  0.7073676 , -0.24226635, -0.46902672, -0.91637258],\n",
       "       [-0.71349436,  0.52921296, -0.06427684,  0.07079409, -0.11530977],\n",
       "       [-0.53756466,  0.6018833 ,  0.43019764,  0.00278131,  0.15688786],\n",
       "       [-0.72005529,  0.58860501, -0.07057453, -0.60066422, -0.62825199],\n",
       "       [-2.0939877 ,  0.80690814, -0.40207978, -0.4524199 , -0.68732983],\n",
       "       [-1.21070575,  0.49426344, -1.34088481, -0.45297472, -0.48903942]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch train_loss valid_loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 148.02859606061662 115.98423767089844 saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flynn/Documents/Desktop/softs/miniconda/envs/airpol/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type MFN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 147.50207846505302 115.56013488769531 saving model\n",
      "2 147.06520044235955 115.17402648925781 saving model\n",
      "3 146.61236935570128 114.72411346435547 saving model\n",
      "4 146.07836986723402 114.17180633544922 saving model\n",
      "5 145.40250723702567 113.51115417480469 saving model\n",
      "6 144.6554728916713 112.66338348388672 saving model\n",
      "7 143.56147112165178 111.53781127929688 saving model\n",
      "8 142.12942722865515 110.00544738769531 saving model\n",
      "9 140.02900986444382 107.83295440673828 saving model\n",
      "10 137.35363151913597 104.84867095947266 saving model\n",
      "11 134.32220241001673 100.68287658691406 saving model\n",
      "12 128.79708099365234 94.39310455322266 saving model\n",
      "13 121.39551035563152 84.52606201171875 saving model\n",
      "14 108.39922986711774 68.81757354736328 saving model\n",
      "15 89.18752688453311 46.43396759033203 saving model\n",
      "16 67.97932406834194 25.161245346069336 saving model\n",
      "17 51.22806340172177 17.280485153198242 saving model\n",
      "18 41.087117513020836 18.57951545715332\n",
      "19 43.430546261015394 18.96548843383789\n",
      "20 38.37784331185477 18.58749771118164\n",
      "21 39.784893308367046 18.83677864074707\n",
      "22 38.0778994787307 18.796873092651367\n",
      "23 40.42488179888044 18.631784439086914\n",
      "24 39.75580596923828 18.540138244628906\n",
      "25 40.24034345717657 18.463071823120117\n",
      "26 35.497226170131135 18.772653579711914\n",
      "27 39.88767660231817 18.724533081054688\n",
      "Epoch    29: reducing learning rate of group 0 to 5.0000e-05.\n",
      "28 35.92990066891625 18.653703689575195\n",
      "29 37.89126368931362 18.600311279296875\n",
      "30 39.711146218436106 18.619413375854492\n",
      "31 38.39228266761417 18.674667358398438\n",
      "32 37.585881096976145 18.644594192504883\n",
      "33 36.659203302292596 18.697656631469727\n",
      "34 40.827861331758044 18.799209594726562\n",
      "35 37.31958516438802 18.81047248840332\n",
      "36 36.95740572611491 18.759958267211914\n",
      "37 37.46436632247198 18.839374542236328\n",
      "38 39.343198004223055 18.9541015625\n",
      "Epoch    40: reducing learning rate of group 0 to 2.5000e-05.\n",
      "39 36.67096592131115 18.823049545288086\n",
      "40 36.546430905659996 18.830381393432617\n",
      "41 36.71774019513811 18.897663116455078\n",
      "42 37.03900137401762 18.873756408691406\n",
      "43 38.87174660818918 18.907730102539062\n",
      "44 36.61960093180338 18.930402755737305\n",
      "45 35.20929790678478 18.972925186157227\n",
      "46 40.70320901416597 18.96113395690918\n",
      "47 37.668192500159854 18.984050750732422\n",
      "48 37.711088907150994 18.946386337280273\n",
      "49 35.7085941859654 18.925514221191406\n",
      "mae:  3.6352371665071845\n",
      "mse:  18.610795679962905\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "config[\"input_dims\"] = [1, 4]\n",
    "hl = 32\n",
    "ha = 32\n",
    "config[\"h_dims\"] = [hl, ha]\n",
    "config[\"memsize\"] = 64\n",
    "config[\"windowsize\"] = 2\n",
    "config[\"batchsize\"] = 32\n",
    "config[\"num_epochs\"] = 50\n",
    "config[\"lr\"] = 0.0001\n",
    "NN1Config = dict()\n",
    "NN1Config[\"shapes\"] = 32\n",
    "NN1Config[\"drop\"] = 0.5\n",
    "NN2Config = dict()\n",
    "NN2Config[\"shapes\"] = 32\n",
    "NN2Config[\"drop\"] = 0.5\n",
    "gamma1Config = dict()\n",
    "gamma1Config[\"shapes\"] = 32\n",
    "gamma1Config[\"drop\"] = 0.5\n",
    "gamma2Config = dict()\n",
    "gamma2Config[\"shapes\"] = 32 \n",
    "gamma2Config[\"drop\"] = 0.5\n",
    "outConfig = dict() \n",
    "outConfig[\"shapes\"] = 32\n",
    "outConfig[\"drop\"] = 0.5\n",
    "configs = [config,NN1Config,NN2Config,gamma1Config,gamma2Config,outConfig]\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "train_mfn(last_train, label_train, last_dev, label_dev, last_test, label_test, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:airpol]",
   "language": "python",
   "name": "conda-env-airpol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
